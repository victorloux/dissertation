<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<SearchIndexes Version="1.0">
    <Documents>
        <Document ID="121">
            <Title>Post death</Title>
            <Text>Web 2.0 has impacted strongly on attitudes to who the audience for a memorial should be. Memorialization of deceased users through online social network sites makes death and grief hard to avoid [43]. Personal memorials abound that are to some degree publicly accessible online. Eulogies of dead loved ones can reach a larger audience and achieve greater longevity online compared to that achieved via obituary pages in the newspapers – e.g. the actor Kevin Costner’s eulogy to the singer Whitney Houston at her funeral has reached over 2.5 million viewers via YouTube9. However, unlike some offline memorials, the online ‘public’ audience can be controlled by privacy settings: whilst the general public cannot gain access to a memorialized Facebook page, an extended personal social network can. These same privacy controls and settings can conversely (and ironically) block those close to the deceased from accessing the same pages, if they were not already ‘friends’ online prior to memorialization. In summary, digital memorials can be expected to operate at various levels from the entirely private and domestic to the completely public and national, with various points in between wherein they might be privately created and curated but have levels of openness to public observation and/or contribution. The audience for a memorial may evolve over time, as personal relevance fades yet public significance continues.
{Moncur:2014by}

</Text>
        </Document>
        <Document ID="117">
            <Title>Manual curation</Title>
            <Text>Another interesting selection process that has emerged is the one of specific archiving when certain events are happening. For instance, the UK Web Archive[ukwa] has, on top of their topical-based collections (Energy, Health and Social Care...), a few event-based collections (Queen Jubilee, Olympic and Paralympic Games, London 2005 terrorist attacks...) which reflect the state of the Web at that given moment in history. Masanès (2006, p.83) notes that this type of archiving must be manually curated and thoroughly preparated. He gives a methodology for this, using presidential elections as an example: a time frame for the collection must be defined (“3 months before the election + 1 month after”), what type of sites will be archived and when (“the political parties’ blogs and websites, every week; analysis, commentary and humourous/satire websites, every month; online newspaper articles, once”).</Text>
            <Notes>Parrallel between generic &amp; personal stuff, that is museums/archives vs personal storage
</Notes>
        </Document>
        <Document ID="72">
            <Title>Further reading</Title>
            <Text>Back, M.D. et al., 2010. Facebook profiles reflect actual personality, not self-idealization. Psychological Science, 21(3), pp.372–374.
Ivonin, L. et al., 2012. Unconscious emotions: quantifying and logging something we are not aware of. Personal and Ubiquitous Computing, 17(4), pp.663–673.
Johnson, B.D., 2011. Science Fiction Prototyping, Morgan &amp; Claypool Publishers.
Roberts, L., 2012. Mapping Cultures, Palgrave Macmillan.
Taylor, A.S. &amp; Harper, R., 2003. The Gift of the Gab?: A Design Oriented Sociology of Young People's Use of Mobiles. Computer Supported Cooperative Work (CSCW), 12(3), pp.267–296.

Brubaker, J.R., Hayes, G.R. &amp; Dourish, P., 2013. Beyond the Grave: Facebook as a Site for the Expansion of Death and Mourning. dx.doi.org, 29(3), pp.152–163.
Moncur, W. &amp; Kirk, D., 2014. An emergent framework for digital memorials. In DIS '14: Proceedings of the 2014 conference on Designing interactive systems. New York, New York, USA:  ACM  Request Permissions, pp. 965–974.
https://photoworld.com/photos-on-the-web/ &gt; physical amount of photographs over a year</Text>
        </Document>
        <Document ID="68">
            <Title>Discussion of results &amp; evidence</Title>
        </Document>
        <Document ID="73">
            <Title>Referenced</Title>
            <Text>Banks, R. &amp; Kirk, D.S., 2008. On the design of technology heirlooms. SIMTech'08.
Ben Hammersley, 2012. 64 Things You Need to Know Now for Then,
Boyd, D., 2014. It's Complicated. pp.1–296.
Hangal, S., Lam, M.S. &amp; Heer, J., 2011. MUSE: reviving memories using email archives, New York, New York, USA: ACM.
Hutton, L. &amp; Henderson, T., 2013. An architecture for ethical and privacy-sensitive social network experiments. SIGMETRICS Performance Evaluation Review, 40(4), pp.90–95.
Jung, M., 2013. Remembrance Value of Topics in Social Media,
Kirk, D.S. &amp; Sellen, A., 2010. On human remains. ACM Transactions on Computer-Human Interaction, 17(3), pp.1–43.
Krotoski, A., 2013. Untangling the Web, Faber &amp; Faber.
Massimi, M. &amp; Baecker, R.M., 2011. Dealing with death in design: developing systems for the bereaved. In CHI '11: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. New York, New York, USA:  ACM Request Permissions, pp. 1001–1010.
McCandless, D., 2009. Information is Beautiful, HarperCollins UK.
Odom, W. et al., 2012. Technology heirlooms?: considerations for passing down and inheriting digital materials. In CHI '12: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. New York, New York, USA:  ACM Request Permissions, pp. 337–346.
Rendgen, S. et al., 2012. Information Graphics, Taschen America Llc.
Rosenberg, D. &amp; Grafton, A., 2013. Cartographies of Time, Princeton Architectural Press.
Vashisht, G. &amp; Thakur, S., Facebook as a Corpus for Emoticons-Based Sentiment Analysis. ijetae.com
Whittaker, S. &amp; Sellen, A., 2010. Beyond total capture: a constructive critique of lifelogging. Communications of the ACM, 53(5).
MacDonald, C. and Atwood, M. (2014). What does it mean for a system to be useful?. Proceedings of the 2014 conference on Designing interactive systems - DIS '14. [online] Available at: http://dx.doi.org/10.1145/2598510.2598600 [Accessed 20 Oct. 2014].
Scott, J. (2009). Geocities. [Blog] ASCII by Jason Scott. Available at: http://ascii.textfiles.com/archives/1956 [Accessed 21 Oct. 2014].
Weger, U. and Pratt, J. (2008). Time flies like an arrow: Space-time compatibility effects suggest the use of a mental timeline. Psychonomic Bulletin &amp; Review, [online] 15(2), pp.426-430. Available at: http://dx.doi.org/10.3758/pbr.15.2.426 [Accessed 18 Oct. 2014].
Santiago, J., Lupáñez, J., Pérez, E. and Funes, M. (2007). Time (also) flies from left to right. Psychonomic Bulletin &amp; Review, [online] 14(3), pp.512-516. Available at: http://dx.doi.org/10.3758/bf03194099 [Accessed 22 Oct. 2014].
Baym, N. (2010). [Air-L] Library of Congress Acquires Entire (Public) Twitter Archive. [email]. Mailing list of the Association of Internet Researchers. Available at: https://web.archive.org/web/20101225053512/http://listserv.aoir.org/pipermail/air-l-aoir.org/2010-April/021125.html
Library of Congress (2010). How Tweet It Is!: Library Acquires Entire Twitter Archive. [Blog] Library of Congress blog. Available at: http://blogs.loc.gov/loc/2010/04/how-tweet-it-is-library-acquires-entire-twitter-archive/ [Accessed 21 Oct. 2014].
Haynes, C. (2013a). Drawing Time. [Blog] Thinking in Practice. Available at: http://thinking-in-practice.com/drawing-time-petrie-museum-cathy-haynes [Accessed 6 Dec. 2014].
Haynes, C. (2013b). How do you map a life?. [Blog] The Timekeeper Project. Available at: http://astormisblowing.org/2013/10/11/how-do-you-map-a-life/ [Accessed 6 Dec. 2014].
Haynes, C. (2012). How to Map a Life. [Lecture at We Are The Time conference]. Studium Generale Rietveld Academie, Amsterdam.  Available at: https://www.youtube.com/watch?v=wWyKUy0MIJ4 [Accessed: 3rd Dec. 2014].
Haynes, C. (2014). What would life be like without the word Time? [Blog] Stereochron Island. Available at: http://stereochron.org/post/93491431833/what-would-life-be-like-without-the-word-time [Accessed: 21st Dec. 2014]
Sterne, L. (1761/2012).  The Life and Opinions of Tristram Shandy, Gentleman. [ebook]. Salt Lake City: Project Gutenberg. Available from: http://www.gutenberg.org/ebooks/39270 [Accessed: 7th Dec. 2014]. pp. 347-348 (Book IV, chapter XL). -&gt; Illustrations for disordered life. In-text: (Sterne [1761] 2012, p. 347)
Boroditsky, L. (2011). How Languages Construct Time. In: S. Dehaene and E. M. Brannon, eds., Space, Time and Number in the Brain, 1st ed. San Diego: Academic Press, pp.333-341. Available at: doi:10.1016/B978-0-12-385948-8.00020-7(http://dx.doi.org/10.1016/B978-0-12-385948-8.00020-7) [Accessed 7 Dec. 2014].
Strass, F. and Bell, W. (1812). Descriptive Guide to “The Stream of Time,” or, General Outline of Universal History, Chronology, and Biography, at One View. Hull: Joseph Simmons. Available at: https://books.google.com/books?id=qhVXAAAAcAAJ&amp;lpg=PP1&amp;ots=prHRRzNJbd&amp;pg=PA8 [Accessed 21 Dec. 2014]. pp. 8-10.
Lewis, R. (2014). How Different Cultures Understand Time. Business Insider. [online] Available at: http://www.businessinsider.com/how-different-cultures-understand-time-2014-5 [Accessed 22 Dec. 2014].
Fowler, H. and Burchfield, R. (1996). The new Fowler's modern English usage. Oxford: Clarendon Press, pp. 197–198.
Hornung, E. (1999). The ancient Egyptian books of the afterlife. Ithaca, N.Y.: Cornell University Press. p. 78.
Stern, S. (2003). Time and Process in Ancient Judaism. Oxford and Portland, Oregon: The Littman Library of Jewish Civilization, p. 127.
Lipton, D. (2005). Reviewed Work: Time and Process in Ancient Judaism by Sacha Stern. Bulletin of the School of Oriental and African Studies, University of London, [online] 68(1), pp. 103-104. Available at: http://www.jstor.org/stable/20181858 [Accessed 30 Dec. 2014].
Evans-Pritchard, E.-E. (1940). The Nuer : a description of the modes of livelihood and political institutions of a Nilotic people. Oxford: Clarendon Press. Available at: https://archive.org/details/nuerdescriptiono00evan [Accessed: 29th Dec. 2014]. Chapter III, pp. 94-138.</Text>
        </Document>
        <Document ID="69">
            <Title>Introduction-1</Title>
            <Notes>http://www2.warwick.ac.uk/fac/soc/al/learning_english/leap/writing/moreinfo/

a statement of the importance of the subject
mention of previous work on the subject
a justification for dealing with the subject
a statement of your objectives
a statement of the limitations of the work
a mention of some of the differing viewpoints on the subject
a definition of the topic being discussed

Move 1: Establishing a research territory
- by showing that the general research area is important, central, interesting, problematic, etc. (optional)

- by introducing and reviewing items of previous research in the area (obligatory)

Move 2: Establishing a niche
- by indicating a gap in the previous research or by extending previous knowledge in some way (obligatory)

Move 3: Occupying the niche
- by outlining purposes or stating the nature of the present research (obligatory)

- by listing research questions of hypotheses

- by announcing principal findings

- by stating the value of the previous research

</Notes>
        </Document>
        <Document ID="122">
            <Title>Authenticity/integrity</Title>
            <Text>Different preservation methods yield different results in what is called authenticity, that is how faithful the rendition (i.e. the presentation) of the preserved object will be. Authenticity is subjective and therefore there is debate around this issue (Del Pozo et al., 2012, p. 7; Cook, 2000); however most agree that while as little difference in the presentation is desirable, it is rarely (if at all) possible to keep a truthful rendition. As an example, del Pozo et al. (2012, p. 8) use a spreadsheet within the migration method:
“For instance, one institution may decide to normalise a spreadsheet into a PDF, which would retain the cell values but destroy any formula used to calculate them. This would favour a particular presentation of that digital object over the information form of the original. On the other hand, another institution might decide to normalise spreadsheets into ODF, which may lose some formatting but retain the formula used to derive each cell. In this case, being able to retain the information form of the file would be seen as more important.”
Thibodeau (2002, p. 28) however points out that “people will want to use the best available technology—or at least technologies they know how to use— for discovery, retrieval, processing, and delivery of preserved information. There is a danger that to the degree that preservation solutions keep things unaltered they will create barriers to satisfying this basic user requirement.” An overly faithful preservation would therefore not be desirable. Again, this is subject to a high number of parameters.
Another important issue is integrity, i.e. “ensuring the object is complete and has not been altered or corrupted” (Harvey, 2012, p. 54; Ross [CITED], 2002, p. 7); that is, the object “is what it purports to be”, and no parts are missing from it. 
This is particularly important, Harvey (2012, p. 54) notes, for business archiving, where the legal value of a digital object could be lost if we cannot demonstrate that this object hasn’t been tampered with, or that a migration processes have not incurred data losses. However, as Lynch (2000) has argued, the integrity of objects is only a matter of trust. The guidelines of UNESCO (2003, p. 22) recommend that the best protection for authenticity is proper documentation of the archival process; Harvey (2012, p. 54) expands on this idea: “for instance, if it can be established that a digital object has always been kept in the custody of an archive in which every change to it has been recorded (such as changes resulting from migration of the bit-stream – the dates at which migration occurred, to which media, and so on), then we can be more secure about its integrity.”</Text>
        </Document>
        <Document ID="118">
            <Title>Selection by genre</Title>
            <Synopsis>Rewrite by social network</Synopsis>
            <Text>There are different genres of websites: institutional website, blogs, personal pages, forums. [[Genre have been studied in the context of the Web to see how they replicate or diverge from the genre in the print- ing world (see Crowston and Williams 1997) - in Masanès p88]]. An interesting point about this is that often, robots (crawlers, or later data analysis) can automatically identify their type [Rehm 2002 -&gt; check reference, Masanès:88] which permits the automatic generation of metadata, or simply to restrict a collection of web pages to certain Web genres.</Text>
            <Notes>Parrallel between generic &amp; personal stuff, that is museums/archives vs personal storage
</Notes>
        </Document>
        <Document ID="74">
            <Title>Introduction</Title>
        </Document>
        <Document ID="75">
            <Title>Need for preservation</Title>
            <Synopsis>Growth of data produced
What is archive
Give ~opinion~ and analysis
</Synopsis>
        </Document>
        <Document ID="123">
            <Title>Grammar/spelling checks</Title>
            <Text>	•	Artefact, not artifact
	•	Data always singular (data is); make a style note from Guardian/Observer style guide - “data takes a singular verb (like agenda), though strictly a plural; no one ever uses "agendum" or "datum””
	•	In-text citation: no comma after author(?)
	•	Dates (accessed by): [Accessed 20 Oct. 2014]
Fine type:
	•	Non-breaking space before page numbers (p.⌙22, esp. in sidenotes)

Notes from reviews:

	•	Make a table of contents!
	•	Do not use end notes but footnotes and reference all links in bibliography not as footnotes
	•	Balance literature review with field research
	•	Do not throw out too much
	•	Rw Literature review to Bibliography, referenced to references
	•	Context&gt;Objectives: reads like a design brief for the honours project, focus on the objectives of the dissertation instead
	•	Intro is good
	•	For subsections: flesh out with notes on the question to be resolved (as per introduction)

References:
	•	DOIs: links to a DOI starting with dx.doi.org e.g. http://dx.doi.org/10.1016/B978-0-12-385948-8.00020-7
	•	Years in parentheses, not after a dot ie. Last, F. (2010) not Last, F. 2010. 
</Text>
        </Document>
        <Document ID="119">
            <Title>Digital heritage</Title>
            <Text>A rare exception to this is Digital Heirlooms, a series of conceptual objects developed by Microsoft Research (Odom et al., 2012) to investigate how digital artefacts can be passed down and inherited. 
[Todo: list downsides (too conceptual, representation of data is linear,  testing as a digital heirloom); reaction of users + what I want to test from that. + Tie in with section in Context about the reaction of users, and with W. Moncur’s papers on digital death]</Text>
        </Document>
        <Document ID="80">
            <Title>Open design</Title>
            <Synopsis>Talk about need for openly designed hardware and software and documentation (+ preservation of that documentation)</Synopsis>
            <Text>[Talk about need for openly designed hardware and software and documentation (+ preservation of that documentation)]
</Text>
        </Document>
        <Document ID="76">
            <Title>Threats to digital continuity</Title>
        </Document>
        <Document ID="81">
            <Title>Choosing a method</Title>
        </Document>
        <Document ID="124">
            <Title>Reasons for preserving</Title>
            <Text>Richard Banks, a researcher at Microsoft Cambridge, suggests that we first preserve artefacts for ourselves. The majority of the time, we simply keep what we consider significant to remind ourselves of particular people, events and places, and to share them with people we care about; we discard what is not meaningful to us by pragmatism (lack of physical space, lack of importance, redundancy) (Banks, 2011, p. 6). This selection process contributes to keeping what we want to remember, and what is the legacy we will want to leave to others after our deaths.
However, the digital age changes this situation. First, the space to store our objects on computers and online is becoming virtually unlimited, and so we tend to accumulate data instead of selecting (Banks, 2011); second, the abundance of online services makes it harder to keep track of everything we create.</Text>
        </Document>
        <Document ID="77">
            <Title>Who should preserve (?)</Title>
            <Synopsis>Explain why stakeholders should be the ones preserving and not rely on external services</Synopsis>
            <Notes>Parrallel between generic &amp; personal stuff, that is museums/archives vs personal storage
</Notes>
        </Document>
        <Document ID="82">
            <Title>New contexts for personal archiving</Title>
            <Synopsis>It is interesting to ask ourselves if the paradigms for digital preservation used by librarians and archivists are still valid for preservation of our personal data. Harvey (2012) and Lukesh (1999, cited in Harvey, 2012, p. 32) note that our personal correspondence and sentimental ar- tefacts stored digitally might be lost if we do not take steps to preserve them, and wonder how we will be able to understand our modern life without these exchanges.
</Synopsis>
        </Document>
        <Document ID="78">
            <Title>How to preserve</Title>
        </Document>
        <Document ID="125">
            <Title>Growth of data</Title>
            <Text>The amount of data being produced digitally is growing  exponentially (SINTEF, 2013), and with this growth comes a greater volume of data worth archiving over the long term (Harvey, 2012, p. 33). However, the preservation of digital materials cause much more issues than traditional preservation paradigms; where we can consider that for paper and physical artefacts, “benign neglect may be the best treatment” (derived from Cloonan (1993, p.596), Harvey (1993, pp.14,140), and Bastian, Cloonan and Harvey (2011, pp.612–613)). </Text>
        </Document>
        <Document ID="83">
            <Title>Introduction copy</Title>
        </Document>
        <Document ID="79">
            <Title>Methods for software preservation</Title>
        </Document>
        <Document ID="84">
            <Title>Storage media</Title>
            <Synopsis>Harvey (1993, pp.14,140), and Bastian, Cloonan and Harvey (2011, pp.612–613)), this cannot be applied to digital storage media because their lifetime is much shorter. This varies highly with the type of medium used and with the storage conditions; for example, samples by the Digital Preservation Coalition (2008, p. 154) show that D3 cartridge tapes deteriorates after 50 years in optimal conditions (25% RH at 10o) but only one year in high humidity and temperatures (50%RH at 28o). Similar results happen for the more commercially available optical media (down to three months and up to 200 years), which are more sensitive to moisture (Harvey, 2012, p.47).</Synopsis>
            <Text>The most important threat to digital data as identified by UNESCO (2003, pp. 30–31) are the carriers used to store these materials. Most of them “are usually unstable and deteriorate within a few years or decades at most”. </Text>
        </Document>
        <Document ID="130">
            <Title>Geocities</Title>
            <Text>The Archive Team has notably rescued pages on GeoCities, a hosting site that was very popular in the mid- to late 90s. It was acquired by Yahoo! in 1999 (in what is now often referred to as one of the worst deals of the dot-com bubble) and suffered a great decline since, ultimately leading Yahoo! to close down Geocities in 2009.
Although often considered of little value by the modern Internet community (Scott, 2009) due to the tacky and amateur design of the websites (animated GIFs, excessive animations, auto-playing MIDI music…), Jason Scott and the Archive Team have decided to back it up and place mirrors of it online for cultural reasons:
But please recall, if you will, that for hundreds of thousands of people, this was their first website. This was where you went to get the chance to publish your ideas to the largest audience you might ever have dreamed of having. Your pet subject or conspiracy theory or collection of writings left the safe confines of your Windows 3.1 box and became something you could walk up to any internet-connected user, hand them the URL, and know they would be able to see your stuff. In full color. […] Already, little gems have shown up in the roughly 8000+ sites I’ve archived. Guitar tab archives. MP3s that surely took the owners hours to rip and generate. GIF files, untouched for 13 years. Fan fiction. Photographs and websites of people long dead. All stuff that, I think, down the line, will have meaning. It’s not for me to judge. It’s for me to collect. (Scott, 2009)
There is certainly a cultural aspect to the whole archive; the visual style of 90s websites, although generally considered of poor taste, has a cultural meaning and brings a certain form of nostalgia and reminiscence when we see it. In 2012 the Museum of the Moving Image, in New York, set up an installation of “Under Construction” GIFs found in the GeoCities archive, small animated symbols that were so commonplace on personal websites that they became a symbol of this era in computing history, a meaning obviously acquired with time. But more importantly, there are indeed memories, photographs, stories, work shared by people 15 to 20 years ago that would otherwise be lost forever without this initiative.</Text>
            <Comments>http://blogs.marketwatch.com/cody/2012/04/17/the-three-worst-timed-deals-of-the-dot-com-bust/

http://fortune.com/2013/05/21/5-worst-internet-acquisitions-of-all-time/ [Accessed 21st Oct. 2014]
http://www.movingimage.us/exhibitions/2012/12/04/detail/under-construction/ [Accessed 21st Oct. 2014]
Include figure/example - see http://www.textfiles.com/underconstruction/</Comments>
        </Document>
        <Document ID="85">
            <Title>Access of media</Title>
            <Synopsis>ven if storage media could survive decades or even centuries, technological change means we will not necessarily have the means to access the data later. In other words, “to retain appropriate access to a digital object, we must maintain a mechanism to derive meaning from that object” (del Pozo et al., 2012, p6): for optical media, this includes not only the drives to read the disc, but also a cable to connect the drive to the computer’s motherboard, a motherboard that has a connector for this type of cable, and an operating system that has drivers for this connectivity (Pearson 2009, slide 11). Once we have access to the bitstream from the computer we need to be able to represent it, which means we also need to keep the software in a version that can open the preserved file. There is a myth, kept by both customers and vendors, that media is long-lived: “[An IT vendor] offered us a special polymer that they guaranteed would preserve a CD-ROM for 100 years. They were unable to answer how we would preserve a CD-ROM player for that length of time.” (Weathley, 2003, cited by Harvey, 2012, p. 37)</Synopsis>
            <Text>“Use of digital materials depends on means of access that work in particular ways: often complex combinations of tools including hardware and software, which typically become obsolete within a few years and are replaced with new tools that work differently” (UNESCO, 2003). Even if storage media could survive decades or even centuries, technological change means we will not necessarily have the means to access the data later. In other words, “to retain appropriate access to a digital object, we must maintain a mechanism to derive meaning from that object” (del Pozo et al., 2012, p6): for optical media, this includes not only the drives to read the disc, but also a cable to connect the drive to the computer’s motherboard, a motherboard that has a connector for this type of cable, and an operating system that has drivers for this connectivity (Pearson 2009, slide 11). Once we have access to the bitstream from the computer we need to be able to represent it, which means we also need to keep the software in a version that can open the preserved file. There is a myth, kept by both customers and vendors, that media is long-lived: “[An IT vendor] offered us a special polymer that they guaranteed would preserve a CD-ROM for 100 years. They were unable to answer how we would preserve a CD-ROM player for that length of time.” (Weathley, 2003, cited by Harvey, 2012, p. 37)
</Text>
        </Document>
        <Document ID="126">
            <Title>MUSE example</Title>
            <Text>An attempt has been made by the MUSE (Memories USing Email) project (Hangal et al., 2011), which aims to find patterns in our emails archives to help us make sense of this large data set. As the authors note in the introduction:
“Email has become a de facto medium of record; many people consciously deposit important information into email, knowing they can look it up later, and thereby use their email account as an informal backup device. Therefore, email archives contain or reflect memories that are extremely valuable for the purposes of reminiscence.” (Hangal et al., 2011)
Early users reported that they have been using it to: make a summary of their work progress over the year; extract and organise certain type of data (for instance, the personal out of the professional, or the important out of the more mundane); finding life milestones inside family emails; picking up work that’s been left unfinished and forgotten; renewing with old relationships; and serendipitous discovery. The authors note in the conclusion that the different uses of their program went beyond their original expectations of simply reminiscing: “the stories above include an example of each one of the ‘5R’s’ described by Sellen and Whittaker (2010): recollection, reminiscing, retrieving, reflecting, and remembering intentions. Further, it suggests that browsing and remembering the past can affect the future.”</Text>
        </Document>
        <Document ID="90">
            <Title>Preservation conditions</Title>
            <Text>This varies highly with the type of medium used and with the storage conditions; for example, samples by the Digital Preservation Coalition (2008, p. 154) show that D3 cartridge tapes deteriorates after 50 years in optimal conditions (25% RH at 10º) but only one year in high humidity and temperatures (50%RH at 28º). Similar results happen for the more commercially available optical media (down to three months and up to a theoretical 200 years), which are more sensitive to moisture (Harvey, 2012, p.47).
</Text>
        </Document>
        <Document ID="86">
            <Title>Software</Title>
            <Synopsis>“Access barriers such as password protection, encryption, security devices, or hard-coded access paths may prevent ongoing access beyond the very limited circumstances for which they were designed”

need to keep crawlers up to date if archiving functions are not provided
</Synopsis>
        </Document>
        <Document ID="131">
            <Title>How the research is made</Title>
            <Text>Primary research
	•	Questionnaires
	•	Interviews
	•	Workshops
	•	Through practice - prototypes alternative ways of visualising memories, and see how it affects people. Focus on something over the whole project (visualisation/time representation, object obsolescence and open design, relationship with family and death)</Text>
        </Document>
        <Document ID="127">
            <Title>Copying/backup</Title>
            <Text>The second advantage, the ability to accurately and easily make copies and backups, is fairly obviously an advantage for personal preservation. However, Banks (2011) notes, the process of backing up is troublesome and storage media do not make our data immune. Additionally, the use of online services to store our data creates an abstraction between the storage material and us; about Flickr: “I have no idea what kind of hardware my files are now stored or even where they are geographically. I just expect to have access to them as long as I pay my bills” (Banks, 2011, p. 27).</Text>
        </Document>
        <Document ID="91">
            <Title>Natural disasters</Title>
            <Synopsis>“Materials may be lost in the event of disasters such as fire, flood, equipment failure, or virus or direct attack that disables stored data and operating systems”

src? + same if you store it locally, need for backups in different locations
</Synopsis>
            <Text>[Natural disasters can occur to any kind of artefact and digital have advantage there because of easy backup + datacenters better protected?]</Text>
        </Document>
        <Document ID="87">
            <Title>Patents</Title>
        </Document>
        <Document ID="88">
            <Title>First draft</Title>
            <Text>
# Introduction

As we shift into the Information Age and a knowledge-based economy, a number of concerns are being raised over the long-term preservation of digital data and artifacts. This literature revim
ew aims to get an overview of the current problems, techniques and practices in preserving data on digital media, including the definition and understanding of the new paradigms created by digital preservation, the importance of being able to store things on the long term, the processes used to curate and select content to be preserved, and the various methods used to do so. It also encompasses legal and ethical issues arising from it,  and how the paradigms and methods change when applied to different types of archiving (e.g. for researchers, for businesses, and for personal businesses).

# Why digital preservation
[Stats of numbers on digital vs physical]

## Why digital degrades faster
### 	UNESCO’s Threats to digital continuity
UNESCO (2003, pp. 30-31) has identified a list of threats to what they call *digital continuity*. These are:
+ “The carriers used to store these digital materials are usually unstable and deteriorate within a few years or decades at most”. Where, in the traditional preservation paradigms for paper-based artifacts, the main rule is that “benign neglect may be the best treatment” (derived from Cloonan (1993, p.596), Harvey (1993, pp.14,140), and Bastian, Cloonan and Harvey (2011, pp.612-613)),  this cannot be applied to digital storage media because their lifetime is much shorter. This varies highly with the type of medium used and with the storage conditions; for example, samples by the Digital Preservation Coalition (2008, p. 154) show that D3 cartridge tapes deteriorates after 50 years in optimal conditions (25% RH at 10º) but only one year in high humidity and temperatures (50%RH at 28º). Similar results happen for the more commercially available optical media (down to three months and up to 200 years), which are more sensitive to moisture (Harvey, 2012, p.47).

+ “Use of digital materials depends on means of access that work in particular ways: often complex combinations of tools including hardware and software, which typically become obsolete within a few years and are replaced with new tools that work diﬀerently” (UNESCO, 2003). Even if storage media could survive decades or even centuries, technological change means we will not necessarily have the means to access the data later. In other words, “to retain appropriate access to a digital object, we must maintain a mechanism to derive meaning from that object” (del Pozo et al., 2012, p6): for optical media, this includes not only the drives to read the disc, but also a cable to connect the drive to the computer’s motherboard, a motherboard that has a connector for this type of cable, and an operating system that has drivers for this connectivity (Pearson 2009, slide 11). Once we have access to the bitstream from the computer we need to be able to represent it, which means we also need to keep the software in a version that can open the preserved file.
There is a myth, kept by both customers and vendors, that media is long-lived: “[An IT vendor] offered us a special polymer that they guaranteed would preserve a CD-ROM for 100 years. They were unable to answer how we would preserve a CD-ROM player for that length of time.” (Weathley, 2003, cited by Harvey, 2012, p. 37)

+ Poor preservation documentation and metadata means it could be hard for potential users to find the data (UNESCO 2003)
+ + poor preservation (“Critical aspects of functionality, such as formatting of documents or the rules by which databases operate, may not be recognised and may be discarded or damaged in preservation processing.”, UNESCO 2003) + “So much contextual information may be lost that the materials themselves are un- intelligible or not trusted even when they can be accessed” -&gt; poor integrity

+ “Materials may be lost in the event of disasters such as fire, flood, equipment failure, or virus or direct attack that disables stored data and operating systems”
+ “Access barriers such as password protection, encryption, security devices, or hard-coded access paths may prevent ongoing access beyond the very limited circumstances for which they were designed”. [This argument is particularly true for web archiving, where crawlers of national libraries retrieve all sort of files to preserve them but do not necessarily have the permission or know the rights owner] [back that up maybe yo]

## Importance of data

### Library material: for society

&gt; “if we librarians do not rise to the occasion, successive generations will know less and have access to less for the first time in human history. This is not a challenge from which we can shrink or a mission in which we can fail” (Gorman, 1997, cited in Harvey, 2012, p. 27)

### For businesses (records)
### Personal archiving / domestic archiving
### Preserving interactive material inc. video games

In addition to the preservation of “static” documents and images, there is a significant desire to preserve video games, which are getting recognised as a form of art similarly to movies and music. The Library of Congress (2007) gave awards for the preservation of video games; there is a growing interest in being able to play “retro” games, but the consoles are obsolete. A large community of hobbyists reverse engineered consoles and produced emulators of these platforms for PC and other platforms different from their intended one. Because no documentation is available, this is a Herculean task; byuu (2011), the developer of an emulator for Super Nintendo games, recalls this for reengineering certain cartridges:

&gt; [Low-level emulation] is also a very expensive operation, monetarily speaking: to obtain the DSP program code requires melting the integrated circuit with nitric acid, scanning in the surface of a chip with an electron microscope, and then either staining and manually reading out or physically altering and monitoring the traces to extract the program and data ROMs. This kind of work can cost up to millions of dollars to have done professionally, depending upon the chip's complexity, due to the extremely specialized knowledge and equipment involved.  (byuu, 2011)

Obviously this is not necessarily a route many institutions would like to take for preserving digital objects. Because this also causes intellectual property issues, Nintendo and Sony have launched their own preservation programs, allowing players to play “classic” games on the newer generations of consoles; this is a far more accurate emulation (as they do have the internal documentation to create these emulators) and respects their monetary interests (players have to buy the games, whereas in hobbyist emulation, you would have to download the game file).

### Useful for researchers in the future

Beyond the governmental and cultural needs for preservation, there is also great need for preserving scientific data. “Data are the foundation on which scientific, engineering, and medical knowledge is built” (Committee on Ensuring the Utility and Integrity of Research Data in a Digital Age, 2009, p. ix, cited in Harvey, 2012, p. 26); therefore it is crucial in a digital era, with enormous data sets and research papers becoming widely public, to ensure this doesn’t disappear.

#### Understanding our history

There is a need to define what is the purpose of archives. As a tool to learn from the past, or to keep records, but as Cook (2000) eloquently puts it:
&gt; “Archives […] are a source of memory about the past, about history, heritage, and culture, about personal roots and family connections, about who we are as human beings and about glimpses into our common humanity through recorded information in all media, much more than they are about narrow accountabilities or administrative continuity.” (Cook, 2000)

Preservation should, therefore, be considered very important. There is some skepticism to this:

&gt; “It was suggested in 2002 that the last 25 years have been a ‘scenario of data loss and poor records that has dogged our progress’ and that, if this is not reversed, ‘the human record of the early 21st century may be unreadable’ (Deegan and Tanner, 2002).
&gt; Rhetoric of this kind is common in the literature, but is regrettably poorly supported with specifics and evidence. Alarmist descriptions abound: there will be ‘a digital black hole… truly a digital dark age from which information may never reappear’ (Deegan and Taylor, 2002) if we do not address the problem, and we will become an amnesiac society. 
&gt;[…]
&gt; It is only possible to conclude, as the authors of the Digital Preservation Coalition’s handbook (2008, p.32) did for the UK, that the evidence of data loss is ‘as yet only largely anecdotal ... [but] it is certain that many potentially valuable digital materials have already been lost’.” (Harvey, 2012, pp. 33-34)

While it’s difficult to argue that preserving digital objects is necessary, there is certainly a need to define what deserves to be protected, and to what level.

#### Understanding the beginnings of the Internet, as we have shifted in an information society
##### Preserving an accurate + [integrity] image of our society at a given point

###### Twitter &amp; Usenet

Camille Paloque-Bergès (2011; 2013) proposes that we can already find historical and sociological evidence of a given period in Internet time when browsing archives of Usenet [footnote: An Internet discussion system established in the 1980s and that is widely regarded as the predecessor of the Web and online fora]. She analysed how the way of speaking has evolved [footnote: See also http://wwwtxt.org (or @wwwtxt on Twitter), a project collecting short phrases of the Usenet to demonstrate this] and how an self-organised community had built itself, noting that we can “feel [the community’s] informational generativity” and understand that one of the biggest qualities of the early users of the Internet was their “recursivity as a public” (Paloque-Bergès, 2013); that is, how they welcomed newcomers — notably through the use of FAQs [footnote: Frequently Asked Questions], which weren’t a common concept back then. There are also large elements of nostalgia coming back (Paloque-Bergès, 2011).

It is also interesting to come back on the archival of Usenet. The messages were not systematically archived, and obviously not available on the Web as it did not exist and is separate. Today’s collections have mainly been recreated from donations of people who did archived Usenet newsgroup, originally to DejaVu News in 1995, and Google Groups in 2001.
A number of privacy concerns were raised with the publication of these searchable archives; messages who were, 15 to 20 years earlier, thought to be confidential are now considered as digital heritage. There was also a large backlash against the archiving made by for-profit companies, and the credit taken later by Google for its donors, with other critiques like Brad Templeton’s (an Internet veteran and important figure; cited in Paloque-Berges, 2013)  remark that the archive was lacking in integrity. However, the searchability of these archives did bring welcomed technical possibilities for an Internet historiography, including a timeline where “the announcement of the Chernobyl disaster on Usenet stands alongside the first message of Linus Torvalds calling for help on what would become the Linux operating system” (Paloque-Bergès, 2013). 

Similar concerns appeared at the announcement of the archival of Twitter by the Library of Congress in 2010. The two are not really comparable, mostly because the archive is not public and is made by a not-for-profit body. On the mailing list of the Association of Internet Researchers, the idea was met with interest: “The public twitter stream is of historical cultural significance and is an amazing repository of mundane moments in the daily lives of many people and records of what they thought important” (Baym, 2010) [on http://listserv.aoir.org/pipermail/air-l-aoir.org/2010-April/021125.html]”. However, the privacy concerns (notably with public and private feeds) were discussed, and as Michael Zimmer remarked:
&gt; “This is the classic “but the information is already public” argument that, while technically true, presumes a false dichotomy that information is either strictly public or private, ignoring any contextual norms that might have guided the initial release of information or how a person expects that information to flow.” (Zimmer, M. [on http://listserv.aoir.org/pipermail/air-l-aoir.org/2010-April/021136.html]; see also his paper (2010) on Facebook privacy, going further in that argument) 

Paloque-Bergès (2010) theorises this argument, suggesting that “information is not an object, but a statement; it goes through socio-technical processes; it doesn’t have an objective nature”. A message in a tweet can have a very different context when taken apart of the moment it was posted. It also emphasises that privacy could change (the user could, later, switch their account to Private, delete the tweet or their account altogether, but this wouldn’t be reflected in the archive. We cannot know what is the user’s original intent when they posted their message, and, as Paloque-Bergès (2010) noted, we aren’t sure the user themselves know.
&gt; “More often than not, in a technical ecosystem in which making content private is more difficult than sharing broadly, teens choose to share, even if doing so creates the impression that they have given up on privacy. It’s not that every teen is desperate for widespread attention; plenty simply see no reason to take the effort to minimize the visibility of their photos and conversations.” (Boyd, 2014).

## Advantages over physical preservation
### Mass preservation, easier access and organisation

### Recording digitally material that’s not born-digital
## Cost of restoration; case studies of lost data

A number of institutions have already experienced loss of data, or data being corrupted. This can have disastrous consequences for businesses, for example, as noted by McAteer (1996, cited by Ross and Gow, 1999, p. iii): “of the 350 companies unexpectedly relocated by the [1993] World Trade Centre (NYC) bombing 150 [43%] ceased trading,
many because they lost access to key business records held in electronic form”. 

Data can, obviously, be sometimes restored. Ross and Gow (1999, pp. 39-42, referenced in Harvey, 2012, p. 34) provide a number of case studies. Harvey (2012, p. 34) also gives the ironic example of the website for the “Functional Requirements for Evidence in Recordkeeping” project, directed by the University of Pittsburgh which was accidentally deleted; but thankfully was automatically saved by the Internet Archive.

There is still the possibility of restoring data by specialised carriers; however Pearson (2009, slide 15) notes that while this was generally effective, this did not work on some items. He also regrets that the restoration loses all metadata in the process; this can be an issue for sensitive or classified information (for business or governmental archives) as the carrier would be able to see it; finally, it is a prohibitely expensive operation, and so a carefully planned preservation could be much cheaper (and obviously more reliable) than neglect.
However, these services are always useful in certain cases. Harvey (2012, p. 41) cites the statistics by the data recovery giant Kroll Ontrack (2011): “hardware and system problems cause most customers to approach them (29 per cent), followed by human error (27 per cent). Software corruption or program problems, computer viruses and natural disasters are the other reasons listed.”

# Defining paradigms for digital preservation
### Pre-digital paradigms
### Inability to preserve the medium, only the bitstream
### Importance of metadata, relationship &amp; context
### Definition of terms		
		
# What to preserve / Selectivity
## Harvey 5 pillars for selectivity (Harvey 2012, p. 26)
* If **unique information** objects that are vulnerable and sensitive and therefore subject to risks can be preserved and protected
* If preservation ensures long-term accessibility for researchers and the public;
* If preservation fosters the accountability of governments and organisations
* If there is an economic or societal advantage in re-using information
* If there is a legal requirement to keep it (NSF-DELOS Working Group on Digital Archiving and Preservation, 2003, p.3).

### Banks reasons for domestic archiving
In *The Future of Looking Back*, Richard Banks (2011) analyses the reasons that lead us to keep or discard physical objects at home. He argues that while making a selection is a positive thing because it forces us to filter our belongings and focus on what is really meaningful, we also run the risk of ridding ourselves of things that we wish we would have kept in retrospect. He points out that the organisation of digital files make this trickier: because we we can keep pretty much everything on virtual space, it is easier to accidentally delete meaningful things alongside the less important. (Banks, 2011 p. 22).

In the context of personal archiving, Banks notes that in the past, the legacy we leave behind us was a highly selective one. Because our possessions are limited by physical space, our ancestors until now “have implicitly indicated that an item is significant through the simple act of keeping it. Although thousands of objects likely crossed their path over their lifetime, the constraints of money and space meant that only a few were retained, often the most significant.” (Banks, 2011, p. 78). He does however argue that this is no longer true in a digital world, because new possibilities mean we simply accumulate digital objects without necessarily ordering them by importance.

### Technical possibility of archiving

### Pros/cons of archiving the entire internet

There are various controversies around whether we should automate the archiving of the Internet, or if we should operate a manual selection. UNESCO (2003, p. 73) notes that those who value a more comprehensive collection (i.e. archiving “all” the Internet) argue that “any information may turn out to have long-term value, and that the costs of detailed selection are greater than the costs of collecting and storing everything”. This argument is particularly true because of the digital media: “unlike non-digital material such as paper-based artifacts, where there is a period of time in which to make selection decisions before deterioration of materials becomes an issue, the time frame for deciding whether or not to preserve digital materials is very short” (Harvey, 2012, p. 57). UNESCO (2003, p.71) backs that claim: “it may not be possible to wait for evidence of enduring value to emerge before making selection decisions”.

Other people [Cook?] think that it would be preferable to keep a curated collection because of a higher technical quality, and this also allow us to settle [negotiate?] preservation rights with the content producers. Harvey adds that we also do not have the technical capabilities to find all the pages on the Internet, and even if we had we would not have the technical possibility to store the whole of the Internet (Harvey 2012, p. 58); again, this is also something recognised by UNESCO: “‘there are usually more things – more information, more records, more publications, more data – than we have the means to keep” (2003, p.70).

The UNESCO (2003) and Cook (2000) both however recognise that a middle ground should be found and that both approaches are valid depending on the goal of the preservation. For instance, the Internet Archive[IA] is going through an automated process (archive as many pages as possible) but, because of technical limitation, only preserves the text and dismisses all other images, stylesheets and external media, which could be critical in some instances. It seems evident that for valued documents these should be retained. The British Library’s Web Archive[ukwa] [http://www.webarchive.org.uk/ukwa/] 

###	Dangers of selection (Cook)
However, Cook (2000) points out the dangers of manual selection for what is worth archiving: “With memory comes the inevitable privileging of certain records and records creators, and the marginalizing or silencing of others” (Cook, 2000, cited in Harvey p. 58). He asserts that the archivists must be impartial and objective, and ensure all points of view are kept to ensure our shared memory does not “becomes counterfeit, or at least transformed into forgery, manipulation, or imagination”. This is something that is clear with manual archiving, but could also be a problem, maybe unintentionally, with the automated discovery of content when archiving Web pages.
		
### Automatic selection (Reputation model)
In order to keep the most valuable content, algorithms could be used. Kawano (2008) proposes to use reputation models, that attempts to determine the relative value of a Web page by considering its “importance, fairness, trustiness, uniqueness and valuation”. This is done using a variety of ways; the algorithm suggested by Kawano relies on the consistency of a web page, that is the amount of time it existed, the number of links it contains and the number of external links to this page  (Kawano, 2008, p. 291).

#### Crawling
#### Manual curating
	
### Special Collections for events (eg before elections, see webarchive.org.uk for other ideas)
Another interesting selection process that has emerged is the one of *specific archiving* when certain events are happening. For instance, the UK Web Archive[ukwa] has, on top of their topical-based collections (Energy, Health and Social Care…), a few event-based collections (Queen Jubilee, Olympic and Paralympic Games, London 2005 terrorist attacks…) which reflect the state of the Web at that given moment in history. Masanès (2006, p.83) notes that this type of archiving must be manually curated and thoroughly preparated. He gives a methodology for this, using presidential elections as an example: a time frame for the collection must be defined (“3 months before the election + 1 month after”), what type of sites will be archived and when (“the political parties’ blogs and websites, every week; analysis, commentary and humourous/satire websites, every month; online newspaper articles, once”).

[Opposing to automated archiving]: Masanès (2006) also describes the technique used for crawling a set of websites automatically. For example, when attempting to save all sites located in France for the French National Library, the robots were instructed to start navigating from 12 existing directories of websites, and restrict what they save to sites with a .fr domain name, or, for more generic top-level domain names (.com, .net, .org…), check if the phone number used for DNS[footnote explanation] registration is a French number (starting with +33). (Masanès 2006, p. 83)

### Selection: by genre [web]
There are different genres of websites: institutional website, blogs, personal pages, forums. [[Genre have been studied in the context of the Web to see how they replicate or diverge from the genre in the print- ing world (see Crowston and Williams 1997) - in Masanès p88]]. An interesting point about this is that often, robots (crawlers, or later data analysis) can automatically identify their type [Rehm 2002 -&gt; check reference, Masanès:88] which permits the automatic generation of metadata, or simply to restrict a collection of web pages to certain Web genres.

# Web archiving: what stands as heritage on the web

# Who should preserve

Digital media changes the original preservation paradigm for deciding who takes the responsibility for archiving content, and how. The processes of creation, fabrication and publication used to be distinct in the “analog” era; but as Nurnberg (1995, p. 21, cited in Harvey, 2012, p. 9) said, “technology tends to erase distinctions between the separate processes of creation, reproduction and distribution that characterize the classic industrial model of print commodities”. Self-publication (particularly on websites) is much more common, and this is also true for businesses and personal data.
The Task Force on Archiving of Digital Information (1996, pp. 19-20) suggests that when, in general, “stakeholders disseminate, use, reuse, recreate and re-disseminate various kinds of digital information, they can easily, even inadvertently, destroy valuable information, corrupt the cultural record, [and ultimately thwart the pursuit of knowledge that is their common end]. Against such a danger, a safety net is needed to ensure that digital information objects with long-term cultural and intellectual value survive the expressions of stakeholder interest with their integrity intact.

[&gt; Society has always created objects and records describing its activities, and it has consciously preserved them in a permanent way… Cultural institutions are recognised custodians of this collective memory: archives, librar[ies] and museums play a vital role in organizing, preserving and providing access to the cultural, intellectual and historical resources of society. They have established formal preservation programs for traditional materials and they understand how to safeguard both the contextual circumstances and the authenticity and integrity of the objects and information placed in their care… It is now evident that the computer has changed forever the way information is created, managed, archives and accessed, and that digital information is now an integral part of our cultural and intellectual heritage. However the institutions that have traditionally been responsible for preserving information now face major technical, organisational, resource, and legal challenges in taking on the preservation of digital holdings (B. Smith, 2002, pp.133-134) [cited in Harvey p. 29].]

A variety of new stakeholders are now required to collaborate more for a good preservation practice to take place. Lavoie and Dempsey (2004, cited in Harvey, 2012, p. 30) argues that cooperation “can enhance the productive capacity of a limited supply of digital preservation funds, by building shared resources, eliminating redundancies, and exploiting economies of scale”. Chapter 11 of the UNESCO guidelines (2003) give more pratical details on how this can take place, giving structural models and tips on what to share (information, standards, division of labour…). [CLOCKSS cooperation to preserve journals (see Harvey p32 + chapter 9); + Harvey p32 National Library of Netherlands &amp; Elsevier Science + most uni libraries access with publishers, even UoD with eg Dawsonera/ACM]

## Legal issues
### (see later) special case for emulation

In certain cases, the archiving process is not made in accordance with the original publisher. This is often the case for the automated archiving of the Internet (e.g. Wayback Machine), which automatically crawl pages, but as a website publisher, it is easily possible to opt out of these schemes [see http://archive.org/about/exclude.php [Accessed 17th Apr 2014]].
In the case of emulation, this is what happens with *reverse engineering*, that is, disassembling the original program to understand how it works and recreate a similar version which can operate from a different hardware or software architecture. This is a common process for proprietary software or hardware, whose source code is not available to the general public and no documentation is provided for developers. Often, this is developed by amateurs and hobbyists, but is illegal as it breaks the intellectual property of the original creator; Nintendo, for example, attacks reverse engineered emulators[http://www.nintendo.com/corp/legal.jsp [Accessed 17th Apr 2014]] and the sharing of ROMs [Read Only Memory; the bitstream of cartridges or discs containing the games]. There is however popular demand for playing these, and Nintendo has developed their own emulators to play legacy games on their new consoles (see the Virtual Console [http://en.wikipedia.org/wiki/Virtual_Console]), while still retaining their property rights (you need to buy the games again on an e-commerce platform). This however causes issue for the longer term preservation: there is no certainty that Nintendo will continue to provide this service on their future generations of consoles, or if they ever cease operations.



A similar issue happened with Apple Computer. In 2005, Apple changed the architecture (that is, the “language” of instructions needed for the processor) of their personal computers from PowerPC to Intel x86. This means that applications needed to be rewritten, sometimes from the ground up, to work natively on the newer Apple computers. To ease the transition, Apple created **Rosetta**, a software that dynamically translates PowerPC instructions to x86 instructions, so that buyers of the new Apple computers were still able to open legacy applications that were not yet rewritten for x86 — although this came with a significant performance hit. In 2009 however, support for PowerPC was entirely discontinued […]

## Libraries

Good curating and preservation starts early. Smith (2003, pp. 2-3, cited in Harvey, 2012, p. 32) suggests that “the critical dependency of preservation on good stewardship begins with the act of creation, and the creator has a decisive role in the longevity of the digital object”. Harvey (2012, p. 32) notes that “for most creators of information in any form this is a new role”; therefore publishers of digital content are not necessarily well prepared for preservation, and this is why collaboration with librarians and archivists can be helpful (Ayre and Muir, 2004). 

Ayre and Muir (2004) note that if the publisher worries about preservation themselves then they generally already own the rights for preservation, but there are negotiations to be done if an external body helps with preservation. 

## National libraries

Ayre and Muir (2004) suggest that the complexity and resources needed for digital preservation may mean that individual libraries may not have the possibility to do the preservation themselves like they used to, but instead rely on national libraries and centralised preservation systems to reduce overhead. This system is used by a number of libraries; even here at the University of Dundee, most journals and a selection of books are available through centralised systems (Dawsonera, Elsevier Science) or publishers directly (ACM).
A report by Hedstrom and Montgomery (1998, cited by Ayre and Muir, 2004) found that most research librarians in the UK thought that a legal deposit to national archives was the best option, but this was not a clear consensus. The report also highlighted that these librarians thought that “other libraries and publishers would need to, and should, have some involvement in preservation”.

Because all preservation methods involve copying of the original object, it might be illegal to make such a copy without the necessary rights, even if it is for preservation purposes. Certain countries have laws to overcome this and allow national libraries to preserve all materials; Ayre and Muir (2004) list a few of them and their specificities, but the situation has since evolved, as a lot of legislations have enacted changes to specifically include digital materials.

## Archivists
## New stakeholders
## Profits vs nonprofits

Harvey (2012) points out doubts in the real interests of for-profit companies to achieve long-term preservation;
&gt; Even given the prevailing market-driven political ethos, it is difficult to envisage a situation where market forces will be sufficient to ensure the preservation of this digital material. The opposite is more likely to apply: ‘in some cases market forces work against long-term preservation by locking customers into proprietary formats and systems’ (Workshop on Research Challenges in Digital Archiving and Long-term Preservation, 2003, pp.x-xi). (Harvey, 2012, p. 32). [link with Google concerns]

# How to preserve
## Software methods
### Making hard copies
What appears to be the simplest solution is to make physical copies (i.e. print) of the digital documents. Although this certainly solves some problems in regards to technological obsolescence, Rothenberg (1999) and Granger (2000) pointed out that this cannot be done for more interactive material besides text and images. There is also, obviously, an issue with the amount of content that can be stored in this way — this would not work for archiving websites. Granger (2000) recognises that “if one’s sole concern is with the intellectual content of a document and the document is of a fairly simple nature […] this at least provides some form of security”.

### Standardisation

Standardisation is the transformation of a document’s content to a format that is standardised, open and well documented, ensuring that we will be able to open it with a variety of applications on a variety of platforms and that we can rebuild a program for visualising that document — as opposed to vendor-specific, closed source program that could be discontinued at any point in the future. Rothenberg (1999, p. 10) claims that this is a bad solution because “standardisation sows the seeds of its own destruction by encouraging vendors to implement non-standard features in order to secure market share” (therefore these non-standard features will be lost if opened in a different application). Granger (2000) argues that the blame is not to be put on the method, but on vendors.
Standardisation is a type of migration and therefore share some of it cons (notably, this absence of vendor-specific features could imply a loss in the presentation and other characteristics). Standards eventually become obsolete so it is also paramount to preserve the specifications of the format, or to define formats that are unambiguous, such as CSV or XML.

A slightly different approach to this method is rebuilding viewers. In this scenario, the bitstream is not migrated to a standardised format, but kept as is; we simply re-build a viewer that is able to read that format. This is not as straightforward as it sounds, because most of the time, formats for proprietary software are not documented and decrypting them would require reverse engineering. If they are documented, then it is likely that this format is already a standard. There exists cases where reverse engineering is possible, or where a non-standard format is documented; for example, Microsoft Word files are not a standard, but there is plenty of documentation available online to produce code able to read them. An experiment by the VERS project (Thibodeau 2002, p. 22) has shown that it is possible to reengineer a viewer for the PDF file format from the specification available; as long as we also preserve these specifications, we will be able to read PDF files in the future.

#### UVC

Lorie (2000, in Thibodeau, 2002, p. 22) has also proposed that the use of a portable programming language, such as Java [footnote: Most programming languages include instructions that are specific to a given platform, therefore the same file will need to be adapted to run on another platform. “Portable” languages like Java are called high-level and are more abstract. They are run inside a virtual machine, which worries about transforming Java code into instructions that are specific to that platform; as such, the same file of code can be ran on any platform.]. Lorie developed a software called Universal Virtual Computer (UVC), able to decrypt a variety of file formats, which can run on any platform supporting Java; because Java could be implemented on virtually any architecture, this means that a reader could be run on any platform without any effort. However, Lorie points out, this approach is limited in terms of functionality, and comes at the price of performance (because portable programming languages are platform-agnostic, this means the code is not optimised for any of the platforms). I am also wondering if this would really work on the long term, as we cannot be sure that future implementations of Java will be backward compatible with code written for a previous version.

### Migration

Migration is the transformation of a document’s content to a format that is not necessarily standardised, but at least current; or, the newer version of same format. This could be, for example, transferring documents from the WordPerfect 6 format (used in the 80s and late 90s) to a Word 2011 format, or a Word 98 file to Word 2011. This should be done regularly, as after a certain period of time new programs will stop having backward compatibility to import these old formats (Thibodeau, 2002). This method has several downsides, mainly being that it is labour intensive and expensive (Thibodeau, 2002; Rothenberg, 1999); product lines, and therefore the migration path, could be terminated at any point (Thibodeau, 2002); the presentation and other characteristics could be changed (although this can be overcome by migrating to more flexible formats like PDF or LaTeX).
Rothenberg (1999) adds that this method is also prone to error, risky (as it could compromise the integrity of the data), not scalable and it requires new solutions for each new format. Granger (2000) argues however that comparatively, other methods (particularly emulation) are potentially much more expensive and labour intensive, and that for most preservation programmes, migration remains the only viable method.

### Computer museums

Another approach to preservation, proposed by Swade (1993), would be to preserve a variety of hardware, software and devices to access obsolete media in “computer museums”. This is generally not considered a good solution; Granger (2000) notes that it would be unreasonably expensive to keep old machines running for an extended period of time, that computers chips will eventually decay anyway (and repair or replacement will be impossible), and that storage media not stand the test of time. There are still two possible use cases, identified by Rothenberg (1999): ensuring that emulators work as expected (by comparing them to original platforms), and helping with the recovery of lost data on obsolete media

### Emulation

Emulation is the dynamic transformation of instruction sets to allow software and/or operating system designed for an obsolete architecture or platform to run on current systems. It is a highly complex method that offers certain benefits; notably, this is the most conservative method, as it is not destructive of the content or presentation. It allows us to run the original program used to create a digital object without altering that bitstream ; it “keeps the look and feel as well as the interactivity” (Granger, 2000) and as such, it is particularly interesting for objects rich in interaction, such as video games, data visualisations, or formats that are particularly complex (for example, medical imagery, 3D modelisations), for which methods like migration would be hard. It also ensures complete integrity and authenticity, which could be crucial for some data types.

There are numerous criticisms against emulation. A number of legal and intellectual property problems are raised, if the emulation strategy is not supported by the original author [see Nintendo &amp; Apple]. The complexity involved in creating emulators might be not worth it and requires a coherent organisation to reduce overhead (Granger 2000). Additionally, Thibodeau (2002, p. 21) argues that keeping the original functionality is not necessarily a positive thing when it comes to the delivery of the preserved content; “most users in the future will never have encountered—not to mention learned how to use—most of the products they will need to access the preserved information objects”, and “it would cut users off from the possibility of using more advanced technologies for discovery, delivery, and analysis”.

&gt; “Emulators themselves become obsolete; therefore, it becomes necessary either to replace the old em- ulator with a new one or to create a new emulator that allows the old emulator to work on new platforms. In fact, if you get into an emula- tion strategy, you have bought into a migration strategy. Either strategy adds complexity over time.” (thibodeau 2002 p.20)

[NB: due to the length limit of this literature review, I have only explained the most popular archiving techniques. See Thibodeau (2002) and Harvey (2012) for a more comprehensive list of methods, including configurable chips, persistent archives, object interchange format, etc.]

## Choosing one

Thibodeau (2002, pp. 15-16) suggests four criteria to choose a preservation method: *feasibility*, *sustainability*, *practicality* and *appropriateness*. 

Feasability means that the hardware and the software for implementing a given method must be existing and developed. 
Sustainability is ensuring that this method can be “applied indefinitely into the future, or that there are credible grounds for asserting that another path will offer a logical sequel to the method, should it cease being sustainable”. This means that the method must be resistant to technological obsolescence, both for storing the data (medium will change) and accessing it (means to access the media ane decode the bitstream will change).

Practicality is establishing that this method will be reasonably difficult and affordable. [This can generally change depending on the resources of the preserving organisation, the available technology at that time and the valuation of the digital objects.]

Finally, appropriateness is using a method that is relevant to the type of material being preserved, and the objectives of the preservation. For example, del Pozo et al. (2012, p. 7) suggests that if the integrity and authenticity of the documents are not paramount, then a migration that keeps the contents but discards the presentation might be the least expensive and simplest option. Inversely, when trying to preserve highly interactive material such as video games or when the specifications for a format are unknown, then emulation is a more pertinent option, because migration would be extremely difficult and inaccurate (Granger, 2000).

### Authenticity/integrity and issues with each method

Different preservation methods yield different results in what is called **authenticity**, that is how faithful the rendition [its presentation] of the preserved object will be. Authenticity is subjective and therefore there is debate around this issue (Del Pozo et al., 2012, p. 7; Cook, 2000); however most agree that while as little difference in the presentation is desirable, it is rarely (if ever) possible to keep a truthful rendition. As an example, del Pozo et al. (2012, p. 8) use a spreadsheet within the migration method:
&gt; “For instance, one institution may decide to normalise a spreadsheet into a PDF, which would retain the cell values but destroy any formula used to calculate them. This would favour a particular presentation of that digital object over the information form of the original. On the other hand, another institution might decide to normalise spreadsheets into ODF, which may lose some formatting but retain the formula used to derive each cell. In this case, being able to retain the information form of the file would be seen as more important.”

Thibodeau (2002, p. 28) however points out that “people will want to use the best available technology—or at least technologies they know how to use—for discovery, retrieval, processing, and delivery of preserved information. There is a danger that to the degree that preservation solutions keep things unaltered they will create barriers to satisfying this basic user requirement.” An overly faithful preservation would therefore not be desirable. Again, this is subject to a high number of parameters.

Another important issue is *integrity*, i.e. “ensuring the object is complete and has not been altered or corrupted” (Harvey, 2012, p. 54; Ross [CITED], 2002, p. 7); that is, the object “is what it purports to be”, and no parts are missing from it. This is particularly important, Harvey (2012, p. 54) notes, for business archiving, where the legal value of a digital object could be lost if we cannot demonstrate that this object hasn’t been tampered with, or that a migration processes have not incurred data losses.
However, as Lynch (2000) has argued, the integrity of objects is only a matter of trust. The guidelines of UNESCO (2003, p. 22) recommend that the best protection for authenticity is proper documentation of the archival process; Harvey (2012, p. 54) expands on this idea: “for instance, if it can be established that a digital object has always been kept in the custody of an archive in which every change to it has been recorded (such as changes resulting from migration of the bit-stream – the dates at which migration occurred, to which media, and so on), then we can be more secure about its integrity.” 

### Emulation / legal rights with videogames



## Use of standards

Some standards such as OAIS (Open Archival Information System), originally developed for space science, could be used. Sadly [no implementation] [source??]

# Personal archiving

On a more personal level, Harvey (2012) and Lukesh (1999) note that personal correspondance might be lost if we do not take steps to preserve them:

&gt; “The widespread shift from writing letters to the use of email has diminished the likelihood that personal correspondence will remain accessible to future historians. Lukesh asked in 1999, ‘Where will our understandings of today and, more critically, the next century be, if this rich source of information is no longer available?’, as scientists, scholars, historians and almost everyone increasingly use email” (Lukesh, 1999) (in Harvey, 2012 p. 32)

While this is a correct idea, I think that, just like the archiving of Usenet, few people kept their letters in the sole purpose of leaving researchers decades from now understand our epoch’s social habits and ways of speaking. There has to be other motivations for preserving our conversations and our objects.

Richard Banks, a researcher at Microsoft Cambridge, suggests that we first preserve artifacts for ourselves. The majority of the time, we simply keep what we consider significant to remind ourselves of particular people, events and places, and to share them with people we care about; we discard what is not meaningful to us by pragmatism (lack of physical space, lack of importance, redundance) (Banks, 2011, p. 6). This selection process contributes to keeping what we want to remember, and what is the legacy we will want to leave to others after our deaths.
However, the digital age changes this situation. First, the space to store our objects on computers and online is becoming virtually unlimited; second, storing photos and data on online services mean we may forget about them in a few years (Banks, 2011).

## What happens after our death
### [Digital heritage]
## Shifts in usages (more photos)
## Advantages (sharing, organisation etc.); record part of our lives we never did before

Digital artifacts have certain fundamental advantages over physical objects: the main two being the ease of organising a collection of data through indexing, cataloguing and adding metadata (Kirk and Sellen, 2010, p. 35), and the ability to duplicate and share that object accurately and virtually cost-free (Banks, 2011). This actually provides numerous advantages in terms of preservation. 

[[+ tie with “In the past ancestors have signified something is important simply by keeping it” part ]]

The first one, ease of organisation, can be significantly helpful in tackling the abundance of digital data. Banks (2011) suggests that the technologies lying behind the ranking in the result of Web search engines could, eventually, help us in understanding and finding what is our most important data. An attempt has been made by the MUSE (Memories USing Email) project (Hangal et al., 2011), which aims to find patterns in our emails archives to help us make sense of this large data set. As the authors note in the introduction:

&gt; “Email has become a de facto medium of record; many people consciously deposit important information into email, knowing they can look it up later, and thereby use their email account as an informal backup device. Therefore, email archives contain or reflect memories that are extremely valuable for the purposes of reminiscence.” (Hangal et al., 2011)

Early users reported that they have been using it to: make a summary of their work progress over the year; extract and organise certain type of data (for instance, the personal out of the professional, or the important out of the more mundane); finding life milestones inside family emails; picking up work that’s been left unfinished and forgotten; renewing with old relationships; and serendipitous discovery. The authors note in the conclusion that the different uses of their program went beyond their original expectations of simply reminiscing: “the stories above include an example of each one of the ‘5R’s’ described by Sellen and Whittaker [2010]: recollection, reminiscing, retrieving, reflecting, and remembering intentions. Further, it suggests that browsing and remembering the past can affect the future.”

The second advantage, the ability to accurately and easily make copies and backups, is fairly obviously an advantage for personal preservation. However, Banks (2011) notes, the process of backing up is troublesome and storage media do not make our data immune. Additionally, the use of online services to store our data creates an abstraction between the storage material and us; about Flickr: “I have no idea what kind of hardware my files are now stored or even where they are geographically. I just expect to have access to them as long as I pay my bills” (Banks, 2011, p. 27).

## Keeping memories attached to digital objects

Banks (2011) also suggests that we could blend in the physical and the digital to allow recording of things we never had the opportunity to before. He gives as example the Weather Camera, an experimental object by Kjen Wilkens (2011) that records wind and temperature along with commentary, instead of an image, as an alternative to recall a place; or 3D backups by Héctor Serrano (2009), another experimental project that allows us to make backups of sentimentally important objects through 3D scanning and 3D printing. 
Similarly, Banks notes tools by Microsoft Research to recreate places in a 3D environment based on photos, allowing to recreate a sentimentally important place at any given point in time provided enough photos are available. A similar technique is used in bigger preservation projects, as described by Zhou et al. (2012), to make high quality 3D models of culturally important places and objects in China.

Kirk and Sellen (2010) found however that ultimately, stories and narrative are more important than the preservation of the artifact itself. They propose that “technology might play a role in capturing and associating stories or narratives with different physical objects”, imagining that we could augment physical objects with RFID tags (and I would add, virtual reality technologies) to pass down stories attached to these objects.

Conclusion</Text>
        </Document>
        <Document ID="10">
            <Title>Appendices</Title>
            <Synopsis>“Be honest and rigorous”
Only contain ALL raw data/transcripts, not analysis

Questions asked
Provide on disc if long audio/video</Synopsis>
        </Document>
        <Document ID="132">
            <Title>Obstacles to digital heritage</Title>
            <Text>
Odom et al., 2012:
these instances highlighted tensions around integrating social networking content from members within the collective family archive. Participants made key distinctions between the thoughtful recording of one's life believed to be reflected in their ancestors' diaries, and their own practices of posting less mindful social networking content targeted at multiple audiences, often outside of the family.

+ Might change how you post online, if you know that people will look at it later

Related concerns also emerged around how a device like BackupBox could cause family members to self-censor the social networking content they posted, or paralyze these practices completely. Some families proposed ways to work around these tensions, such as using a special hash tag or a specific application to send updates only to Backup Box.
</Text>
        </Document>
        <Document ID="128">
            <Title>Indexing &amp; cataloguing</Title>
            <Text>The first one, ease of organisation, can be significantly helpful in tackling the abundance of digital data. Banks (2011) suggests that the technologies lying behind the ranking in the result of Web search engines could, eventually, help us in understanding and finding what is our most important data.</Text>
        </Document>
        <Document ID="92">
            <Title>Software barriers</Title>
        </Document>
        <Document ID="89">
            <Title>Long draft</Title>
            <Text>Introduction
As we shift into the Information Age and a knowledge-based economy, a number of concerns are being raised over the long-term preservation of digital data and artifacts. This literature review aims to get an overview of the current problems, techniques and practices in preserving data on digital media, including the definition and understanding of the new paradigms created by digital preservation, the importance of being able to store things on the long term, the processes used to curate and select content to be preserved, and the various methods used to do so. It also encompasses legal and ethical issues arising from it, and how the paradigms and methods change when applied to different types of archiving (e.g. for researchers, for businesses, and for personal businesses).
Why digital preservation
[Stats of numbers on digital vs physical]
Why digital degrades faster UNESCO’s Threats to digital continuity
UNESCO (2003, pp. 30–31) has identified a list of threats to what they call digital continuity. These are: + “The carriers used to store these digital materials are usually unstable and deteriorate within a few years or decades at most”. Where, in the traditional preservation paradigms for paper-based artifacts, the main rule is that “benign neglect may be the best treatment” (derived from Cloonan (1993, p.596), Harvey (1993, pp.14,140), and Bastian, Cloonan and Harvey (2011, pp.612–613)), this cannot be applied to digital storage media because their lifetime is much shorter. This varies highly with the type of medium used and with the storage conditions; for example, samples by the Digital Preservation Coalition (2008, p. 154) show that D3 cartridge tapes deteriorates after 50 years in optimal conditions (25% RH at 10o) but only one year in high humidity and temperatures (50%RH at 28o). Similar results happen for the more commercially available optical media (down to three months and up to 200 years), which are more sensitive to moisture (Harvey, 2012, p.47).
“Use of digital materials depends on means of access that work in
￼￼
particular ways: often complex combinations of tools including hardware and software, which typically become obsolete within a few years and are replaced with new tools that work differently” (UNESCO, 2003). Even if storage media could survive decades or even centuries, technological change means we will not necessarily have the means to access the data later. In other words, “to retain appropriate access to a digital object, we must maintain a mechanism to derive meaning from that object” (del Pozo et al., 2012, p6): for optical media, this includes not only the drives to read the disc, but also a cable to connect the drive to the computer’s motherboard, a motherboard that has a connector for this type of cable, and an operating system that has drivers for this connectivity (Pearson 2009, slide 11). Once we have access to the bitstream from the computer we need to be able to represent it, which means we also need to keep the software in a version that can open the preserved file. There is a myth, kept by both customers and vendors, that media is long-lived: “[An IT vendor] offered us a special polymer that they guaranteed would preserve a CD-ROM for 100 years. They were unable to answer how we would preserve a CD-ROM player for that length of time.” (Weathley, 2003, cited by Harvey, 2012, p. 37)
Poor preservation documentation and metadata means it could be hard for potential users to find the data (UNESCO 2003)
poor preservation (“Critical aspects of functionality, such as formatting of documents or the rules by which databases operate, may not be recognised and may be discarded or damaged in preservation processing.”, UNESCO 2003) + “So much contextual information may be lost that the materials themselves are un- intelligible or not trusted even when they can be accessed” -&gt; poor integrity
“Materials may be lost in the event of disasters such as fire, flood, equipment failure, or virus or direct attack that disables stored data and operating systems”
“Access barriers such as password protection, encryption, security devices, or hard-coded access paths may prevent ongoing access beyond the very limited circumstances for which they were designed”. [This argument is particularly true for web archiving, where crawlers of national libraries retrieve all sort of files to preserve them but do not
￼￼￼￼￼￼
necessarily have the permission or know the rights owner][back that up maybe yo]
Importance of data Library material: for society
“if we librarians do not rise to the occasion, successive generations will know less and have access to less for the first time in human history. This is not a challenge from which we can shrink or a mission in which we can fail” (Gorman, 1997, cited in Harvey, 2012, p. 27)
For businesses (records)
Personal archiving / domestic archiving
Preserving interactive material inc. video games
In addition to the preservation of “static” documents and images, there is a significant desire to preserve video games, which are getting recognised as a form of art similarly to movies and music. The Library of Congress (2007) gave awards for the preservation of video games; there is a growing interest in being able to play “retro” games, but the consoles are obsolete. A large community of hobbyists reverse engineered consoles and produced emulators of these platforms for PC and other platforms different from their intended one. Because no documentation is available, this is a Herculean task; byuu (2011), the developer of an emulator for Super Nintendo games, recalls this for reengineering certain cartridges:
[Low-level emulation] is also a very expensive operation, monetarily speaking: to obtain the DSP program code requires melting the integrated circuit with nitric acid, scanning in the surface of a chip with an electron microscope, and then either staining and manually reading out or physically altering and monitoring the traces to extract the program and data ROMs. This kind of work can cost up to millions of dollars to have done professionally, depending upon the chip’s complexity, due to the extremely specialized knowledge and equipment involved. (byuu, 2011)
Obviously this is not necessarily a route many institutions would like to take for preserving digital objects. Because this also causes intellectual
￼￼￼
property issues, Nintendo and Sony have launched their own preservation programs, allowing players to play “classic” games on the newer generations of consoles; this is a far more accurate emulation (as they do have the internal documentation to create these emulators) and respects their monetary interests (players have to buy the games, whereas in hobbyist emulation, you would have to download the game file).
Useful for researchers in the future
Beyond the governmental and cultural needs for preservation, there is also great need for preserving scientific data. “Data are the foundation on which scientific, engineering, and medical knowledge is built” (Committee on Ensuring the Utility and Integrity of Research Data in a Digital Age, 2009, p. ix, cited in Harvey, 2012, p. 26); therefore it is crucial in a digital era, with enormous data sets and research papers becoming widely public, to ensure this doesn’t disappear.
Understanding our history
There is a need to define what is the purpose of archives. As a tool to learn from the past, or to keep records, but as Cook (2000) eloquently puts it:
“Archives [...] are a source of memory about the past, about history, heritage, and culture, about personal roots and family connections, about who we are as human beings and about glimpses into our common humanity through recorded information in all media, much more than they are about narrow accountabilities or administrative continuity.” (Cook, 2000)
Preservation should, therefore, be considered very important. There is some skepticism to this:
￼￼
“It was suggested in 2002 that the last 25 years have been a ‘scenario of data loss and poor records that has dogged our progress’ and that, if this is not reversed, ‘the human record of the early 21st century may be unreadable’ (Deegan and Tanner, 2002). Rhetoric of this kind is common in the literature, but is regrettably poorly supported with specifics and evidence. Alarmist descriptions abound: there will be ‘a digital black hole... truly a digital dark age from which information may never reappear’ (Deegan and Taylor, 2002) if we do not address the problem, and we will become an amnesiac society. [...] It is only possible to conclude, as the authors of the Digital Preservation Coalition’s handbook (2008, p.32) did for the UK, that the evidence of data loss is ‘as yet only largely anecdotal ... [but] it is certain that many potentially valuable digital materials have already been lost’.” (Harvey, 2012, pp. 33–34)
While it’s difficult to argue that preserving digital objects is necessary, there is certainly a need to define what deserves to be protected, and to what level.
Understanding the beginnings of the Internet, as we have shifted in an information society
Preserving an accurate + [integrity] image of our society at a given point Twitter &amp; Usenet
Camille Paloque-Bergès (2011; 2013) proposes that we can already find historical and sociological evidence of a given period in Internet time when browsing archives of Usenet [footnote: An Internet discussion system established in the 1980s and that is widely regarded as the predecessor of the Web and online fora]. She analysed how the way of speaking has evolved [footnote: See also http://wwwtxt.org (or @wwwtxt on Twitter), a project collecting short phrases of the Usenet to demonstrate this] and how an self-organised community had built itself, noting that we can “feel [the community’s] informational generativity” and understand that one of the biggest qualities of the early users of the Internet was their “recursivity as a public” (Paloque-Bergès, 2013); that is, how they welcomed newcomers — notably through the use of FAQs [footnote: Frequently Asked Questions], which weren’t a common concept back then. There are also large elements of nostalgia coming back (Paloque-Bergès, 2011).
It is also interesting to come back on the archival of Usenet. The messages were not systematically archived, and obviously not available on the Web as
￼￼
it did not exist and is separate. Today’s collections have mainly been recreated from donations of people who did archived Usenet newsgroup, originally to DejaVu News in 1995, and Google Groups in 2001. A number of privacy concerns were raised with the publication of these searchable archives; messages who were, 15 to 20 years earlier, thought to be confidential are now considered as digital heritage. There was also a large backlash against the archiving made by for-profit companies, and the credit taken later by Google for its donors, with other critiques like Brad Templeton’s (an Internet veteran and important figure; cited in Paloque- Berges, 2013) remark that the archive was lacking in integrity. However, the searchability of these archives did bring welcomed technical possibilities for an Internet historiography, including a timeline where “the announcement of the Chernobyl disaster on Usenet stands alongside the first message of Linus Torvalds calling for help on what would become the Linux operating system” (Paloque-Bergès, 2013).
Similar concerns appeared at the announcement of the archival of Twitter by the Library of Congress in 2010. The two are not really comparable, mostly because the archive is not public and is made by a not-for-profit body. On the mailing list of the Association of Internet Researchers, the idea was met with interest: “The public twitter stream is of historical cultural significance and is an amazing repository of mundane moments in the daily lives of many people and records of what they thought important. [Nancy Baym on http://listserv.aoir.org/pipermail/air-l-aoir.org/2010- April/021125.html]”. However, the privacy concerns (notably with public and private feeds) were discussed, and as Michael Zimmer remarked:
“This is the classic “but the information is already public” argument that, while technically true, presumes a false dichotomy that information is either strictly public or private, ignoring any contextual norms that might have guided the initial release of information or how a person expects that information to flow.” (Zimmer, M. [on http://listserv.aoir.org/pipermail/air-l-aoir.org/2010-April/021136.html]; see also his paper (2010) on Facebook privacy, going further in that argument)
Paloque-Bergès (2010) theorises this argument, suggesting that “information is not an object, but a statement; it goes through socio-technical processes; it doesn’t have an objective nature”. A message in a tweet can have a very
￼￼
different context when taken apart of the moment it was posted. It also emphasises that the user could, later, switch their account to Private or delete it altogether, but this wouldn’t be reflected in the archive. We cannot know what is the user’s original intent when they posted their message, and, as Paloque-Bergès (2010) noted, we aren’t sure the user themselves know.
“More often than not, in a technical ecosystem in which making content private is more difficult than sharing broadly, teens choose to share, even if doing so creates the impression that they have given up on privacy. It’s not that every teen is desperate for widespread attention; plenty simply see no reason to take the effort to minimize the visibility of their photos and conversations.” (Boyd, 2014).
Advantages over physical preservation Mass preservation, easier access and organisation Recording digitally material that’s not born-digital
Cost of restoration; case studies of lost data
A number of institutions have already experienced loss of data, or data being corrupted. This can have disastrous consequences for businesses, for example, as noted by McAteer (1996, cited by Ross and Gow, 1999, p. iii): “of the 350 companies unexpectedly relocated by the [1993] World Trade Centre (NYC) bombing 150 [43%] ceased trading, many because they lost access to key business records held in electronic form”.
Data can, obviously, be sometimes restored. Ross and Gow (1999, pp. 39–42, referenced in Harvey, 2012, p. 34) provide a number of case studies. Harvey (2012, p. 34) also gives the ironic example of the website for the “Functional Requirements for Evidence in Recordkeeping” project, directed by the University of Pittsburgh which was accidentally deleted; but thankfully was automatically saved by the Internet Archive.
There is still the possibility of restoring data by specialised carriers; however Pearson (2009, slide 15) notes that while this was generally effective, this did not work on some items. He also regrets that the
￼￼
restoration loses all metadata in the process; this can be an issue for sensitive or classified information (for business or governmental archives) as the carrier would be able to see it; finally, it is a prohibitely expensive operation, and so a carefully planned preservation could be much cheaper (and obviously more reliable) than neglect. However, these services are always useful in certain cases. Harvey (2012, p. 41) cites the statistics by the data recovery giant Kroll Ontrack (2011): “hardware and system problems cause most customers to approach them (29 per cent), followed by human error (27 per cent). Software corruption or program problems, computer viruses and natural disasters are the other reasons listed.”
Defining paradigms for digital
preservation
Pre-digital paradigms
Inability to preserve the medium, only the bitstream Importance of metadata, relationship &amp; context Definition of terms
What to preserve / Selectivity
Harvey 5 pillars for selectivity (Harvey 2012, p. 26)
￼
￼￼     + If **unique information** objects that are vulnerable
 and sensitive and therefore subject to risks can be preserved
 and protected
     + If preservation ensures long-term accessibility for
 researchers and the public;
     + If preservation fosters the accountability of
 governments and organisations
     + If there is an economic or societal advantage in re-
 using information
     + If there is a legal requirement to keep it (NSF-DELOS
 Working Group on Digital Archiving and Preservation, 2003,
 p.3).
Banks reasons for domestic archiving
In The Future of Looking Back, Richard Banks (2011) analyses the reasons that lead us to keep or discard physical objects at home. He argues that while making a selection is a positive thing because it forces us to filter our belongings and focus on what is really meaningful, we also run the risk of ridding ourselves of things that we wish we would have kept in retrospect. He points out that the organisation of digital files make this trickier: because we we can keep pretty much everything on virtual space, it is easier to accidentally delete meaningful things alongside the less important. (Banks, 2011 p. 22).
In the context of personal archiving, Banks notes that in the past, the legacy we leave behind us was a highly selective one. Because our possessions are limited by physical space, our ancestors until now “have implicitly indicated that an item is significant through the simple act of keeping it. Although thousands of objects likely crossed their path over their lifetime, the constraints of money and space meant that only a few were retained, often the most significant.” (Banks, 2011, p. 78). He does however argue that this is no longer true in a digital world, because new possibilities mean we simply accumulate digital objects without necessarily ordering them by importance.
Technical possibility of archiving Pros/cons of archiving the entire internet
There are various controversies around whether we should automate the archiving of the Internet, or if we should operate a manual selection. UNESCO (2003, p. 73) notes that those who value a more comprehensive collection (i.e. archiving “all” the Internet) argue that “any information may turn out to have long-term value, and that the costs of detailed selection are greater than the costs of collecting and storing everything”. This argument is particularly true because of the digital media: “unlike non-digital material such as paper-based artifacts, where there is a period of time in which to make selection decisions before deterioration of materials becomes an issue, the time frame for deciding whether or not to preserve digital materials is very short” (Harvey, 2012, p. 57). UNESCO (2003, p.71) backs that claim: “it may not be possible to wait for evidence of enduring value to emerge before making selection decisions”.
Other people [Cook?] think that it would be preferable to keep a curated collection because of a higher technical quality, and this also allow us to settle [negotiate?] preservation rights with the content producers. Harvey adds that we also do not have the technical capabilities to find all the pages on the Internet, and even if we had we would not have the technical possibility to store the whole of the Internet (Harvey 2012, p. 58); again, this is also something recognised by UNESCO: “‘there are usually more things – more information, more records, more publications, more data – than we have the means to keep” (2003, p.70).
The UNESCO (2003) and Cook (2000) both however recognise that a middle ground should be found and that both approaches are valid depending on the goal of the preservation. For instance, the Internet Archive[IA] is going through an automated process (archive as many pages as possible) but, because of technical limitation, only preserves the text and dismisses all other images, stylesheets and external media, which could be critical in some instances. It seems evident that for valued documents these should be retained. The British Library’s Web Archive[ukwa] [http://www.webarchive.org.uk/ukwa/]
Dangers of selection (Cook)
However, Cook (2000) points out the dangers of manual selection for what is worth archiving: “With memory comes the inevitable privileging of certain records and records creators, and the marginalizing or silencing of others” (Cook, 2000, cited in Harvey p. 58). He asserts that the archivists
￼
must be impartial and objective, and ensure all points of view are kept to ensure our shared memory does not “becomes counterfeit, or at least transformed into forgery, manipulation, or imagination”. This is something that is clear with manual archiving, but could also be a problem, maybe unintentionally, with the automated discovery of content when archiving Web pages.
Automatic selection (Reputation model)
In order to keep the most valuable content, algorithms could be used. Kawano (2008) proposes to use reputation models, that attempts to determine the relative value of a Web page by considering its “importance, fairness, trustiness, uniqueness and valuation”. This is done using a variety of ways; the algorithm suggested by Kawano relies on the consistency of a web page, that is the amount of time it existed, the number of links it contains and the number of external links to this page (Kawano, 2008, p. 291).
Crawling
Manual curating
Special Collections for events (eg before elections, see webarchive.org.uk for other ideas)
Another interesting selection process that has emerged is the one of specific archiving when certain events are happening. For instance, the UK Web Archive[ukwa] has, on top of their topical-based collections (Energy, Health and Social Care...), a few event-based collections (Queen Jubilee, Olympic and Paralympic Games, London 2005 terrorist attacks...) which reflect the state of the Web at that given moment in history. Masanès (2006, p.83) notes that this type of archiving must be manually curated and thoroughly preparated. He gives a methodology for this, using presidential elections as an example: a time frame for the collection must be defined (“3 months before the election + 1 month after”), what type of sites will be archived and when (“the political parties’ blogs and websites, every week; analysis, commentary and humourous/satire websites, every month; online newspaper articles, once”).
Selection: by genre
There are different genres of websites: institutional website, blogs, personal
￼
pages, forums. [[Genre have been studied in the context of the Web to see how they replicate or diverge from the genre in the print- ing world (see Crowston and Williams 1997) - in Masanès p88]]. An interesting point about this is that often, robots (crawlers, or later data analysis) can automatically identify their type [Rehm 2002 -&gt; check reference, Masanès:88] which permits the automatic generation of metadata, or simply to restrict a collection of web pages to certain Web genres.
Web archiving: what stands as heritage on the web
Who should preserve
Digital media changes the original preservation paradigm for deciding who takes the responsibility for archiving content, and how. The processes of creation, fabrication and publication used to be distinct in the “analog” era; but as Nurnberg (1995, p. 21, cited in Harvey, 2012, p. 9) said, “technology tends to erase distinctions between the separate processes of creation, reproduction and distribution that characterize the classic industrial model of print commodities”. Self-publication (particularly on websites) is much more common, and this is also true for businesses and personal data. The Task Force on Archiving of Digital Information (1996, pp. 19–20) suggests that when, in general, “stakeholders disseminate, use, reuse, recreate and re-disseminate various kinds of digital information, they can easily, even inadvertently, destroy valuable information, corrupt the cultural record, [and ultimately thwart the pursuit of knowledge that is their common end]. Against such a danger, a safety net is needed to ensure that digital information objects with long-term cultural and intellectual value survive the expressions of stakeholder interest with their integrity intact.
[&gt; Society has always created objects and records describing its activities, and it has consciously preserved them in a permanent way... Cultural institutions are recognised custodians of this collective memory: archives, librar[ies] and museums play a vital role in organizing, preserving and providing access to the cultural, intellectual and historical resources of society. They have established formal preservation programs for traditional
￼
materials and they understand how to safeguard both the contextual circumstances and the authenticity and integrity of the objects and information placed in their care... It is now evident that the computer has changed forever the way information is created, managed, archives and accessed, and that digital information is now an integral part of our cultural and intellectual heritage. However the institutions that have traditionally been responsible for preserving information now face major technical, organisational, resource, and legal challenges in taking on the preservation of digital holdings (B. Smith, 2002, pp.133–134) [cited in Harvey p. 29].]
A variety of new stakeholders are now required to collaborate more for a good preservation practice to take place. Lavoie and Dempsey (2004, cited in Harvey, 2012, p. 30) argues that cooperation “can enhance the productive capacity of a limited supply of digital preservation funds, by building shared resources, eliminating redundancies, and exploiting economies of scale”. Chapter 11 of the UNESCO guidelines (2003) give more pratical details on how this can take place, giving structural models and tips on what to share (information, standards, division of labour...). [CLOCKSS cooperation to preserve journals (see Harvey p32 + chapter 9); + Harvey p32 National Library of Netherlands &amp; Elsevier Science + most uni libraries access with publishers, even UoD with eg Dawsonera/ACM]
Legal issues
(see later) special case for emulation
In certain cases, the archiving process is not made in accordance with the original publisher. This is often the case for the automated archiving of the Internet (e.g. Wayback Machine), which automatically crawl pages, but as a website publisher, it is easily possible to opt out of these schemes [see http://archive.org/about/exclude.php [Accessed 17th Apr 2014]]. In the case of emulation, this is what happens with reverse engineering, that is, disassembling the original program to understand how it works and recreate a similar version which can operate from a different hardware or software architecture. This is a common process for proprietary software or hardware, whose source code is not available to the general public and no documentation is provided for developers. Often, this is developed by amateurs and hobbyists, but is illegal as it breaks the intellectual property of the original creator; Nintendo, for example, attacks reverse engineered
￼
emulators[http://www.nintendo.com/corp/legal.jsp [Accessed 17th Apr 2014]] and the sharing of ROMs [Read Only Memory; the bitstream of cartridges or discs containing the games]. There is however popular demand for playing these, and Nintendo has developed their own emulators to play legacy games on their new consoles (see the Virtual Console [http://en.wikipedia.org/wiki/Virtual_Console]), while still retaining their property rights (you need to buy the games again on an e-commerce platform). This however causes issue for the longer term preservation: there is no certainty that Nintendo will continue to provide this service on their future generations of consoles, or if they ever cease operations.
A similar issue happened with Apple Computer. In 2005, Apple changed the architecture (that is, the “language” of instructions needed for the processor) of their personal computers from PowerPC to Intel x86. This means that applications needed to be rewritten, sometimes from the ground up, to work natively on the newer Apple computers. To ease the transition, Apple created Rosetta, a software that dynamically translates PowerPC instructions to x86 instructions, so that buyers of the new Apple computers were still able to open legacy applications that were not yet rewritten for x86 — although this came with a significant performance hit. In 2009 however, support for PowerPC was entirely discontinued [...]
Depends on the type of content Libraries
Good curating and preservation starts early. Smith (2003, pp. 2–3, cited in Harvey, 2012, p. 32) suggests that “the critical dependency of preservation on good stewardship begins with the act of creation, and the creator has a decisive role in the longevity of the digital object”. Harvey (2012, p. 32) notes that “for most creators of information in any form this is a new role”; therefore publishers of digital content are not necessarily well prepared for preservation, and this is why collaboration with librarians and archivists can be helpful (Ayre and Muir, 2004).
Ayre and Muir (2004) note that if the publisher worries about preservation themselves then they generally already own the rights for preservation, but there are negotiations to be done if an external body helps with
￼
preservation.
National libraries
Ayre and Muir (2004) suggest that the complexity and resources needed for digital preservation may mean that individual libraries may not have the possibility to do the preservation themselves like they used to, but instead rely on national libraries and centralised preservation systems to reduce overhead. This system is used by a number of libraries; even here at the University of Dundee, most journals and a selection of books are available through centralised systems (Dawsonera, Elsevier Science) or publishers directly (ACM). A report by Hedstrom and Montgomery (1998, cited by Ayre and Muir, 2004) found that most research librarians in the UK thought that a legal deposit to national archives was the best option, but this was not a clear consensus. The report also highlighted that these librarians thought that “other libraries and publishers would need to, and should, have some involvement in preservation”.
Archivists
New stakeholders
Profits vs nonprofits
Harvey (2012) points out doubts in the real interests of for-profit companies to achieve long-term preservation;
Even given the prevailing market-driven political ethos, it is difficult to envisage a situation where market forces will be sufficient to ensure the preservation of this digital material. The opposite is more likely to apply: ‘in some cases market forces work against long-term preservation by locking customers into proprietary formats and systems’ (Workshop on Research Challenges in Digital Archiving and Long-term Preservation, 2003, pp.x-xi). (Harvey, 2012, p. 32)
Personal archiving
￼￼
Yourself? Online services
How to preserve
Preparation
Software methods Making hard copies
What appears to be the simplest solution is to make physical copies (i.e. print) of the digital documents. Although this certainly solves some problems in regards to technological obsolescence, Rothenberg (1999) and Granger (2000) pointed out that this cannot be done for more interactive material besides text and images. There is also, obviously, an issue with the amount of content that can be stored in this way — this would not work for archiving websites. Granger (2000) recognises that “if one’s sole concern is with the intellectual content of a document and the document is of a fairly simple nature [...] this at least provides some form of security”.
Standardisation
Standardisation is the transformation of a document’s content to a format that is standardised, open and well documented, ensuring that we will be able to open it with a variety of applications on a variety of platforms and that we can rebuild a program for visualising that document — as opposed to vendor-specific, closed source program that could be discontinued at any point in the future. Rothenberg (1999, p. 10) claims that this is a bad solution because “standardisation sows the seeds of its own destruction by encouraging vendors to implement non-standard features in order to secure market share” (therefore these non-standard features will be lost if opened in a different application). Granger (2000) argues that the blame is not to be put on the method, but on vendors. Standardisation is a type of migration and therefore share some of it cons (notably, this absence of vendor-specific features could imply a loss in the presentation and other characteristics). Standards eventually become obsolete so it is also paramount to preserve the specifications of the format, or to define formats
￼
that are unambiguous, such as CSV or XML.
A slightly different approach to this method is rebuilding viewers. In this scenario, the bitstream is not migrated to a standardised format, but kept as is; we simply re-build a viewer that is able to read that format. This is not as straightforward as it sounds, because most of the time, formats for proprietary software are not documented and decrypting them would require reverse engineering. If they are documented, then it is likely that this format is already a standard. There exists cases where reverse engineering is possible, or where a non-standard format is documented; for example, Microsoft Word files are not a standard, but there is plenty of documentation available online to produce code able to read them. An experiment by the VERS project (Thibodeau 2002, p. 22) has shown that it is possible to reengineer a viewer for the PDF file format from the specification available; as long as we also preserve these specifications, we will be able to read PDF files in the future.
UVC
Lorie (2000, in Thibodeau, 2002, p. 22) has also proposed that the use of a portable programming language, such as Java [footnote: Most programming languages include instructions that are specific to a given platform, therefore the same file will need to be adapted to run on another platform. “Portable” languages like Java are called high-level and are more abstract. They are run inside a virtual machine, which worries about transforming Java code into instructions that are specific to that platform; as such, the same file of code can be ran on any platform.]. Lorie developed a software called Universal Virtual Computer (UVC), able to decrypt a variety of file formats, which can run on any platform supporting Java; because Java could be implemented on virtually any architecture, this means that a reader could be run on any platform without any effort. However, Lorie points out, this approach is limited in terms of functionality, and comes at the price of performance (because portable programming languages are platform-agnostic, this means the code is not optimised for any of the platforms). I am also wondering if this would really work on the long term, as we cannot be sure that future implementations of Java will be backward compatible with code written for a previous version.
Migration
￼
Migration is the transformation of a document’s content to a format that is not necessarily standardised, but at least current; or, the newer version of same format. This could be, for example, transferring documents from the WordPerfect 6 format (used in the 80s and late 90s) to a Word 2011 format, or a Word 98 file to Word 2011. This should be done regularly, as after a certain period of time new programs will stop having backward compatibility to import these old formats (Thibodeau, 2002). This method has several downsides, mainly being that it is labour intensive and expensive (Thibodeau, 2002; Rothenberg, 1999); product lines, and therefore the migration path, could be terminated at any point (Thibodeau, 2002); the presentation and other characteristics could be changed (although this can be overcome by migrating to more flexible formats like PDF or LaTeX). Rothenberg (1999) adds that this method is also prone to error, risky (as it could compromise the integrity of the data), not scalable and it requires new solutions for each new format. Granger (2000) argues however that comparatively, other methods (particularly emulation) are potentially much more expensive and labour intensive, and that for most preservation programmes, migration remains the only viable method.
Computer museums
Another approach to preservation, proposed by Swade (1993), would be to preserve a variety of hardware, software and devices to access obsolete media in “computer museums”. This is generally not considered a good solution; Granger (2000) notes that it would be unreasonably expensive to keep old machines running for an extended period of time, that computers chips will eventually decay anyway (and repair or replacement will be impossible), and that storage media not stand the test of time. There are still two possible use cases, identified by Rothenberg (1999): ensuring that emulators work as expected (by comparing them to original platforms), and helping with the recovery of lost data on obsolete media
Emulation
Emulation is the dynamic transformation of instruction sets to allow software and/or operating system designed for an obsolete architecture or platform to run on current systems. It is a highly complex method that offers certain benefits; notably, this is the most conservative method, as it is not destructive of the content or presentation. It allows us to run the original program used to create a digital object without altering that
￼
bitstream ; it “keeps the look and feel as well as the interactivity” (Granger, 2000) and as such, it is particularly interesting for objects rich in interaction, such as video games, data visualisations, or formats that are particularly complex (for example, medical imagery, 3D modelisations), for which methods like migration would be hard. It also ensures complete integrity and authenticity, which could be crucial for some data types.
There are numerous criticisms against emulation. A number of legal and intellectual property problems are raised, if the emulation strategy is not supported by the original author [see Nintendo &amp; Apple]. The complexity involved in creating emulators might be not worth it and requires a coherent organisation to reduce overhead (Granger 2000). Additionally, Thibodeau (2002, p. 21) argues that keeping the original functionality is not necessarily a positive thing when it comes to the delivery of the preserved content; “most users in the future will never have encountered—not to mention learned how to use—most of the products they will need to access the preserved information objects”, and “it would cut users off from the possibility of using more advanced technologies for discovery, delivery, and analysis”.
“Emulators themselves become obsolete; therefore, it becomes necessary either to replace the old em- ulator with a new one or to create a new emulator that allows the old emulator to work on new platforms. In fact, if you get into an emula- tion strategy, you have bought into a migration strategy. Either strategy adds complexity over time.” (thibodeau 2002 p.20)
[NB: due to the length limit of this literature review, I have only explained the most popular archiving techniques. See Thibodeau (2002) and Harvey (2012) for a more comprehensive list of methods, including configurable chips, persistent archives, object interchange format, etc.]
Choosing one
Thibodeau (2002, pp. 15–16) suggests four criteria to choose a preservation method: feasibility, sustainability, practicality and appropriateness.
Feasability means that the hardware and the software for implementing a given method must be existing and developed. Sustainability is ensuring
￼￼
that this method can be “applied indefinitely into the future, or that there are credible grounds for asserting that another path will offer a logical sequel to the method, should it cease being sustainable”. This means that the method must be resistant to technological obsolescence, both for storing the data (medium will change) and accessing it (means to access the media ane decode the bitstream will change).
Practicality is establishing that this method will be reasonably difficult and affordable. [This can generally change depending on the resources of the preserving organisation, the available technology at that time and the valuation of the digital objects.]
Finally, appropriateness is using a method that is relevant to the type of material being preserved, and the objectives of the preservation. For example, del Pozo et al. (2012, p. 7) suggests that if the integrity and authenticity of the documents are not paramount, then a migration that keeps the contents but discards the presentation might be the least expensive and simplest option. Inversely, when trying to preserve highly interactive material such as video games or when the specifications for a format are unknown, then emulation is a more pertinent option, because migration would be extremely difficult and inaccurate (Granger, 2000).
Authenticity/integrity and issues with each method
Different preservation methods yield different results in what is called authenticity, that is how faithful the rendition [its presentation] of the preserved object will be. Authenticity is subjective and therefore there is debate around this issue (Del Pozo et al., 2012, p. 7; Cook, 2000); however most agree that while as little difference in the presentation is desirable, it is rarely (if ever) possible to keep a truthful rendition. As an example, del Pozo et al. (2012, p. 8) use a spreadsheet within the migration method:
“For instance, one institution may decide to normalise a spreadsheet into a PDF, which would retain the cell values but destroy any formula used to calculate them. This would favour a particular presentation of that digital object over the information form of the original. On the other hand, another institution might decide to normalise spreadsheets into ODF, which may lose some formatting but retain the formula used to derive each cell. In this case, being able to retain the information form of the file would be seen as more important.”
￼￼
￼Thibodeau (2002, p. 28) however points out that “people will want to use the best available technology—or at least technologies they know how to use— for discovery, retrieval, processing, and delivery of preserved information. There is a danger that to the degree that preservation solutions keep things unaltered they will create barriers to satisfying this basic user requirement.” An overly faithful preservation would therefore not be desirable. Again, this is subject to a high number of parameters.
Another important issue is integrity, i.e. “ensuring the object is complete and has not been altered or corrupted” (Harvey, 2012, p. 54; Ross [CITED], 2002, p. 7); that is, the object “is what it purports to be”, and no parts are missing from it. This is particularly important, Harvey (2012, p. 54) notes, for business archiving, where the legal value of a digital object could be lost if we cannot demonstrate that this object hasn’t been tampered with, or that a migration processes have not incurred data losses. However, as Lynch (2000) has argued, the integrity of objects is only a matter of trust. The guidelines of UNESCO (2003, p. 22) recommend that the best protection for authenticity is proper documentation of the archival process; Harvey (2012, p. 54) expands on this idea: “for instance, if it can be established that a digital object has always been kept in the custody of an archive in which every change to it has been recorded (such as changes resulting from migration of the bit-stream – the dates at which migration occurred, to which media, and so on), then we can be more secure about its integrity.”
Emulation / legal rights with videogames Use of standards
Some standards such as OAIS (Open Archival Information System), originally developed for space science, could be used. Sadly [no implementation][source??]
Domestic archiving Archiving photos
What we store online
Personal media &amp; teenagers; privacy options Data already public
What happens after our
### [Digital heritage]
Shifts in usages (more photos)
Advantages (sharing, organisation etc.); record part of our lives we never did before
Keeping memories attached to digital objects
or storing physical objects digitally
-&gt; (larger scale? China book)
Conclusion
￼</Text>
        </Document>
        <Document ID="11">
            <Title>Introduction</Title>
            <Text>The research presented in this dissertation stems from observations of the recent shift of many of our communications towards social networking websites. Whilst creating memories (be it photographs, videos, textual stories…) digitally rather than physically is not exactly a new thing, the places where we share them online are progressively going further away from our own control. This creates worrying uncertainty in terms of preservation of our digital legacy.
Traditionally, we have kept what was sentimentally important to us in our homes, on materials that were not aging very fast (paper, for instance, will last for decades without any particular intervention needed to retain its integrity). Most importantly, the constraints of cost and physical space meant that we produced and kept relatively few artifacts, retaining only what was the most important to us in our lives. The immateriality of current technologies, however, allow us to create these artifacts very cheaply and store hundreds of these elements in a reduced space, which progressively eliminates this need for curation and selection, and instead let us record our lives more frequently and accumulate these records.
But this immateriality does not come without its downsides. Digital information is, as we will see, very fragile; not only because of the storage media used to store it, but also due to the technological obsolescence of interfaces and software to access these media. The emergence of social networks and services of “cloud” storage, however, produces a whole new issue: the long-term existence and safety of the data that we put on these is entirely dependent on the commercial success and policies of the provider. The short history of the Internet has already proven us that social networks and online spaces do not last eternally, and when they shut down or stop being maintained, our data disappears forever (examples include Bebo, MSN, MySpace, Orkut, GeoCities…). There are numerous reasons, that will be described later in this dissertation, to think that some (if not all) of the dominant social networks at the time of writing (Facebook, Twitter, Instagram, LinkedIn, Pinterest…) will be replaced in a few years and that our data will share the same fate, with possibly bigger consequences considering the extreme growth in the amount of users and content shared. There is an alarming lack of awareness about this impending destruction  of our memories, and most importantly a lack of simple solutions for the general public to be able to preserve digital memories over the long term. The questions that I will try to answer here are: is it worth capturing digital memories shared on social networks? How to store these in the long-term? Which privacy levels are desired for that content? And finally, how can we design an interface that would allow users to go through a complex lifetime of digital memories in a meaningful and organised way? 
The present research was done in parallel with my final year's honours project that is aiming to tackle this issue. As such I have used elements of the development of this project to inform the questions asked, and used prototypes to gather insights and find answers. Through a literature review in the field of digital preservation, structured interviews and user testing I am trying here to answer these questions, and decrypt a bit further the large question of personal archiving and representation of events over time.</Text>
            <Comments>Source http://www.ebizmba.com/articles/social-networking-websites
[Accessed 17/10/2014]
https://web.archive.org/web/20141007035034/http://www.ebizmba.com/articles/social-networking-websites
</Comments>
            <Notes>http://www2.warwick.ac.uk/fac/soc/al/learning_english/leap/writing/moreinfo/

a statement of the importance of the subject
mention of previous work on the subject
a justification for dealing with the subject
a statement of your objectives
a statement of the limitations of the work
a mention of some of the differing viewpoints on the subject
a definition of the topic being discussed

Move 1: Establishing a research territory
- by showing that the general research area is important, central, interesting, problematic, etc. (optional)

- by introducing and reviewing items of previous research in the area (obligatory)

Move 2: Establishing a niche
- by indicating a gap in the previous research or by extending previous knowledge in some way (obligatory)

Move 3: Occupying the niche
- by outlining purposes or stating the nature of the present research (obligatory)

- by listing research questions of hypotheses

- by announcing principal findings

- by stating the value of the previous research

</Notes>
        </Document>
        <Document ID="93">
            <Title>Interactive material</Title>
        </Document>
        <Document ID="94">
            <Title>Context/emulation</Title>
            <Text>[Possibility of keeping context/interfaces along with content for remembering better]</Text>
        </Document>
        <Document ID="12">
            <Title>Untitled</Title>
        </Document>
        <Document ID="133">
            <Title>A note about the draft</Title>
            <Text>This draft lacks important parts to it. Obviously, the outcomes of the interviews as they aren’t done yet, but parts of the context are also missing and possibly in the wrong order. Due to a heavy workload in the past weeks with deliverables with the honours project, I couldn’t finish it as much as I wanted to.
It is particularly worth saying that the literature review chapter is much longer than it will be in the final dissertation. It includes a lot of elements from the first draft of my literature review last semester that have been omitted in the final version, and new sections about individual backups. This puts this chapter at around 6,000 words, but it will be edited down to 2,000—2,500 words. As a result the chapters about context and methodology are much less developed, and for now it is mostly a structure with titles, synopses and notes for each section, which will be ultimately expanded to make up about 2,000 words each. Notes in redacted sections are generally marked [within square brackets].</Text>
        </Document>
        <Document ID="129">
            <Title>Archive Team</Title>
            <Text>In 2009, following the quick shutdown of hosting sites like AOL Hometown and Podango, technology historian and web archivist Jason Scott started the Archive Team, a “loose collective of rogue archivists, programmers, writers and loudmouths dedicated to saving our digital heritage”. The Archive Team aggressively downloads whole websites that risk to close down soon, and then mirrors them (i.e. put a static copy online so that the contents are still accessible). Their mission statement helps us understand why we cannot trust online services to be the only holders of our data:
“Corporations do not contemplate their own inevitable end. At least, they don't do it in public, unless they are in really bad shape. When times are good, those thoughts are pushed away, and end users are encouraged to do the same. When times are bad, they tend to go very bad, very quickly - if you're lucky, you'll have an announcement. Your data is never totally safe. Backing up your data is always necessary, even if it's stored elsewhere.
Entropy will rear its head, if you leave things up to chance. You will lose 4 years worth of email, including communications from the early days of your marriage, and the receipt to that flat panel monitor you bought online that now has tons of dead pixels. It doesn't take much for catastrophic data loss; one absent-minded mistake, or the ravages of time, can wipe out years of data.
There is real convenience to web services like Google Apps. It's tempting to get wrapped up in that convenience and never take a step outside of it. [With a little work, convenience can be evenly matched with user control and agency.]
Businesses can be extremely helpful, but they are also self-interested. As benevolent as Web services present themselves to be, your data is valuable to them - they aren't running this for your benefit. And it should be valuable to you, too.”</Text>
            <Comments>http://ascii.textfiles.com/archives/1649 [Accessed 20th October 2014]
http://archiveteam.org [Accessed 20th October 2014]
Using “crawling”, a technique used by search engines to easily find all the pages of a website by copying a page and recursively going through all of its links.
In information theory, entropy is the loss of information considered useful (Barrett, 1995, p.185)
http://archiveteam.org/index.php?title=Why_Back_Up%3F [Accessed 20th October 2014]</Comments>
        </Document>
        <Document ID="95">
            <Title>Understanding history</Title>
            <Text>Harvey (2012) and Lukesh (1999) question whether digital personal correspondance might be lost if we do not take steps to preserve them:
“The widespread shift from writing letters to the use of email has diminished the likelihood that personal correspondence will remain accessible to future historians. Lukesh asked in 1999, ‘Where will our understandings of today and, more critically, the next century be, if this rich source of information is no longer available?’, as scientists, scholars, historians and almost everyone increasingly use email” (Lukesh, 1999) (in Harvey, 2012 p. 32)
While this is a correct idea, I think that few people kept their letters in the sole purpose of leaving researchers decades from now understand our epoch’s social habits and ways of speaking. There have to be other motivations for preserving our conversations and our digital objects.</Text>
            <Comments>Grammar a bit confusing (review)</Comments>
        </Document>
        <Document ID="13">
            <Title>Untitled</Title>
        </Document>
        <Document ID="134">
            <Title>Results</Title>
        </Document>
        <Document ID="14">
            <Title>Conclusion</Title>
        </Document>
        <Document ID="96">
            <Title>For social networks</Title>
            <Text>In preparation of future data losses, perhaps, the US Library of Congress maintains a permanent archive of Twitter since 2010 (LoC, 2010). However, this is again kept with a cultural intention: “The public Twitter stream is of historical cultural significance and is an amazing repository of mundane moments in the daily lives of many people and records of what they thought important” (Baym, 2010). Keeping a parallel with the GeoCities archive, I believe that the jargon, hashtags and emojis of the present day could become the ‘Under Construction’ GIFs of the next decade, becoming relics and symbols of a time on the Internet. However, there seems to be little possibility for individuals to create archives of their own account, for retrieving their memories at a later time and not from a scientific/historical perspective. A few services, such as SocialSafe or Frostbox offer to back up social media accounts; they are principally marketed towards businesses and authorities, whose communications on social media need to be archived in case of dispute, but they can also be used by individuals. However these services are subscription-based (or offer subscription-based premiums, but back up using proprietary archive formats) and as such we are simply repeating the situation of social networking sites, where backed up data could disappear if the companies go out of business — possibly in a worse way, since these companies do not have the financial means of social networks to survive for very long.</Text>
            <Comments>http://www.socialsafe.net/
http://www.frostbox.com/</Comments>
            <Notes>In The Future of Looking Back, Richard Banks (2011) analyses the reasons that lead us to keep or discard physical objects at home. He argues that while making a selection is a positive thing because it forces us to filter our belongings and focus on what is really meaningful, we also run the risk of ridding ourselves of things that we wish we would have kept in retrospect. He points out that the organisation of digital files make this trickier: because we we can keep pretty much everything on virtual space, it is easier to accidentally delete meaningful things alongside the less important. (Banks, 2011 p. 22).
In the context of personal archiving, Banks notes that in the past, the legacy we leave behind us was a highly selective one. Because our possessions are limited by physical space, our ancestors until now “have implicitly indicated that an item is significant through the simple act of keeping it. Although thousands of objects likely crossed their path over their lifetime, the constraints of money and space meant that only a few were retained, often the most significant.” (Banks, 2011, p. 78). He does however argue that this is no longer true in a digital world, because new possibilities mean we simply accumulate digital objects without necessarily ordering them by importance.
</Notes>
        </Document>
        <Document ID="97">
            <Title>What to preserve</Title>
            <Notes>+ 
In order to keep the most valuable content, algorithms could be used. Kawano (2008) proposes to use reputation models, that attempts to determine the relative value of a Web page by considering its “importance, fairness, trustiness, uniqueness and valuation”. This is done using a variety of ways; the algorithm suggested by Kawano relies on the consistency of a web page, that is the amount of time it existed, the number of links it contains and the number of external links to this page (Kawano, 2008, p. 291).

+ Manual curation
+ all SNSs?</Notes>
        </Document>
        <Document ID="15">
            <Title>Untitled</Title>
        </Document>
        <Document ID="135">
            <Title>Lecture</Title>
            <Text>
How do we even think about time, as people? That’s an odd question, but it’s also quite an interesting one for my project, because I need to visually represent time to represent the events happening in a life. And when you try to picture time, you transform time into space, which means it’s no longer time. So the most natural way, for us raised in a Western culture, is this…

# Timeline

A line. It’s a very simple, linear way of looking at time, it’s is logical order of when things happened, In terms of space, it’s a movement, that goes from the past, behind us, and the future, ahead of us. **[next slide]** The timeline on a single line is a very convenient thing because we can map units to it, all the same size, from years to minutes. And so, we’re very used to it. That’s how we’ve been taught history at school, that’s how we organise our CVs, that’s precisely, how social networks show content on a profile; your Twitter or Instagram timeline, or your own profile on Facebook is displayed, except it’s vertical instead of horizontal, the present at the top and the past below. It’s also used as another metaphor on computers…

# Progress bar

The progress bar. Same principle. Left: you’re at the start. Here: you’ve made some progress, you’re this close to the end. Right: there you are, you’re at the end. Simple. The very interesting thing about this is the name. **Progress** bar. We’re progressing from start to finish in terms of time. That notion of progress is very important for the timeline, keep that in mind, I’ll come back to it later. It’s actually what’s created it. The timeline as we know it, this one **[1 slide before]**, is only 250 years old. That model is very natural for us, but it’s actually a very cultural and somehow novel thing to have this single line, straight X axis with events on it that goes from left to right.

# A cultural thing

The origin of this direction, this movement, actually comes from reading. When we read, the upcoming information is on the right; in a sentence, or in a book, you make progress in the book. It’s an important thing to note because it’s very Western to think like that. Speakers of languages that are read from left to right, actually think of time in a left-to-right manner. There’s been studies *[Tversky, Kugelmass, and Winter 1991]* that asked Arabic and Hebrew speakers to order stickers, for example breakfast, lunch and dinner, or pictures from a person from younger to older, and they put it right-to-left, whereas English speakers did the reverse. And even in software, if you open software in Arabic, you’ll see that the progress bars are reversed.

On a side note, interestingly, there’s been other studies that have been done with some aboriginals from Australia, the Pormpuraaw community, and their languages do not have words for left or right. They tend to use absolute space directions like North, South, West, East for everything. So for example they would say “move your cup to the north-west a little bit”, or “the boy standing on the south of Mary is my brother”. They have amazing abilities to know which way they’re facing. And so researchers have been interested to know how they think about time, because they don’t have the same cultural idea of left or right to go further. When they’ve been asked to order things, instead of putting them right to left in front of them, they lay the stickers down and been ordering them from east to west, just like the sun, from morning to evening. 

# Cultural differences 2 (down arrow)

But it’s not just the writing direction. Mandarin speakers tend to think of time vertically, more often than we do, even though modern Chinese is read in rows not columns, but their conception of movement is also different — they see a moving timeline and a static observer, whereas we tend to visualise ourselves moving in the static timeline. Speakers of the Aymara language, in South America, think of time from left to right, like Arabic speakers, even though the language reads right-to-left; this is because they see the past as being in front of them, something that they can look at, whereas the future is in their back, it is the unknown, what you cannot see. [Lera Boroditsky, How Language constructs time]. 
+ link to how everyday language in English carry these notions of representing time as well (before, after, long and short times, intervals [spaces in between], move an appointment two hours *back*, the clock is 5 minutes *fast* (movement) or *ahead*, we have lunch in the middle of the day, implying that day is a line when it’s more accurately a circle) [source: Cartographies of Time 2010]

So. Even in the modern, current world, people tend to think of time in very different ways, different directions, different ideas of motion, for different reasons. 

# Ouroboros

Sometimes it’s not just a straight line. We can often find that famous Egyptian symbol, the Ouroboros, a snake or a dragon eating its tail, to represent the fact that time can be thought of as a recurring cycle, instead of something linear, with a start and with an end. Today we mostly use this representation for the dial of a clock, because days are easy to visualise as something that repeat themselves. And it’s perfectly natural; the Earth rotates continuously around the Earth, that representation actually comes from sundials. [It’s interesting because we don’t do the same thing when we pick up a calendar — personally when I think of the months, I have a straight line going from January to December but it’s not a circle even though it’s recurring.]

What’s interesting about this idea, it’s that Mayan and Pagan traditions visualised history and life as something like that. For life, they didn’t see birth as a new start and death as an end, just something that kept going. [If that helps with the concept, in the Balinese language, the word for grandparent and grandchildren is the same (kumpi). They see lives as continuations of someone else’s.]

Overall, from the research I’ve made, no matter which culture, our graphical representations of time are always a line, or a circle. The direction may change, the perception in terms of movement changes, but it’s still based around a line. What can change is the way we put events on it. There can be several dimensions to it. There can be several parallel lines. There can be streams of events merging and separating.

# The history of the timeline

18 - Genealogical tree, descendants of Noah’s son Japhet [Hartmann Schedel]
19 - record of English history from Alfred the Great (871–99) to Henry III (1216–72),
[Peter of Poitiers] (13th century)
20 - 1475 Epitome of chronicles and histories
21 - [twelfth-century] Joachim of Fiore - biblical past, Christian present, and a transformed future into a single, complex vision
22 - [16th century] Johann Funck’s - historical calendar, great events of Old and New Testament history
23 - paper chart with a pivoting central arm. Rings representing kingdoms and radial wedges representing centuries. The names of kingdoms are printed on the moveable arm. 
24 - [Johann Georg Hagelgans] 1718 - maintained the familiar historical matrix in the background, his enormous charts burst through everywhere with images, maps, and data.
25 - 1769 [Joseph Priestley] - A New Chart of History.  **Regularized the distribution of dates**.
26 -  Girolamo Andrea Martignoni - Historical map of Italy, 1721 - like a map, rivers and land masses turn out not to be landscape features but temporal metaphors —territories of history and rivers of time. [The streams at the top of the chart represent the nations conquered by the Roman Empire; those at the bottom, the nations that emerged from it; and the great lake at the center, the empire itself.]
27 -  Edward Quin’s An Historical Atlas, [1828]. Quin’s maps showed how the political world was divided up at different moments of history, and, through
the device of clouds rolling back, he indicated how much of the world was known to the West at each stage in history.
28 - Friedrich Strass [1804] Strom der Zeiten
29 - [Richard Cunningham Shimeall] - 1833 - radial columns represent centuries from the Creation to the Apocalypse.
30 - Wallis’ New Game of Universal History and Chronology [1840]. Such events as the first use of paper in England, the invention of engraving, and the discovery of longitude.
31 - Emma Willard’s Temple of Time from 1846 - standing columns represent centuries: those on the right are emblazoned with the names of important historical figures of the old world, those on the left, of the new. The floor shows a historical stream chart. The ceiling functions as a chart of biography.

So after all that you’re probably wondering…

# WHERE AM I GOING WITH ALL THIS 
also when’s the cake

My point is that there’s there’s several things happening at the same time, and more importantly that
# A life doesn’t look like this
But more like this **[next slide]**
The inventor of the timeline himself, Priestley, recognised that “historical narrative is not linear. It **moves backward and forward making comparisons and contrasts, and branches irregularly following plots and subplots**. […] The form of the timeline emphasises overarching patterns and the big story.” The timeline is a “great mechanical help to know history, but is not an image of history itself”. *[Cartographies of Time p.20]*

# Progression

Before the straight timeline, people didn’t see the future as being necessarily progressive. In the 1620s, Francis Bacon changed that and thought that the **technological and scientific revolutions would mean we would keep perfecting ourselves and the nature. Things always keep getting better as technology and democracy advances.** That’s the reasoning behind the timeline, and by extension, the progress bar that I’ve shown you before: the further we get on the line, the more progress we’ve made. 

**But that’s not necessarily true of our lives.** They’re not just advancing like that, they’re much more complex, like history. And if we try to visualise our life linearly, if we put too many measurements, it’s not a true picture of our life. We could do it, but I don’t think that’s ambitious, because memories link to each other and that’s how they make sense, that’s what makes them valuable. And I think overall, when we try to map out the complexity of everything that’s happened in our life, it’s more interesting to look back not at the big picture and think in terms of time units, years, months, but in terms of moments, what’s important and what’s not.
</Text>
        </Document>
        <Document ID="98">
            <Title>Hard copies</Title>
            <Synopsis>Print out photographs etc. Show arguments against mostly but also why it could be a viable option in some contexts</Synopsis>
            <Text>What appears to be the simplest solution is to make physical copies (i.e. print) of the digital documents. Although this certainly solves some problems in regards to technological obsolescence, Rothenberg (1999) and Granger (2000) pointed out that this cannot be done for more interactive material besides text and images. There is also, obviously, an issue with the amount of content that can be stored in this way — this would not work for archiving websites. Granger (2000) recognises that “if one’s sole concern is with the intellectual content of a document and the document is of a fairly simple nature [...] this at least provides some form of security”.
</Text>
        </Document>
        <Document ID="16">
            <Title>What I want to prove</Title>
            <Synopsis>What are my objectives with that research, what are the results I expect
</Synopsis>
        </Document>
        <Document ID="20">
            <Title>Spatial model of timeline</Title>
            <Text>As we have seen, the technical complexity of preserving digital information is a problem in itself. But even once we have chosen a suitable preservation scheme, there is another equally important question that must then be raised: how do we organise and retrieve our data? We cannot simply gather all digital items and let them pile up unorganised. In the context of a digital personal life, several artefacts are created daily (social network updates, emails, messages) and after a decade this can build up to tens of thousands of pieces of data, that we will need to go through once we are looking for something. 
Most social networks present our post history using a reverse timeline; newer posts at the top, older posts at the bottom. Viewing individual events in chronological order seem like a rational choice. It is however interesting to ask ourselves how do we think about time, in order to understand why this seems rational. Time itself is not a concept that can be visualised. Whenever we think about time, we actually transform it into space in order to picture it in our minds. The most natural way, for us, is to visualise an arrow going like the one in fig. Timeline/movement.
[Fig. Timeline/movement]
It goes into the realm of space because it is a movement; the past is behind us and the future is ahead of us. This idea of movement is repeated in our everyday language (Weger and Pratt, 2008, p. 426 and Boroditsky, 2010, p. 334); we use the words before, after, we have times that are short and long, we have intervals (literally, spaces in between) between two times; watches can be 5 minutes fast or ahead, and calendars and clocks make us think of time on segments with a start and an end (the beginning of the month, the middle of the day). However, languages vary and research has shown that different cultures and languages have a different mental representation of time, visually, and that this idea of a line going from left to right is in fact a cultural, nurtured interpretation of time that we have as Westerners, but is not innate.

“I’m a big fan of chronological timelines – maps that visualise history sequentially and mnemonically so we can comprehend it better. They’re such a useful, vital tool. But they are just a tool. And they have a history and a politics of their own. Part of that history comes from a fairly new idea that history is progressive. (Up until the eighteenth century and beyond the majority view was that we’re in decline and awaiting the End Times). […] That marvellous book Cartographies of Time by Anthony Grafton and Daniel Rosenberg gives a glorious visual history and analysis of how, in the mid-eighteenth century, the timeline (as we know it now) was introduced as a new form of graphic organisation of history. It unified and measured all of history in a novel way. And because the names of great men seemed to cluster closer to the present than the past it appeared to be an objective picture of historical progress. I imagine this picture also helped gradually replace the dominant idea that history is driven by divine will with the idea that the causal relationship between historical events is intrinsic, and can be shaped by human will. But that graphic device quickly became taken for a picture of truth – an objective, inarguable representation of how the past was and how the future should and will be.” (Haynes, 2013a)



But as Priestley recognised, his innovations posed problems too: historical narrative is not linear. It moves backward and forward making comparisons and contrasts, and branches irregularly following plots and subplots. Part of the advantage of the matrix form was that it facilitated the scholar’s understanding of the many intersecting trajectories of history. The form of the timeline, by contrast, emphasised overarching patterns and the big story. This proved a great advantage in some respects, but not all. And Priestley readily admitted this. For him, the timeline was a “most excellent mechanical help to the knowledge of his- tory,” not an image of history itself. (Cartographies of time p.20)

</Text>
        </Document>
        <Document ID="99">
            <Title>Standardisation</Title>
            <Synopsis>Standardisation is the transformation of a document’s content to a format that is standardised, open and well documented, ensuring that we will be able to open it with a variety of applications on a variety of platforms and that we can rebuild a program for visualising that document — as opposed to vendor-specific, closed source program that could be discontinued at any point in the future. Rothenberg (1999, p. 10)</Synopsis>
            <Text>Standardisation is the transformation of a document’s content to a format that is standardised, open and well documented, ensuring that we will be able to open it with a variety of applications on a variety of platforms and that we can rebuild a program for visualising that document — as opposed to vendor-specific, closed source program that could be discontinued at any point in the future. Rothenberg (1999, p. 10) claims that this is a bad solution because “standardisation sows the seeds of its own destruction by encouraging vendors to implement non-standard features in order to secure market share” (therefore these non-standard features will be lost if opened in a different application). Granger (2000) argues that the blame is not to be put on the method, but on vendors. Standardisation is a type of migration and therefore share some of it cons (notably, this absence of vendor-specific features could imply a loss in the presentation and other characteristics). Standards eventually become obsolete so it is also paramount to preserve the specifications of the format, or to define formats that are unambiguous, such as CSV or XML.
A slightly different approach to this method is rebuilding viewers. In this scenario, the bitstream is not migrated to a standardised format, but kept as is; we simply re-build a viewer that is able to read that format. This is not as straightforward as it sounds, because most of the time, formats for proprietary software are not documented and decrypting them would require reverse engineering. If they are documented, then it is likely that this format is already a standard. There exists cases where reverse engineering is possible, or where a non-standard format is documented; for example, Microsoft Word files are not a standard, but there is plenty of documentation available online to produce code able to read them. An experiment by the VERS project (Thibodeau 2002, p. 22) has shown that it is possible to reengineer a viewer for the PDF file format from the specification available; as long as we also preserve these specifications, we will be able to read PDF files in the future.</Text>
        </Document>
        <Document ID="17">
            <Title>Document prototypes</Title>
            <Synopsis>Document the creation of prototypes (put full process/documentation in appendix, but generally mention what made me come to this research)</Synopsis>
            <Text>Find inspiration for prototypes
How to make prototypes
What questions needs to be answered — which ones will be answered by prototypes, which ones by ethnography, which ones by asking directly
</Text>
            <Notes>“be your own ethnographer”
see book on auto ethnography
appendices should contain concrete information

Start off by explaining which questions need to be answered by the workshop

WHY do I want alternative visualisations — what’s wrong with the timeline, why do I think it’s relevant to think about alternative ways

Get people’s opinion on this
Would people curate it themselves?

-&gt; explain why it would be more interesting to make it automatic and not manually curated (people would not be bothered probably, back that up with interaction design books/thoughts about blocking the user in what they’re doing) 

-&gt; Try that (make them draw it, map their life, highlight which parts are present on SNS)
(don’t be too directive but give examples, eg what led you to get to Dundee, what happened in Dundee that caused you to meet your partner, etc)


-&gt; make people find important photos on fb, look at how they proceed
-&gt; ask more questions about what they do not post on FB, if they would like to get this integrated in the archive

-&gt; concerns about privacy
</Notes>
        </Document>
        <Document ID="21">
            <Title>Applying it to a personal life</Title>
            <Text>“To ‘think’ time, we visualise it. But by making pictures and models of time, we turn it into space and matter. In other words, to try and think of time we turn it into what it is not. Reading Henri Bergson’s Matter and Memory I thought about the implications of fixing time so it becomes space. Bergson is warning us that that way of thinking can lead to quantifying qualities of life that exceed measurement, that are bigger than the mechanistic. That’s because we start to confuse the metrics for the life, taking the data to be more tangible than the thing.” (Haynes, 2013a)
“The picture of time as a line, of course, reduces the complex and multiple qualities of life, and that’s the other question being raised by this project: how do we map a life in a way that gives space and value to our actual lived experience — to the knots, ruins, dead ends and failures as well as the triumphs and the leaps?” (Haynes, 2013a)
“The timeline is one of a number of new time-measuring tools – like the mechanical clock – that first started becoming a part of everyday life in the mid-eighteenth century. […] Tristram Shandy’s form as a story and as a material object is so absolutely knotty, complicated, elusive, unmeasurable and unmappable that it is a brilliant counterpoint to the progressive linear model of history. […] .”  (Haynes, 2013a)

Link with Context &gt; Objectives &gt; Desire to visualise differently 
“And it surely is faulty. A measurement is only a shadow of the thing. If we mistake the map for the land, the score for the performance, the timeline for the life, then we reduce existence to its shadows. As the philosopher Henri Bergson warned, lived time in all its fullness can’t be measured. Doing so risks reducing a life to a machine or commodity.
So what if we tried an experiment and mapped our life as we really experience it to be: a map without measurements? One that gives space to events according to their significance rather than their length in clock-time?
What if we made that map just for our self and included what we would leave out of a job application – a map that includes the mazes and trapdoors along with the parades of glory and the leaps of progress? Might we value the failures and the ‘time wasted’ more then? And when we accept and value our past experience in all its fullness, how does that change our view of the future?” (Haynes, 2013b)</Text>
        </Document>
        <Document ID="140">
            <Title>Figures list</Title>
            <Text>
TristamShandy/diagA
Diagrams mapping the course of Tristram Shandy’s life story, by Laurence Sterne. From the 1761 satire on linear narrative The Life and Opinions of Tristram Shandy, Gentleman.
Sterne, L. (1761/2012).  The Life and Opinions of Tristram Shandy, Gentleman. [ebook]. Salt Lake City: Project Gutenberg. Available from: http://www.gutenberg.org/ebooks/39270 [Accessed: 7th Dec. 2014]. pp. 347-348 (Book IV, chapter XL)

TristamShandy/diagB
idem
idem
Cartographies/1-NurembergChronicle
Nuremberg Chronicle by Hartmann Scheidel, undated. Depicts the descendants of  Noah’s son Japhet, using the tree as a metaphor.
Hartmann Scheidel, Nuremberg Chronicle. Courtesy of the Department of Rare Books and Special Collections, Princeton University Library. Found in Cartographies of Time, p. 32.
Cartographies/2-PrincetonMS57
Record of English history from Alfred the Great (871–99) to Henry III (1216–72), in the manuscript 57 of Princeton. 
Anonymous. Princeton MS. 57; Courtesy of the Department of Rare Books and Special Collections, Princeton University Library.  Found in Cartographies of Time, p. 35.
Cartographies/3-Chronicarum
Codex genealogy using vertical streams. Chronicarum et historiarum epitome, 1475, anonymous.
Anonymous. Chronicarum et historiarum epitome, 1475. Courtesy of Burke Library, Union Theological Seminary. Found in Cartographies of Time, p. 39
Cartographies/4-JoachimOfFiore
Diagram showing interestecting states or ages of world history, by followers of Joachim of Fiore, in Oxfords’ manuscript 255a.
Joachim of Fiore. 12th century. Oxford MS. 255a, Corpus Christi College, f. 11r. Cartographies of Time, p. 57.
Cartographies/5-JohannFunck
Historical calendar, showing events of the Old and New testaments in modern dates and terms. Johann Funck, undated.
Johann Funck. Undated. From Cartographies of Time, p. 73
Cartographies/6-DiscusChronologicus
Discus chronologicus by German engraver Christoph Weigel, early 1720s. It is a paper chart with a pivoting central arm, with rings representing kingdoms and radial wedges representing centuries.
Christoph Weigel. Discus Chronologicus. Courtesy of Department of Rare Books and Special Collections, Princeton University Library. From Cartographies of Time, p. 105
Cartographies/7-CartaIstorica
Historical map of Italy by Girolamo Andrea Martignoni, in 1721. It uses a visual analogy between geographic space and historical time. “Though he calls them maps, Martignoni’s works are not historical maps in the conventional sense of geographical snapshots from different moments in history: they are chronological charts presented in a cartographic form. While, at a glance, they seem to depict a circular territory with a great lake at the center and rivers running to and fro, on examination, these rivers and land masses turn out not to be landscape features but temporal metaphors—territories of history and rivers of time. The streams at the top of the chart represent the nations conquered by the Roman Empire; those at the bottom, the nations that emerged from it; and the great lake at the center, the empire itself.” [Cartographies of Time]
Girolamo Andrea Martignoni, Spiegazione della carta istorica dell’Italia (Historical map of Italy), Rome, 1721. Courtesy of Department of Rare Books and Special Collections, Princeton University Library. From Cartographies of Time, p.109.
Cartographies/8-AtlasHistoricus
This very large chronological chart from the Atlas historicus, published by the German engraver Johann Georg Hagelgans in 1718, is exceptionally rich; it contains images, maps and data, while keeping a tabular format.
Johann Georg Hagelgans, Atlas Historicus. 1718. Courtesy of Department of Rare Books and Special Collections, Princeton University Library. From Cartographies of Time, p. 107
Cartographies/9-NewChartOfHistory
A New Chart of History, published in 1769 by Joseph Priestley, normalised the distribution of dates.
Joseph Priestley, A New Chart of History. 1769. Courtesy of the Library Company of Philadelphia. From Cartographies of Time, pp. 120-121
Cartographies/10-HistoricalAtlas
Edward Quin’s An Historical Atlas, published in 1828, is an interesting concept. Page after page, moment in history after another, it shows the progression of the political divisions and Westerners’ knowledge of the world, using clouds rolling back to mask unknown regions.
Edward Quin, An Historical Atlas. Courtesy of Department of Rare Books and Special Collections, Princeton University Library. From Cartographies of Time, pp. 128-129
Cartographies/11-StromDerZeiten
Strom der Zeiten (Stream of Time) is a highly influential chart published in 1804 by Friedrich Strass in Austria. Strass thought that linear charts such as Priestley’s were misleading because their regular measurements implied that history was uniform, and instead chose to use the metaphor of streams.
William Bell, English translation of Friedrich Strass’s 1804 Strom der Zeiten, London, 1849. Courtesy of Cotsen Children’s Collection, Department of Rare Books and Special Collections, Princeton University Library. From Cartographies of Time, p. 144
Cartographies/12-EcclesiasticalChart
1833 chart by Richard Cunningham Shimeall. It is read from the inside to the outside; the radial columns represent centuries from the Creation to the Apocalypse.
Richard Cunningham Shimeall, Distinctive Complete Ecclesiastical Chart from the Earliest Records, Sacred and Profane, Down to the Present Day. 1833. Courtesy of Burke Library, Union Theological Seminary. From Cartographies of Time, p. 164
Cartographies/13-GameOfUniversalHistory
Wallis’ New Game of Universal History and Chronology  is a 1840 game sheet showing various technological advances such as the first use of paper in England, the invention of engraving and the discovery of longitude.
Wallis’ New Game of Universal History and Chronology, 1840. Courtesy of Cotsen Children’s Library, Department of Rare Books and Special Collections, Princeton University Library. From Cartographies of Time, p. 194.
Cartographies/14-WillardTemple
Temple of Time by Emma Willard, published in 1846, is a 3D projection of chronography. Each century is represented by a standing column, with historical figures of the Old and the New world on each side, and a stream chart on the floor.
Emma Willard, Temple of Time, 1846. Courtesy of General Research Division, The New York Public Library, Astor, Lenox and Tilden Foundations. From Cartographies of Time, p. 201
Timelines/movement
Timeline going from left to right
Self made (reuse the one made for the slides)
Timelines/progressBarWestern
Western progress bar
id
Timelines/progressBarRTL
Right-to-left progress bar
web
Timelines/Ouroboros
 Ouroboros drawing from a Byzantine Greek alchemical manuscript, 1478.
 Ouroboros drawing, from the 1478 manuscript  Parisinus graecus 2327, which was a copy of a lost manuscript of an early medieval tract attributed to Synosius of Cyrene (died in 412).

</Text>
        </Document>
        <Document ID="136">
            <Title>Questions asked</Title>
        </Document>
        <Document ID="18">
            <Title>Literature review</Title>
            <Synopsis>Reference (but edit) the literature review in line with what I have written before

Go through methods of archiving, particularly</Synopsis>
        </Document>
        <Document ID="22">
            <Title>Screen based interaction patterns</Title>
            <Notes>possibly physical? (see PeDeTe boxes exemple)</Notes>
        </Document>
        <Document ID="19">
            <Title>Time representation</Title>
            <Synopsis>Origin of research - small review of what I have found about time representation
</Synopsis>
        </Document>
        <Document ID="137">
            <Title>Drawings</Title>
        </Document>
        <Document ID="23">
            <Title>Reflection on experiments</Title>
        </Document>
        <Document ID="141">
            <Title>Untitled</Title>
        </Document>
        <Document ID="24">
            <Title>Issues of the project</Title>
        </Document>
        <Document ID="142">
            <Title>Acknowledgements</Title>
            <Text>I am indebted to the work of a few people which have pointed me to extremely valuable sources, and that helped me in setting more precise objectives and gaining valuable outlooks for both my honours project and my dissertation. Most notably, the research blog of Miriam Julius http://multimedia-memories.tumblr.com/; the research blog of the Materialising Memories team, led by Elise van den Hoven http://www.materialisingmemories.com/updates/; the research blogs of Cathy Haynes, artist (The Timekeeper Project and Stereochron Island) and the talks of Jason Scott, technology archivist (most notably “From COLO to YOLO: Confessions of an angry archivist” at Bacon 2014, in London).</Text>
        </Document>
        <Document ID="138">
            <Title>Recording</Title>
        </Document>
        <Document ID="25">
            <Title>Evaluate them</Title>
        </Document>
        <Document ID="30">
            <Title>3D-backups-serrano</Title>
        </Document>
        <Document ID="26">
            <Title>Tech Heirlooms - considerations</Title>
            <Text>Technology Heirlooms? Considerations for Passing Down and Inheriting Digital Materials
William Odom1, Richard Banks2, Richard Harper2, David Kirk3, Siân Lindley2, Abigail Sellen2
Carnegie Mellon University1 Human-Computer Interaction Institute PA 15213 Pittsburgh, USA wodom@cs.cmu.edu
ABSTRACT
Material artifacts are passed down as a way of sustaining relationships and family history. However, new issues are emerging as families are increasingly left with the digital remains of their loved ones. We designed three devices to investigate how digital materials might be passed down, lived with and inherited in the future. We conducted in- home interviews with 8 families using the devices to pro- voke discussion about how technology might support (or complicate) their existing practices. Sessions revealed fami- lies desired to treat their archives in ways not fully sup- ported by technology as well as potential tensions that could emerge. Findings are interpreted to detail design considerations for future work in this emerging space.
Author Keywords
Technology Heirlooms, Memories, Digital Inheritance
ACM Classification Keywords
H5.m. Information interfaces and presentation (e.g., HCI): Miscellaneous.
INTRODUCTION
Material artifacts are passed down across generations of family members as a way of sustaining social relationships and bolstering ideas of shared heritage, history and values. These heirloom objects often offer connections to the past that extend before and potentially beyond the current owner’s life. As we live more of our lives “online”, it is interesting to ask how digital content will find its place among these physical collections of things that connect us to the past. After all, digital technology makes it possible for people to accumulate vast and diverse digital archives. In the future will children look back over their grand- mother’s digital photos or Facebook content to explore what her life was like? Will these digital things be passed down the same way as physical things are?
Microsoft Research Cambridge2 Cambridge CB3 0FB, UK {rbanks r.harper, sianl, asellen}@microsoft.com
Newcastle University3 Culture Lab Newcastle, UK, NE1 7RU david.kirk@ncl.ac.uk
￼Figure 1. The three ‘technology heirloom’ devices: the Timecard (left), BackupBox (center), and the Digital Slide Viewer (right).
Research in the HCI community has illustrated a diverse range of ways people are drawing on digital objects to re- flect on and reminisce about the past [e.g., 14]. Very recent work has described new complications that are emerging as loved ones pass away and leave complex assortments of digital remains for the living to come to terms with [e.g., 16, 19]. Many of these issues point to the fact that we are seeing a proliferation of personally meaningful digital arti- facts. However, little work to date has progressed beyond explorations of current practice to explore how these sensi- tive materials might persist over time, across owners and across generations in the future.
With this in mind, we designed three devices (see Figure 1) as a way of encouraging people to think more concretely about how digital materials might be inherited in the future. The aim was to use these design artifacts to explore how the processes of passing down digital materials among family members might be better supported as well as to reveal po- tential unintended consequences that could emerge. They are: the Digital Slide Viewer, which packages treasured family photo albums in the form factor of a traditional slide viewer; Timecard, a device that enables people to assemble, present and hide away digital content of multiple family members along a chronological timeline; and Backup Box, which locally stores a person’s Twitter archive on a daily basis in a form that can be handed down. We conducted in- home interviews with 8 families, using the devices to pro- voke discussions about how technology might fit within (or complicate) their practices of inheriting and passing down digital collections in the future. These sessions opened up discussions that provided insights into how families desired to treat their archives in ways not fully supported by tech- nology. They also revealed emergent tensions as members critically considered futures embodied by (and beyond) the devices and reflected on consequences that could emerge. With these findings in mind, this paper concludes with a
￼Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
CHI’12, May 5–10, 2012, Austin, Texas, USA. Copyright 2012 ACM 978-1-4503-1015-4/12/05...$10.00.
discussion of four design considerations aimed at sensitiz- ing the design space toward better supporting the work of inheriting, living with and passing down significant digital materials: designing technologies to be put away; support- ing the moral work of safeguarding; enabling multiple roles; and enabling multiple representations in the archive.
BACKGROUND AND RELATED WORK
Artifacts play important roles as triggers for personal and shared memories. Over time these things signify our rela- tionships with each other and can mediate how people re- member their loved ones. The roles material artifacts play in supporting personal and familial memory [2] as well as ideas of family history and heritage [10] have been central concerns across several disciplines in the social sciences and humanities. Currently, there is a growing literature ex- ploring the process of passing down objects as not merely reflecting our relationships with loved ones, but in essence constituting them over time [6]. A special emphasis has been given to how objects signify human relationships with the living as well as stand in as proxy for the departed [17].
With the increasing presence of digital artifacts and systems in everyday life, the nature of human interactions is shift- ing—people now commonly mediate between material things and digital technologies. It is not surprising then that research related to the effects of digital artifacts being left behind by departed loved ones is starting to emerge in quite a vital way. Based on an empirical study of bereaved fami- lies, Massimi and Baecker [16] speculate on future chal- lenges related to digital inheritance, including: the claiming problem—digital materials lack clear affordances for inher- iting, and the afterlifelog problem—reimagining the role of digital materials representing the lives of departed family members could provide opportunities for family members to remember loved ones. In a related study, Odom et al. [19] describe how relationships with the departed loved one continues to evolve, often mediated by inherited objects. It suggests concerns such as supporting the endurance of a cohesive archive and developing richer tools for contextual- izing inherited digital content.
More generally, there exists a history of research in the HCI community exploring the recording and archiving personal or family memories. Kaye et al. [13] describe how digital archives could better support the work of self-reflection and understanding. Kirk &amp; Sellen [14] present a values-oriented approach to support the archiving of families’ cherished digital materials. Importantly, they highlight how the movement and storage of artifacts around the home plays central roles in preserving them for future generations.
Additionally, several projects [e.g., 8, 24] have designed and studied devices in laboratory environments that, in varying ways, enable families to attribute audio annotations to physical objects and digital photos. These studies specu- lated that shifting interaction away from the PC and toward dedicated devices may be more appropriate for supporting
social practices of storytelling. Outside of the lab environ- ment, Petrelli et al. [21] present a rare example of how reminiscence could be triggered by encasing audio record- ings of family events in the form of a vintage FM radio.
Collectively, these strands of research have made important contributions to understanding how interactive technologies could better support digitally capturing family memories and revisiting them. They also reveal how new problems are emerging as members attempt to make sense out of in- herited digital content, and consider how they themselves will pass down their digital legacy. Our work attempts to bring these strands of research together. We want to inves- tigate how digital technology might fit within (or compli- cate) families’ existing practices, and how the design space could be critically developed through and sensitized by these understandings. Beyond work that has gone before, we do this by grounding discussion around a set of working prototype devices that aim to make concrete new ideas for dealing with families’ growing legacies of data.
METHODOLOGY
We designed three working devices to critically explore potential future interactions, experiences and practices sur- rounding the inheritance of digital content. Although these objects offer some diversity in design we synthesized a methodological approach that united them. Specifically, we used them to provoke reflection on the materials themselves and encourage a dialogue about (and beyond) the stances and potential futures they embody. Our methodology drew from a number of approaches, including speculative design [9], critical design [3], reflective design [23], technology probes [12], and design-oriented HCI [5].
The process leading to the development of these devices consisted of the following. We conducted review sessions of theoretical literature and empirical studies (many are noted previously). We then ideated many design concepts and progressively refined and clustered several conceptu- ally related sets to construct an understanding of the overall design space. Comparable to Schön’s notion of design as a reflective conversation with materials [22], we engaged in a reflective dialogue with theoretical and empirical materials, and iterative development and critique of the design con- cepts themselves, to arrive at our final devices.
We intended the form and presentation of each respective device to be resolved to the extent that, at first glance, they might appear relatively familiar in comparison to other do- mestic artifacts. We wanted the devices’ material aesthetics to, on the surface, evoke a sense of the warm qualities asso- ciated with antique or heirloom objects (e.g., veneered oak composing an old chest compared to plastics encasing many contemporary appliances). The three devices are designed as a visual family, each encased in a European Oak veneer with a single surface of color. Further, the digital technol- ogy of each artifact is integrated into a form characterized by affordances that enable them to be fluidly opened up and
put away. These design choices were influenced in part by prior work illustrating how the qualities of certain materi- als, such as wood, can inspire a perceived sense of durabil- ity [20]; and how the invocation, experience and putting away of inherited objects—digital and physical—appears central in supporting meaningful, self-determined interac- tions with them [19].
Nonetheless, it is important to point out that the notion of ‘designing an heirloom’ can seem contradictory. The ways in which an object achieves heirloom status is highly idio- syncratic and heterogeneous; what one family may regard as an heirloom will likely not retain the same meaning for another. Additionally, heirlooms often directly owe to the people that possessed them previously and the material his- tories inscribed through their use over time.
Thus, it must be stressed quite crucially that we did not aim to evaluate our design concepts per se. Indeed, a more tradi- tional ‘evaluation’ would require a deployment for many years—if not decades—to understand how the devices shaped people’s practices and experiences as they accumu- lated digital content and were (or were not) passed down to another generation. Rather, we used the devices to provoke discussion around—and beyond—the potential futures they might embody and inspire; and to explore issues and in- sights that emerge through these discussions. Additionally, we populated the devices with digital content from a re- search team member’s personal collection, as opposed to each family’s specific content. This team member’s digital content captured years of personal and family experiences, as well as materials left behind after the loss of a close elder family member. Nonetheless, this clearly has limitations. The digital materials left behind by, for example, a teenager or middle-aged person would be different. However, this approach did appear effective in providing families with enough context to understand and relate the devices to their own lives, while remaining open enough to encourage them to envision new ideas or uses. In what immediately follows, we describe each of the concepts in turn, and then provide details on our participants and study.
The Digital Slide Viewer is a device for the local archiving of different collections of a family’s digital photographs (see Figure 2). The device is an augmented vintage analog slide viewer popular in the United Kingdom in the 1970s. Physical slide tokens, laser cut from acrylic, symbolically correspond to photo albums previously stored online or locally by a family. The slides and viewer are stored and organized in an oak case. Each slide has a unique strip of color on its back, which is recognized by a color sensor to determine which album should be made viewable from in- ternal memory. When a slide is inserted, the photos in the corresponding album become viewable, which may be se- quentially explored by tilting the device left, to move backward, or right, to move forward, in the set. The digital slide viewer is driven by a Gadgeteer [18] microprocessor board, which several sensors and devices are plugged into,
including: a 100x100 pixel display; an SD card (in an inter- nal SD reader) for image storage; an RGB reader for detect- ing a unique color present on each slide token (to invoke different photo collections); and a breakout board with two tilt sensors for supporting navigation. A mini USB connec- tor powers the device. Content for the photo albums was supplied by a research team member and models their exact organization. These 20 albums cover a diverse range of events over several years, including family trips and mo- ments in a young child’s life as well as mundane experi- ences (e.g., a family informally creating artwork together).
Figure 2. From left to right: The viewer in case with the slides; View of a photo; Families often desired to store the slide viewer in spaces where other significant artifacts were kept.
Issues framing the rationale for this concept included: How would the form and presentation of this device be perceived to support or complicate participants’ existing practices of viewing family photos, against the backdrop of their own physical and digital albums? How would integrating digital photo albums into an artifact that may already be familiar to some members shape perceptions of these digital materials?
Timecard enables family members to construct and present a timeline representing the life of a loved one, which is stored and displayed on a dedicated device (see Figure 3). Timelines can be created for a departed family member as a form of memorial, or simply to map the lives of several family members as a matter of preserving family history. Family members can add digital content (e.g. text, images) to the system via a web interface and backend online serv- ice, which is used to transfer content locally to the device. During the upload phase, people are able to attribute spe- cific dates to the content, which dictate where items appear on the timeline. The Timecard case includes doors that en- able it to easily be opened up or put away; the touch screen sits behind the doors. It is stand-alone and can sit of a shelf or on display elsewhere in the home. A fanless mini-PC runs the Timecard application displayed on the screen.
Figure 3. From left to right: Children from F4 interact with his- torical metadata; The timeline UI view; Several families placed Timecard (closed up) on display with other things in the home.
Photos can randomly cycle in full screen mode. Touching a photo brings up a timeline view of all the images of a per- son chronologically; the timeline (and collated content) can
￼￼
then be explored via the touchscreen. In addition to per- sonal annotations, family members can attribute metadata of historical events (scraped from Wikipedia) to the time- line to help better contextualize the life and times of an an- cestor. We speculated this design choice might make the life stages of different ancestors more meaningful for future generations. A research team member that had recently ex- perienced the loss of an elder close family member pro- vided the content Timecard presented in this study. This included physical objects and photos that he had been be- queathed (which were later scanned), as well as photos over the years that depicted the member in different life stages.
Issues framing the rationale for this concept included: How might technologies fit within, extend or complicate fami- lies’ practices of remembering and commemorating the lives of loved ones? How could these narratives be passed down and how could chronology affect these practices? We were also interested in where families perceived they would keep an artifact like this in their home and how it would be treated considering its potentially sensitive nature. For ex- ample, would enabling content to be made public shape perceptions of its placement in storytelling practices?
BackupBox is a digital store of a lifetime of Tweets posted to the micro-blogging website Twitter.com (see figure 4). Through a WIFI connection, it copies messages from the internet to a self-contained hard drive. There they are pre- served for a future time when they might be drawn on as a resource to revisit the mundane and extraordinary moments of a family member’s life captured by their Twitter account. We selected Twitter in contrast to other social media ac- counts (e.g., Facebook) as we speculated the 140 character limit for each entry would produce more concise and easily accessible entries. However, during the study participants speculated on how their own digital materials (e.g., Face- book content) might relate to—and extend beyond—the BackupBox concept, which we will discuss in detail later.
Figure 4. From left to right: The removable lid; Mom2 presses a icon to open a Tweet; UI design for an opened Tweet.
The physical form consists of a box with a removable lid, intended to conceal the growing archive of digital materials so as to not attract attention, while still inviting exploration if a family member chooses to open it up. The user interface presents Tweets in chronological order along the X axis; the Y axis indicates the time of day each Tweet was posted. The interface is navigated via a touch screen and each Tweet item in the timeline is symbolically represented as a non-descript flower; touching a specific element will pre- sent the contents of the message. A fanless mini-PC runs
the BackupBox application displayed on the screen. Con- sidering the potentially sensitive nature of some messages, we speculated this design choice could provide an addi- tional layer of comfort by requiring people to physically invoke the content beyond just removing the lid. The Twit- ter content on BackupBox at the time of the study was ar- chived from nine months of the device routinely backing up one of our research team members’ Twitter account.
Issues framing the rationale for this concept included: Would the BackupBox surface tensions around the proc- esses of passing down personal digital content that is cre- ated and stored online? Would family members perceive a physical instantiation of a digital service to be valuable? Would family members perceive social media content, such as Twitter data, to be similar or different to existing percep- tions of materials to be passed down in a family archive?
Participants and Data Analysis
We recruited 8 families (F1-F8) from the southeastern re- gion of the United Kingdom to participate in our study. This approach clearly has limitations; for example, it makes the results hard to generalize to another population of users. However, we wanted to focus on a specific group to gain a richer descriptive understanding of the space as a whole to inform what might be salient issues for future research. Two parents from each family participated (with the excep- tion of (F5); only the mother participated). All families had at least 1 child; F2, F4, F5, F6 and F8 all had young or teenage children, all of whom participated in the study. F1, F3 and F7 had children in their early to mid-twenties which all lived outside of the parents’ home; 4 out of 5 of these young adults participated. Three families (F1, F6, F7) had members representing two generations that participated (i.e. children and parents); the remainder had members repre- senting three generations that took part in the study (i.e. children, parents and grandparents). 5 of the 8 families had experienced the loss of at least one grandparent in the past 5 years; all inherited objects from these experiences. In total 36 people participated in the study—15 children (ages rang- ing from 9-25), 15 parents (mid 30s-early 50s), and 6 grandparents (late 60s-late 70s). The occupations of parents ranged from schoolteacher to IT consultant to plumber; occupations of non-student children included sales atten- dant, law clerk, and barista; all grandparents were retired. We recruited this participant pool as they could offer a range of experiences with physical and digital objects.
All interviews were conducted at the parents’ home, where family members collectively convened prior to the inter- view. The choice of the parents’ home appeared most ap- propriate as they typically housed an assortment of artifacts ranging from heirlooms that had been passed down over at least one generation, to objects that were anticipated to be passed down to their children. One home visit was con- ducted per family and lasted between 2 to 3 hours. Visits began with parents (at times together with grandparents
￼
and/or children) giving us a tour of their home, with em- phasis on where they kept heirlooms or objects that might become heirlooms. They were asked to describe stories associated with these artifacts, how they were received, who is responsible for them, and reasons for keeping them in particular spaces. We also explored if members pos- sessed digital collections they desired to hold onto (and potentially pass down), and where they were kept. We then asked members to gather a selection of artifacts emerging in the tour and to arrange them in a central room in the home. This was to provide a rich backdrop of participants’ posses- sions that could serve as a basis for comparison when ex- ploring the devices.
All participating members then reconvened in the central room (often living room or kitchen). We conducted a brief discussion to clarify experiences surrounding the artifacts arranged in the room. We then began sessions using the devices. We were careful to make clear that all the devices are concepts to be used as starting points for discussion about and beyond them; family members were encouraged to envision what they would (or would not) want them to be. One device was introduced at a time, and each had a specific semi-structured session conducted with it. How- ever, members were free to go between devices if desired. For each device, researchers offered a short narrative pro- viding background context, illustrating how it could be in- teracted with in the process. These introductions were kept brief. Emphasis was placed on family members exploring the device and coming to their own interpretations of it; they were encouraged to imagine what kind of future each device projects and consider what that would be like.
At appropriate moments during sessions of exploration and discussion, we posed open-ended questions. Questions were designed to critically elicit reflections on topics including: how narratives persist with personal artifacts as they are passed down; how and when cherished objects are used; what kind of family ‘image’ they construct; how physical and digital archives are maintained and how the social roles of members surrounding their care may change; and where they will go when they are passed down. Members were asked to contrast their descriptions with how the device might or might not fit within their practices. We altered the order devices were introduced to families across the study. After all devices had been discussed, we asked members to take us on another tour of their home, this time considering where they would keep them in their home and why.
All interviews were audio recorded, which resulted in nearly 20 hours of recordings; photographs were addition- ally taken to document objects and spaces discussed during the interview. We listened to recordings and transcribed segments relevant to heirlooms and interview questions (as opposed to general chat), which were organized into themes. Meetings were held with the research team to dis- cuss and corroborate emergent themes; we coded the textual
documents using these themes. In addition, we created af- finity diagrams using sticky notes to order findings across families and reveal unexpected connections.
FINDINGS
In what follows, we present several examples taken from field observations with families, which we feel capture the core themes emerging across our interviews. We refer to participants by their role — GF (Grandfather), GM (Grandmother) Mom, Dad, S (Son), D (Daughter) — fol- lowed by a number indicating the family. In the case of children, the reference includes a second number indicating the child’s age. For example D4-13 would stand for a 13- years-old daughter from family 4.
Figure 5. Family members interacting with the Technology Heirlooms during in home interview sessions.
The storage and safekeeping of family heirlooms
Interviews in families’ homes revealed a diverse range of material and digital artifacts members kept and desired to pass down. In what follows, we first describe families’ per- ceptions of their material heirlooms and their digital collec- tions. We then detail how families drew on the devices to envision alternatives to better support their practices.
Despite representing some of their most valued possessions, families commonly described ‘using’ their heirlooms infre- quently, at times several years lapsing in between these instances. It was also common for families to clearly differ- entiate heirlooms from other domestic objects: “We don’t use them like you’d use a [television] remote. ...Their pur- pose is something bigger.” (Mom3). Instead, practices sur- rounding heirlooms were bound up with having them pre- sent and ensuring their safekeeping. Dad1 describes an al- bum containing photos and memorabilia of his family’s ancestors: “we rarely go back to them. ...it’s having that peace of mind that they’re there [motioning to bookshelf] and we’ll see to it that they’re there until it’s time for my kids to take them.”
Safekeeping was understood as occurring across genera- tions and was bound up with the passing on of items. In
￼
some cases, older members preemptively passed down heir- looms to ensure their transfer to the next generation: “...making sure [they] make it through time, that feels as important as the things themselves. ...telling my daughter what they mean, the people they represent, while she has them, that’s going to help them last” (GM5). Similar to their material heirlooms, families sought to safeguard treas- ured digital collections for future generations. These in- cluded things such as: digital photos, videos, documents, and to some extent, artworks and music.
Various tensions were bound up with the notion of safe- guarding digital collections, however, especially relating to practices surrounding their backing up. For example, it was a common strategy for families to use external hard drives to back up their digital collections. However, in some cases the extra task (and hassle) of updating a secondary storage location led to the external hard drive being routinely ne- glected. In others, families described a general distrust over the longevity of their personal computers, which led them to create extensive backups on physical media (e.g., CDs or DVDs). Tensions also emerged with this approach, namely due to doubts over how long these media would last and the physical space their storage required. Other concerns in- cluded the potential to lose the physical media: “the prob- lem with CDs is if we lose one ... we’d lose a whole a chap- ter of the kids growing up” (Dad8); as well as concerns that the aesthetics of physical media failed to convey the pre- ciousness of the content. As Mom7 put it: “they deserve better than that.”
The use of online services to store digital family collections is an alternative to creating local backups, and members from all families reported using photo sharing services (e.g. Facebook, Flickr) or email to share select family photos with specific people to varying extents. However, these services were viewed as supporting sharing rather than the safekeeping of sentimental content: “We put things online to share them, not to preserve them. ...all our intimate [digi- tal] memories, we want to know where they are, keep them in order. ...the thought of them being where someone could get at them. That makes us uneasy” (Dad6). Parents in two families (F4, F7) maintained accounts through the cloud storage service dropbox.com, and similar concerns also emerged: “I’ll put things for my work or my music in drop- box, but I wouldn’t put anything too valuable to us there. What if our account was hacked or deleted? ...it feels too risky” (Dad7). When possible, we probed teenagers’ per- ceptions of storing content online. They typically reported fewer immediate reservations about hosting personal con- tent online, but tended to react strongly against integrating their own digital content into family collections.
Unexpectedly, some families drew on the physical forms of Timecard and, especially, BackupBox to propose alterna- tives that might help alleviate some of tensions mentioned above. While these ideas varied, they were united insofar as they proposed that a storage system distributed among peo-
ple was an appropriate way to preserve familial content. Consider Dad4’s reflection: “my brother, my wife’s brother. ...they would be the guardian of our kids if we passed away. We’d do the same for them. ...it makes sense that they could guard our [digital] things and we could do the same. ...So if one of our homes burned down or our thing [i.e. device] died, there’d still be one or two copies out there, like at my brother’s or at Mum’s place. Same would go for them.”
Dad4’s reflection captures how some families drew on the devices to propose potential uses of technology that might better support their desires to safely preserve precious digi- tal archives over time.
Embodied digital forms: settling in and setting the tone
The embodiment of digital content in physical forms con- veyed through our devices provoked discussions across home visits. Below we detail how families saw ways in which physical properties might enable them to treat, relate to and live with sentimental digital content.
A primary theme across interviews centered on how captur- ing digital family archives in forms distinct from the com- puter might both project and engender a deeper sense of care for these materials: “Putting our family photos and videos and all in a different folder [on our computer] doesn’t do them justice. There is so much on [our computer] that we won’t give a toss about in a year. ...our photos, videos, that’s the bit that matters. ......[The devices] get away from all the clutter. ...they show you care and makes you want to care for them, tend to them” (Mom3). GM5 similarly noted, “there’s something about being able to say ‘what’s important, it’s all in here’ and pick it up, give it to someone or keep it in a special place that suits it.” Other families speculated on potential benefits of storing digital content in domestic spaces populated by their treasured material things. For example, when considering where the Digital Slide Viewer would be stored in their home, F1 se- lected a small living room cupboard that housed several sentimental items: “...having it packed up next to the Chi- nese boards and albums and medals. ... seeing it age with them, the things we’ll always have. It feels right. ...we want to hold onto our [digital] family photos like those things I suppose. Putting it there makes it feel like it’s findings its place. ..with our things, in our home” (Mom1).
Four of the eight families (F3, F5, F6, F8) we interviewed possessed only a single computer, all of which were desk- tops set up in home office or kitchen locations. These fami- lies in particular reflected on how moving their sentimental digital archives to other domestic places could better prime interactions with them: “we have this chest. ...It has little trinkets and bits and bobs that we’ve saved over the years, some old stuff from me Mum. ...this is where it [Digital Slide Viewer] should go. Opening [the chest], seeing those things and bringing out the [digital] slide box, that’d be a more natural way of coming to them [photos] than booting up the computer” (Dad6). Mom6 then continued: “We’ve
got this habit about the chest. When we get into it, it sets a tone. It’s time to take a moment and look through them. ...having it [Digital Slide Viewer] in the chest, it’d blend right in. ...with what we’re already doing and the things that’ve always been there.” Mom8 contrasts Timecard’s location in her living room with the home office-based computer: “I don’t walk by our computer in the office and think of the memories that’re on it. ...This feels somewhere in between. ...it’d remind me of the memories in there, but if it’s closed up, we could walk past it and leave it at that. ...That makes it feel like a more complete part of our life.”
Additionally, the vintage form factor of the Digital Slide Viewer itself appeared to help set the tone for reminiscing about the past. Members of several families recognized its form, which led to discussions about their lives when they last used one. After one such discussion with her son and granddaughter, GM2 noted: “seeing something familiar from the past, it triggers all these memories and associa- tions I haven’t thought about in a while. ...it feels like a real way of starting to get back to the past and remember it, with the photos and all.” Often younger members were ac- tively included in these discussions as the device was passed around; in some cases, they initiated discussions themselves: “D4-13: Mummy you had one of these. Is this what you used to look at pictures? M4: Yes I did. It was [grandmother’s], she can tell you where she got it from.”
Collectively, this sample of reflections helps illustrate how giving digital collections physical properties might better support the dynamics of living with them over time, from intentional engagement to simply letting them persist among other significant domestic possessions and spaces.
Curating, Integrating and Changing Roles
Families adopted several practices to construct a meaning- ful whole out of collections of heterogeneous artifacts they desired to pass down. In what follows, we provide an over- view of these practices, before detailing how Timecard in particular provoked discussions about potential benefits and complications technology might present in this context.
A common practice we observed across families was the use of notes and other materials to explicitly detail the his- tory of family artifacts to preserve their meanings. These instances ranged from Dad2’s collection of his great grand- father’s medals and other artifacts from World War One, to Mom6’s scrapbook owing to her own life as well as to several departed ancestors. Across these examples, family members included short notes and, at times, materials de- tailing local and historical events occurring when specific artifacts were in use to help communicate their significance to future generations.
It was also common for families to consciously prune col- lections of important physical materials to avoid creating an archive of undesirable size and scale, and to underpin a sense of coherence. We found both parents and grandpar- ents engaged in this practice and while at times difficult, it
was considered an essential part of ensuring cherished familial artifacts made it to the next generation.
However, the constraints families imposed on their material archives did not always translate to their digital collections. Mom5 contrasts her family’s physical photo albums with their digital collections: “With the [physical] albums we have to decide what to put in there and what’s not quite worth it. ...On our computer they pile up. We have so many photos on there now and we keep taking more. ...It starts to feel endless, really.” In two cases (F5, F6), families elected to print out physical copies of digital photos and integrate them into physical albums, to make them easier to manage and pass down. However, when posed with the question of how (or if) families would wish to cull their digital collections for the future, most members were ambivalent.
While archives of material things supported heterogeneity in a way that digital archives did not, they were typically associated with one branch of a family. Discussions of Timecard highlighted that having a place to collate content from multiple sources would also be desirable: “...thinking about when my Dad passed away. I have some digital pho- tos of him and my sister does, and we both have some of his things. ...If we were able to put some of them [digital things] together, when we’re feeling up to it, that would be meaningful. ...We could have something celebrating his life, and us with him” (Dad7). Mom1 speculated on the potential benefits of distributed curation of sensitive digital materials over time: “having a place where my brother could add an event in one of our parent’s lives and I could leave it for a while, and then add something. ...let things come out slowly over time, that would be valuable. ...it would create a new record of our family.”
Digital archives were also noted for supporting collabora- tion within nuclear families. However, this also raised con- cerns. Some families perceived that this could complicate meaning: “If everyone is putting in things like moments in history or notes about a person, it’s going to make things confusing. There has to be some kind of quality control” (Dad1). Timecard triggered other families to consider how social roles of members would be supported: “We [parents] take most of the responsibility for preserving things about our parents’ lives and our lives with the kids. ...I like how we could all see it and add to it. That is useful for everyone. But we [parents] need to be able to make sure it doesn’t become a mess” (Mom6).
In some cases Timecard triggered intergenerational discus- sions among living family members in the room about past experiences and family history. For example, after interact- ing with a metadata tag relating to the date of India’s inde- pendence (15 August 1947), D4-13 felt prompted to ask her Grandmother about her life during this time period. After describing what her life was like as a young girl then, GM4 reflected on what she remembered of her father immedi- ately following World War II. At the conclusion of GM4’s story, Dad4 remarked: “well, that’s a bit of our family his-
tory I haven’t heard. I wish we could’ve recorded [it] in this box [Timecard], right then and there.” Dad4 highlights the potential value of capturing emergent conversations about family members’ own lives; this opens a space to consider how such records could make interactions with the device richer in the future.
However, some discussion emerged about how perceptions of past experiences can shift over time and how technology could pose challenges: “Even if we remember things from the past the same, the way we feel about them can change. ...like if a photo or summit later reminds us of a falling out we had with a relative. We chuck it in the bin to be done with it. ...So if I put something in there [motioning to Time- card], I should also be able to take it out” (S3-25). His re- marks represent discussions that emerged with the Time- card and the Digital Slide Viewer: the need to take things out of digital archives as fluidly as they are put in.
Tensions over new digital materials in the family archive
Our aim with Backup Box was to provoke family members to consider the potential role of social networking data, such as Twitter updates, within family archives. Backup Box was highly contentious across families. Several related kinds of criticism emerged, as we describe.
Several families possessed diaries written by ancestors now considered important parts of the family archive. These diaries tended to contain mundane information (e.g., a list of household chores completed on any given day) with a sprinkling of extraordinary events (e.g., marriage of family member, birth of child). When asked to speculate on simi- larities and differences among the diaries and Backup Box, family members drew strong distinctions. Dad7’s percep- tion of the difference between his father’s diaries and social media content is exemplary of members of several families’ sentiments: “when I open one of his diaries and see what he wrote, I know he sat down and thought for a moment, and that feels significant. ...with stuff like Twitter, people rattle things off, sometimes without thinking ...the intention is different and I suppose that makes a huge difference.”
Backup Box also raised issues over the potentially vast amounts of social networking content other family mem- bers would have to reconcile with. D3-22 prospectively considered what it would be like to receive her brother’s social networking archive: “He posts stuff to Twitter and Facebook literally all of the time. I can’t imagine how many updates there would be for one or two years, let alone a decade. How would we deal with that?” Other participants speculated on how years worth of Twitter data could trap a small amount of meaningful insights from a person’s life within a sea of trivial entries, potentially making it difficult to explore or let go of: “If I got, say, Mum’s Twitter. I’m sure there’d be some stuff I’d enjoy seeing, but I’d have no idea how to find it. ...I’d probably keep it, but not know what to do” (D1-21).
When D3-22 concluded reflecting on her brother’s social media content (mentioned above), she noted: “And it’s so much about him, but not all that much about us. ...or our family.” This statement captures deeper concerns echoed by members of several families: that social media content is often targeted at different audiences, which could make its place in the family archive controversial. Mom2 describes how this quality could lead to undesirable experiences: “on- line it’s easy to act [in] very different ways to different peo- ple. Even I confess to that, and I wouldn’t exactly want other people to know about this. ...it feels a little scary that we could learn something about someone that maybe we weren’t supposed to know, or didn’t want to.” Teenagers in the families we interviewed typically were frequent users of social networking services, and also reacted against the inclusion of their content in the family archive. D6-17 re- flects on her personal social media content: “I could see looking back on it myself, but it would be weird if other people in my family used it to think about me. I’d rather make myself something that would go in it. ...something that’d show my family something special about me.”
Related concerns also emerged around how a device like BackupBox could cause family members to self-censor the social networking content they posted, or paralyze these practices completely. Some families proposed ways to work around these tensions, such as using a special hash tag or a specific application to send updates only to Backup Box.
DISCUSSION AND DESIGN CONSIDERATIONS
It is clear physical and digital objects hold significant places in families’ lives, and that these are envisaged as retaining this significance over time and across generations. A key contribution of our study is to surface insights on how technology might open up new opportunities for pass- ing down and inheriting digital materials, as well as new complications that they could introduce. Our findings reveal a range of ways families desired to treat and live with their significant digital materials. Several of these cases were characterized by their desires to treat these archives differ- ently, integrate them into more appropriate places in the home, and tend to their care and safety. Other instances suggested how technology might better support social prac- tices of creating more cohesive sets of materials to be passed down, creating archives from multiple branches of the family, and documenting conversations that emerge around them. The devices also raise a range of potential consequences that could emerge if careful consideration is not given to new technological interventions. In what fol- lows we present several research and design considerations for the HCI community that emerged from the study.
Designing technologies to be put away— Similar to mate- rial heirlooms, participants perceived value in supporting the dynamics of living with treasured digital collections, from knowing their location, to tending to their well being, to actively interacting with them. That the physical forms, material qualities and affordances of our devices enabled
them to be packaged away, discretely displayed, or actively explored seemed to resonate with families and some of their existing rituals, practices and values. Reforming digital materials in this way allows them to fit into the wider ecol- ogy of archived materials in the home and situates them within a familiar context of artifact-mediated reflecting, remembering and learning about the past. Beyond designing explicitly for ‘use’, this consideration emphasizes the aes- thetics of integrating treasured digital materials into envi- ronments as a whole over time, a notion parallel to ‘slow technology’ [11]. Collectively, these findings suggest that to support more sensitive and nuanced engagement with cherished digital familial content requires the artful design of technologies that can be put away, drawn on alongside others, and which evoke rich experiences when interacted with. This is more complex than it sounds; comments about the fractious intrusions of waiting for machines to ‘boot up’ are indicative of this.
Supporting the moral work of safeguarding—Notions of the value of ‘deep storage’ [14], redolent in our interviews, highlight clear unresolved tensions between digital and physical materials. For example, the encasing of our de- vices may last 100 years (or longer), while their technologi- cal components may last for only 5 years. This highlights the need to design new storage systems that are extremely robust and can handle sporadic use. There are opportunities to explore combined advances in solid-state storage and low power consumption. Although even with hardware innova- tion it is hard to imagine end users not having to engage with some degree of archive maintenance, as such advances will not resolve significant issues of evolving file format standards and ensuing compatibility issues. We suggest the ritual work of preservation may accommodate issues of safeguarding. Several instances from our findings suggest that tending to material heirlooms is itself a significant act: rituals of care could therefore be appropriated as opportuni- ties for the physical maintenance and updating of these technologies.
While the Cloud offers a technical solution to problems of storage, our findings reveal that knowing where one’s sen- sitive digital materials are located is bound to the sense that one is keeping them safe. ‘Storing’ these kinds of sensitive materials in online places raised concerns, especially in terms of ceding the higher-level social and moral work of safekeeping to a third party service. An approach to safe- guarding raised by participants themselves, which lends particularly well to digital technologies (as opposed to physical possessions), is the distribution of storage devices and the (redundant) copying of familial archives across multiple homes and branched families. This would leverage the value of existing social-familial networks and could help alleviate immediate concerns over the loss of cherished content, while supporting the higher-level work of safe- keeping content in morally and socially appropriate ways.
Enabling multiple roles in the archive—The solution pointed to above would need to be nuanced, however. One of the largest issues our devices provoked families to con- sider was the various roles members play in maintaining family archives, and how they would be supported in these roles by future technologies. From contributing new materi- als, to curating collections (organizing and editing etc.), family members play important roles in sustaining the fam- ily archive [15]. So while technologies might open up op- portunities for mirroring archives across homes, richer combinations would need to be carefully considered.
Families suggested Timecard’s indirect, distributed nature could create an opportunity for mapping family history "slowly over time". In other cases it seemed to open the opportunity to support storytelling and the recording of family history. Both of these opportunities potentially illus- trate how a digital artifact from the past might accrue value through repeated interaction, and resonate with prior re- search [8, 24] suggesting the prospect of integrating multi- ple family perspectives as beneficial.
However, it was clear that family members’ approaches to archiving were expected to differ, and this raised concerns over how 'quality control' could be upheld. These issues raise significant questions for future research. How does the architecture of new technology reinforce the moral ac- countability of access to the content? Who has the right to delete or edit entries? How is this accountability repre- sented in the system? What is the communicative structure that envelops the archive and provision of material within it, and how is this negotiated through technology? Better understanding these concerns seems a crucial part of de- signing new systems to support the persistence of a family’s digital legacy across generations. Research proposing im- plications for ‘forgetting’ as a feature in system design [e.g., 1] could be leveraged in future explorations, as could emerging research on multi-lifespan design [7].
Enabling multiple representations in the archive—While previous research suggested people desired to pass down their social networking content to other family members [19], families across our study reacted strongly against hav- ing a technology like the BackupBox. In particular, these instances highlighted tensions around integrating social networking content from members within the collective family archive. Participants made key distinctions between the thoughtful recording of one's life believed to be re- flected in their ancestors' diaries, and their own practices of posting less mindful social networking content targeted at multiple audiences, often outside of the family. These reac- tions surfaced clear boundaries members had for how they wanted to author their presentation of self within the family archive. Prior work has explored how technology could productively support members in presenting different repre- sentations of their selves to each other through novel tools for curating family photo collections [4]. While there are clear differences between curated photos and social net-
working data, this work could be leveraged to further ex- plore how different aspects of unique social bonds between family members could be preserved, while also leaving space for authorship of one’s self image in the family ar- chive as a whole.
CONCLUSION
We designed three devices aimed at provoking families to consider how technology might fit within their practices of inheriting, living with and passing down digital collections in the future. Families’ reactions revealed several ways digital materials fall short of supporting the values and practices they associated with physical heirlooms, and high- lighted new opportunities for design. While researching in this space is inherently challenging, our methodology pro- vided a way to engage family members in confronting po- tential benefits and tensions projected by our devices and draw on their own experiences to make sense of possible futures—and envision ideas beyond them. These reactions provided salient points to consider as people increasingly acquire cherished digital collections that they may desire to pass down alongside material heirlooms. Based on these findings we proposed designing technologies to be put away, supporting the moral work of safeguarding, enabling multiple roles, and enabling multiple representations in the archive as considerations for future HCI research and prac- tice. Importantly, our devices did not explicitly explore how to address the challenges that the sheer size and scale of meaningful digital content pose as families increasingly amass larger archives. Designing new forms and ways of interacting with massive archives of sentimental materials marks a clear area for future research. Ultimately, we hope this study will inspire future research into how technologies can better support the range of experiences that come with inheriting, living with and passing down treasured digital possessions over time and across generations.
ACKNOWLEDGMENTS
We thank Alex Taylor and Mike Massimi for their thoughts on this paper, as well as Phil Gosset, Tim Regan, and Mark Selby for their help on this project.
REFERENCES
1. Bannon, L. 2006. Forgetting as a feature, not a bug: the duality of memory and implications for ubiquitous comput- ing. CoDesign, V ol. 2, 1, 3-15.
2. Belk, R. 1990. The Role of Possessions in Constructing and Maintaining a Sense of Past. Advances in Consumer Re- search, 17, 669-676.
3. Dunne, T., Raby, F. 2001. Design Noir: The Secret Life of Electronic Objects. Birkhauser.
4. Durrant, A., Taylor, A., Frohlich, D., Sellen, A., Uzzell, D. 2009. Photo Displays and Intergenerational Relationships in the Family Home. Proc. of BCS HCI ’09, 10-19.
5. Fallman, D. 2003. Design-oriented human-computer inter- action. In Proc. Of CHI '03, 225-232.
6. Finch, J., Mason, J. 2000. Passing On: Kinship &amp; Inheri- tance in England. Routledge.
7. Friedman, B., Nathan, L. 2010. Multi-lifespan Information System Design: A Research Initiative for the HCI Commu- nity. Proc. of CHI ’10, 2243-2246
8. Frohlich, D., Murphy, R. 2000. The Memory Box. Per- sonal Ubiquitous Comput. 4, 4, 238-240.
9. Gaver, B., Martin, H. Alternatives: Exploring Information Appliances through Conceptual Design Proposals. In Proc.of CHI ’00, 2000, 209-216.
10. Hallam, E., Hockey, J. 2001. Death, Memory and Material Culture. Oxford: Berg.
11. Hallnas, L., Redstrom, J. 2001. Slow Technology: Design- ing for Reflection. Personal Ubiquit. Comput. 5, 201-212.
12.Hutchinson, H. et al. 2003. Technology probes: inspiring design for and with families. Proc. of CHI ’03, 17-24.
13.Kaye, J. et al. 2006. To have and to hold: exploring the personal archive. Proc. of CHI '06, 275-284.
14.Kirk, D., Sellen, A. 2010. On human remains: Value and practice in the home archiving of cherished objects. In ACM Trans. Comput. –Hum. Interact. 17, 3, 10.
15. Kirk, D. Izadi, S., Sellen, A., et al. 2010. Opening up the family archive. Proc. of CSCW ’10, 261-270.
16.Massimi, M., Baecker, R. 2010. A Death in the Family: Opportunities for Designing Technologies for the Be- reaved. Proc. of CHI ’10, 1821-1830.
17.Miller, D., Parrot, F. 2009. Loss and Material Culture in South London. J. of R. Anthro. Inst., Vol. 15, 3, 502-519.
18. .Net Gadgeteer. http://research.microsoft.com/en- us/projects/gadgeteer/
19.Odom, W., Harper, R., Sellen, A., Kirk, D., Banks, R. 2010. Passing on and putting to rest: Understanding be- reavement in the context of interactive technologies. Proc. of CHI ’10, 1831-1840.
20. Odom, W., Pierce, J., Stolterman, E., Blevis, E. 2009. Un- derstanding why we preserve some things and discard oth- ers in the context of interaction design. Proc. of CHI ’09, 1053-1062.
21.Petrelli, D., Villar, N., Kalnikaite, V., Dib, L., Whittaker, S. 2010. FM radio: family interplay with sonic mementos. Proc. Of CHI '10, 2371-2380.
22. Schön, D., Bennet, J. 1996. Reflective Conversation with Materials. Bringing Design to Software, 171-189.
23. Sengers, P., Boehner, K., David, S., Kaye, J. 2005. Reflec- tive design. Proc. of CC '05, 49-58.
24.Stevens, M, Abowd, G., Truong, K., Vollmer, F. 2003. Getting into the Living Memory Box: Family archives and holistic design. Personal Ubiquitous Comput, 7, 210-216.
</Text>
        </Document>
        <Document ID="143">
            <Title>Style note</Title>
            <Text>The word data, in this dissertation, will generally refer to the computing term for information, as a singular mass entity. I have therefore deliberately used it in the singular, as recommended by the new Fowler’s modern English usage (1996).</Text>
        </Document>
        <Document ID="31">
            <Title>draft-annotated</Title>
            <Text>Introduction
As we shift into the Information Age and a knowledge-based economy, a number of concerns are being raised over the long-term preservation of digital data and artifacts. This literature review aims to get an overview of the current problems, techniques and practices in preserving data on digital media, including the definition and understanding of the new paradigms created by digital preservation, the importance of being able to store things on the long term, the processes used to curate and select content to be preserved, and the various methods used to do so. It also encompasses legal and ethical issues arising from it, and how the paradigms and methods change when applied to different types of archiving (e.g. for researchers, for businesses, and for personal businesses).
Why digital preservation
[Stats of numbers on digital vs physical]
Why digital degrades faster UNESCO’s Threats to digital continuity
UNESCO (2003, pp. 30–31) has identified a list of threats to what they call digital continuity. These are: + “The carriers used to store these digital materials are usually unstable and deteriorate within a few years or decades at most”. Where, in the traditional preservation paradigms for paper-based artifacts, the main rule is that “benign neglect may be the best treatment” (derived from Cloonan (1993, p.596), Harvey (1993, pp.14,140), and Bastian, Cloonan and Harvey (2011, pp.612–613)), this cannot be applied to digital storage media because their lifetime is much shorter. This varies highly with the type of medium used and with the storage conditions; for example, samples by the Digital Preservation Coalition (2008, p. 154) show that D3 cartridge tapes deteriorates after 50 years in optimal conditions (25% RH at 10o) but only one year in high humidity and temperatures (50%RH at 28o). Similar results happen for the more commercially available optical media (down to three months and up to 200 years), which are more sensitive to moisture (Harvey, 2012, p.47).
“Use of digital materials depends on means of access that work in
￼￼
￼particular ways: often complex combinations of tools including hardware and software, which typically become obsolete within a few years and are replaced with new tools that work differently” (UNESCO, 2003). Even if storage media could survive decades or even centuries, technological change means we will not necessarily have the means to access the data later. In other words, “to retain appropriate access to a digital object, we must maintain a mechanism to derive meaning from that object” (del Pozo et al., 2012, p6): for optical media, this includes not only the drives to read the disc, but also a cable to connect the drive to the computer’s motherboard, a motherboard that has a connector for this type of cable, and an operating system that has drivers for this connectivity (Pearson 2009, slide 11). Once we have access to the bitstream from the computer we need to be able to represent it, which means we also need to keep the software in a version that can open the preserved file. There is a myth, kept by both customers and vendors, that media is long-lived: “[An IT vendor] offered us a special polymer that they guaranteed would preserve a CD-ROM for 100 years. They were unable to answer how we would preserve a CD-ROM player for that length of time.” (Weathley, 2003, cited by Harvey, 2012, p. 37)
Poor preservation documentation and metadata means it could be hard for potential users to find the data (UNESCO 2003)
poor preservation (“Critical aspects of functionality, such as formatting of documents or the rules by which databases operate, may not be recognised and may be discarded or damaged in preservation processing.”, UNESCO 2003) + “So much contextual information may be lost that the materials themselves are un- intelligible or not trusted even when they can be accessed” -&gt; poor integrity
“Materials may be lost in the event of disasters such as fire, flood, equipment failure, or virus or direct attack that disables stored data and operating systems”
“Access barriers such as password protection, encryption, security devices, or hard-coded access paths may prevent ongoing access beyond the very limited circumstances for which they were designed”. [This argument is particularly true for web archiving, where crawlers of national libraries retrieve all sort of files to preserve them but do not
￼￼￼￼￼
￼necessarily have the permission or know the rights owner][back that up maybe yo]
Importance of data Library material: for society
“if we librarians do not rise to the occasion, successive generations will know less and have access to less for the first time in human history. This is not a challenge from which we can shrink or a mission in which we can fail” (Gorman, 1997, cited in Harvey, 2012, p. 27)
For businesses (records)
Personal archiving / domestic archiving
Preserving interactive material inc. video games
In addition to the preservation of “static” documents and images, there is a significant desire to preserve video games, which are getting recognised as a form of art similarly to movies and music. The Library of Congress (2007) gave awards for the preservation of video games; there is a growing interest in being able to play “retro” games, but the consoles are obsolete. A large community of hobbyists reverse engineered consoles and produced emulators of these platforms for PC and other platforms different from their intended one. Because no documentation is available, this is a Herculean task; byuu (2011), the developer of an emulator for Super Nintendo games, recalls this for reengineering certain cartridges:
[Low-level emulation] is also a very expensive operation, monetarily speaking: to obtain the DSP program code requires melting the integrated circuit with nitric acid, scanning in the surface of a chip with an electron microscope, and then either staining and manually reading out or physically altering and monitoring the traces to extract the program and data ROMs. This kind of work can cost up to millions of dollars to have done professionally, depending upon the chip’s complexity, due to the extremely specialized knowledge and equipment involved. (byuu, 2011)
Obviously this is not necessarily a route many institutions would like to take for preserving digital objects. Because this also causes intellectual
￼￼
￼property issues, Nintendo and Sony have launched their own preservation programs, allowing players to play “classic” games on the newer generations of consoles; this is a far more accurate emulation (as they do have the internal documentation to create these emulators) and respects their monetary interests (players have to buy the games, whereas in hobbyist emulation, you would have to download the game file).
Useful for researchers in the future
Beyond the governmental and cultural needs for preservation, there is also great need for preserving scientific data. “Data are the foundation on which scientific, engineering, and medical knowledge is built” (Committee on Ensuring the Utility and Integrity of Research Data in a Digital Age, 2009, p. ix, cited in Harvey, 2012, p. 26); therefore it is crucial in a digital era, with enormous data sets and research papers becoming widely public, to ensure this doesn’t disappear.
Understanding our history
There is a need to define what is the purpose of archives. As a tool to learn from the past, or to keep records, but as Cook (2000) eloquently puts it:
“Archives [...] are a source of memory about the past, about history, heritage, and culture, about personal roots and family connections, about who we are as human beings and about glimpses into our common humanity through recorded information in all media, much more than they are about narrow accountabilities or administrative continuity.” (Cook, 2000)
Preservation should, therefore, be considered very important. There is some skepticism to this:
￼
￼￼“It was suggested in 2002 that the last 25 years have been a ‘scenario of data loss and poor records that has dogged our progress’ and that, if this is not reversed, ‘the human record of the early 21st century may be unreadable’ (Deegan and Tanner, 2002). Rhetoric of this kind is common in the literature, but is regrettably poorly supported with specifics and evidence. Alarmist descriptions abound: there will be ‘a digital black hole... truly a digital dark age from which information may never reappear’ (Deegan and Taylor, 2002) if we do not address the problem, and we will become an amnesiac society. [...] It is only possible to conclude, as the authors of the Digital Preservation Coalition’s handbook (2008, p.32) did for the UK, that the evidence of data loss is ‘as yet only largely anecdotal ... [but] it is certain that many potentially valuable digital materials have already been lost’.” (Harvey, 2012, pp. 33–34)
While it’s difficult to argue that preserving digital objects is necessary, there is certainly a need to define what deserves to be protected, and to what level.
Understanding the beginnings of the Internet, as we have shifted in an information society
Preserving an accurate + [integrity] image of our society at a given point Twitter &amp; Usenet
Camille Paloque-Bergès (2011; 2013) proposes that we can already find historical and sociological evidence of a given period in Internet time when browsing archives of Usenet [footnote: An Internet discussion system established in the 1980s and that is widely regarded as the predecessor of the Web and online fora]. She analysed how the way of speaking has evolved [footnote: See also http://wwwtxt.org (or @wwwtxt on Twitter), a project collecting short phrases of the Usenet to demonstrate this] and how an self-organised community had built itself, noting that we can “feel [the community’s] informational generativity” and understand that one of the biggest qualities of the early users of the Internet was their “recursivity as a public” (Paloque-Bergès, 2013); that is, how they welcomed newcomers — notably through the use of FAQs [footnote: Frequently Asked Questions], which weren’t a common concept back then. There are also large elements of nostalgia coming back (Paloque-Bergès, 2011).
It is also interesting to come back on the archival of Usenet. The messages were not systematically archived, and obviously not available on the Web as
￼it did not exist and is separate. Today’s collections have mainly been recreated from donations of people who did archived Usenet newsgroup, originally to DejaVu News in 1995, and Google Groups in 2001. A number of privacy concerns were raised with the publication of these searchable archives; messages who were, 15 to 20 years earlier, thought to be confidential are now considered as digital heritage. There was also a large backlash against the archiving made by for-profit companies, and the credit taken later by Google for its donors, with other critiques like Brad Templeton’s (an Internet veteran and important figure; cited in Paloque- Berges, 2013) remark that the archive was lacking in integrity. However, the searchability of these archives did bring welcomed technical possibilities for an Internet historiography, including a timeline where “the announcement of the Chernobyl disaster on Usenet stands alongside the first message of Linus Torvalds calling for help on what would become the Linux operating system” (Paloque-Bergès, 2013).
Similar concerns appeared at the announcement of the archival of Twitter by the Library of Congress in 2010. The two are not really comparable, mostly because the archive is not public and is made by a not-for-profit body. On the mailing list of the Association of Internet Researchers, the idea was met with interest: “The public twitter stream is of historical cultural significance and is an amazing repository of mundane moments in the daily lives of many people and records of what they thought important” (Baym, 2010) [on http://listserv.aoir.org/pipermail/air-l-aoir.org/2010- April/021125.html]”. However, the privacy concerns (notably with public and private feeds) were discussed, and as Michael Zimmer remarked:
“This is the classic “but the information is already public” argument that, while technically true, presumes a false dichotomy that information is either strictly public or private, ignoring any contextual norms that might have guided the initial release of information or how a person expects that information to flow.” (Zimmer, M. [on http://listserv.aoir.org/pipermail/air-l-aoir.org/2010-April/021136.html]; see also his paper (2010) on Facebook privacy, going further in that argument)
Paloque-Bergès (2010) theorises this argument, suggesting that “information is not an object, but a statement; it goes through socio-technical processes; it doesn’t have an objective nature”. A message in a tweet can have a very
￼
￼different context when taken apart of the moment it was posted. It also emphasises that privacy could change (the user could, later, switch their account to Private, delete the tweet or their account altogether, but this wouldn’t be reflected in the archive. We cannot know what is the user’s original intent when they posted their message, and, as Paloque-Bergès (2010) noted, we aren’t sure the user themselves know.
“More often than not, in a technical ecosystem in which making content private is more difficult than sharing broadly, teens choose to share, even if doing so creates the impression that they have given up on privacy. It’s not that every teen is desperate for widespread attention; plenty simply see no reason to take the effort to minimize the visibility of their photos and conversations.” (Boyd, 2014).
Advantages over physical preservation Mass preservation, easier access and organisation Recording digitally material that’s not born-digital
Cost of restoration; case studies of lost data
A number of institutions have already experienced loss of data, or data being corrupted. This can have disastrous consequences for businesses, for example, as noted by McAteer (1996, cited by Ross and Gow, 1999, p. iii): “of the 350 companies unexpectedly relocated by the [1993] World Trade Centre (NYC) bombing 150 [43%] ceased trading, many because they lost access to key business records held in electronic form”.
Data can, obviously, be sometimes restored. Ross and Gow (1999, pp. 39–42, referenced in Harvey, 2012, p. 34) provide a number of case studies. Harvey (2012, p. 34) also gives the ironic example of the website for the “Functional Requirements for Evidence in Recordkeeping” project, directed by the University of Pittsburgh which was accidentally deleted; but thankfully was automatically saved by the Internet Archive.
There is still the possibility of restoring data by specialised carriers; however Pearson (2009, slide 15) notes that while this was generally effective, this did not work on some items. He also regrets that the
￼
￼restoration loses all metadata in the process; this can be an issue for sensitive or classified information (for business or governmental archives) as the carrier would be able to see it; finally, it is a prohibitely expensive operation, and so a carefully planned preservation could be much cheaper (and obviously more reliable) than neglect. However, these services are always useful in certain cases. Harvey (2012, p. 41) cites the statistics by the data recovery giant Kroll Ontrack (2011): “hardware and system problems cause most customers to approach them (29 per cent), followed by human error (27 per cent). Software corruption or program problems, computer viruses and natural disasters are the other reasons listed.”
Defining paradigms for digital
preservation
Pre-digital paradigms
Inability to preserve the medium, only the bitstream Importance of metadata, relationship &amp; context Definition of terms
What to preserve / Selectivity
Harvey 5 pillars for selectivity (Harvey 2012, p. 26)
If unique information objects that are vulnerable and sensitive and therefore subject to risks can be preserved and protected
If preservation ensures long-term accessibility for researchers and the public;
If preservation fosters the accountability of governments and organisations
If there is an economic or societal advantage in re-using information
￼￼￼￼
￼If there is a legal requirement to keep it (NSF-DELOS Working Group on Digital Archiving and Preservation, 2003, p.3).
Banks reasons for domestic archiving
In The Future of Looking Back, Richard Banks (2011) analyses the reasons that lead us to keep or discard physical objects at home. He argues that while making a selection is a positive thing because it forces us to filter our belongings and focus on what is really meaningful, we also run the risk of ridding ourselves of things that we wish we would have kept in retrospect. He points out that the organisation of digital files make this trickier: because we we can keep pretty much everything on virtual space, it is easier to accidentally delete meaningful things alongside the less important. (Banks, 2011 p. 22).
In the context of personal archiving, Banks notes that in the past, the legacy we leave behind us was a highly selective one. Because our possessions are limited by physical space, our ancestors until now “have implicitly indicated that an item is significant through the simple act of keeping it. Although thousands of objects likely crossed their path over their lifetime, the constraints of money and space meant that only a few were retained, often the most significant.” (Banks, 2011, p. 78). He does however argue that this is no longer true in a digital world, because new possibilities mean we simply accumulate digital objects without necessarily ordering them by importance.
Technical possibility of archiving Pros/cons of archiving the entire internet
There are various controversies around whether we should automate the archiving of the Internet, or if we should operate a manual selection. UNESCO (2003, p. 73) notes that those who value a more comprehensive collection (i.e. archiving “all” the Internet) argue that “any information may turn out to have long-term value, and that the costs of detailed selection are greater than the costs of collecting and storing everything”. This argument is particularly true because of the digital media: “unlike non-digital material such as paper-based artifacts, where there is a period of time in which to make selection decisions before deterioration of materials becomes an issue, the time frame for deciding whether or not to preserve digital materials is very short” (Harvey, 2012, p. 57). UNESCO (2003, p.71)
￼
￼backs that claim: “it may not be possible to wait for evidence of enduring value to emerge before making selection decisions”.
Other people [Cook?] think that it would be preferable to keep a curated collection because of a higher technical quality, and this also allow us to settle [negotiate?] preservation rights with the content producers. Harvey adds that we also do not have the technical capabilities to find all the pages on the Internet, and even if we had we would not have the technical possibility to store the whole of the Internet (Harvey 2012, p. 58); again, this is also something recognised by UNESCO: “‘there are usually more things – more information, more records, more publications, more data – than we have the means to keep” (2003, p.70).
The UNESCO (2003) and Cook (2000) both however recognise that a middle ground should be found and that both approaches are valid depending on the goal of the preservation. For instance, the Internet Archive[IA] is going through an automated process (archive as many pages as possible) but, because of technical limitation, only preserves the text and dismisses all other images, stylesheets and external media, which could be critical in some instances. It seems evident that for valued documents these should be retained. The British Library’s Web Archive[ukwa] [http://www.webarchive.org.uk/ukwa/]
Dangers of selection (Cook)
However, Cook (2000) points out the dangers of manual selection for what is worth archiving: “With memory comes the inevitable privileging of certain records and records creators, and the marginalizing or silencing of others” (Cook, 2000, cited in Harvey p. 58). He asserts that the archivists must be impartial and objective, and ensure all points of view are kept to ensure our shared memory does not “becomes counterfeit, or at least transformed into forgery, manipulation, or imagination”. This is something that is clear with manual archiving, but could also be a problem, maybe unintentionally, with the automated discovery of content when archiving Web pages.
Automatic selection (Reputation model)
In order to keep the most valuable content, algorithms could be used. Kawano (2008) proposes to use reputation models, that attempts to
￼determine the relative value of a Web page by considering its “importance, fairness, trustiness, uniqueness and valuation”. This is done using a variety of ways; the algorithm suggested by Kawano relies on the consistency of a web page, that is the amount of time it existed, the number of links it contains and the number of external links to this page (Kawano, 2008, p. 291).
Crawling
Manual curating
Special Collections for events (eg before elections, see webarchive.org.uk for other ideas)
Another interesting selection process that has emerged is the one of specific archiving when certain events are happening. For instance, the UK Web Archive[ukwa] has, on top of their topical-based collections (Energy, Health and Social Care...), a few event-based collections (Queen Jubilee, Olympic and Paralympic Games, London 2005 terrorist attacks...) which reflect the state of the Web at that given moment in history. Masanès (2006, p.83) notes that this type of archiving must be manually curated and thoroughly preparated. He gives a methodology for this, using presidential elections as an example: a time frame for the collection must be defined (“3 months before the election + 1 month after”), what type of sites will be archived and when (“the political parties’ blogs and websites, every week; analysis, commentary and humourous/satire websites, every month; online newspaper articles, once”).
Selection: by genre
There are different genres of websites: institutional website, blogs, personal pages, forums. [[Genre have been studied in the context of the Web to see how they replicate or diverge from the genre in the print- ing world (see Crowston and Williams 1997) - in Masanès p88]]. An interesting point about this is that often, robots (crawlers, or later data analysis) can automatically identify their type [Rehm 2002 -&gt; check reference, Masanès:88] which permits the automatic generation of metadata, or simply to restrict a collection of web pages to certain Web genres.
Web archiving: what stands as
heritage on the web Who should preserve
Digital media changes the original preservation paradigm for deciding who takes the responsibility for archiving content, and how. The processes of creation, fabrication and publication used to be distinct in the “analog” era; but as Nurnberg (1995, p. 21, cited in Harvey, 2012, p. 9) said, “technology tends to erase distinctions between the separate processes of creation, reproduction and distribution that characterize the classic industrial model of print commodities”. Self-publication (particularly on websites) is much more common, and this is also true for businesses and personal data. The Task Force on Archiving of Digital Information (1996, pp. 19–20) suggests that when, in general, “stakeholders disseminate, use, reuse, recreate and re-disseminate various kinds of digital information, they can easily, even inadvertently, destroy valuable information, corrupt the cultural record, [and ultimately thwart the pursuit of knowledge that is their common end]. Against such a danger, a safety net is needed to ensure that digital information objects with long-term cultural and intellectual value survive the expressions of stakeholder interest with their integrity intact.
[&gt; Society has always created objects and records describing its activities, and it has consciously preserved them in a permanent way... Cultural institutions are recognised custodians of this collective memory: archives, librar[ies] and museums play a vital role in organizing, preserving and providing access to the cultural, intellectual and historical resources of society. They have established formal preservation programs for traditional materials and they understand how to safeguard both the contextual circumstances and the authenticity and integrity of the objects and information placed in their care... It is now evident that the computer has changed forever the way information is created, managed, archives and accessed, and that digital information is now an integral part of our cultural and intellectual heritage. However the institutions that have traditionally been responsible for preserving information now face major technical, organisational, resource, and legal challenges in taking on the preservation of digital holdings (B. Smith, 2002, pp.133–134) [cited in Harvey p. 29].]
￼
￼A variety of new stakeholders are now required to collaborate more for a good preservation practice to take place. Lavoie and Dempsey (2004, cited in Harvey, 2012, p. 30) argues that cooperation “can enhance the productive capacity of a limited supply of digital preservation funds, by building shared resources, eliminating redundancies, and exploiting economies of scale”. Chapter 11 of the UNESCO guidelines (2003) give more pratical details on how this can take place, giving structural models and tips on what to share (information, standards, division of labour...). [CLOCKSS cooperation to preserve journals (see Harvey p32 + chapter 9); + Harvey p32 National Library of Netherlands &amp; Elsevier Science + most uni libraries access with publishers, even UoD with eg Dawsonera/ACM]
Legal issues
(see later) special case for emulation
In certain cases, the archiving process is not made in accordance with the original publisher. This is often the case for the automated archiving of the Internet (e.g. Wayback Machine), which automatically crawl pages, but as a website publisher, it is easily possible to opt out of these schemes [see http://archive.org/about/exclude.php [Accessed 17th Apr 2014]]. In the case of emulation, this is what happens with reverse engineering, that is, disassembling the original program to understand how it works and recreate a similar version which can operate from a different hardware or software architecture. This is a common process for proprietary software or hardware, whose source code is not available to the general public and no documentation is provided for developers. Often, this is developed by amateurs and hobbyists, but is illegal as it breaks the intellectual property of the original creator; Nintendo, for example, attacks reverse engineered emulators[http://www.nintendo.com/corp/legal.jsp [Accessed 17th Apr 2014]] and the sharing of ROMs [Read Only Memory; the bitstream of cartridges or discs containing the games]. There is however popular demand for playing these, and Nintendo has developed their own emulators to play legacy games on their new consoles (see the Virtual Console [http://en.wikipedia.org/wiki/Virtual_Console]), while still retaining their property rights (you need to buy the games again on an e-commerce platform). This however causes issue for the longer term preservation: there is no certainty that Nintendo will continue to provide this service on their future generations of consoles, or if they ever cease operations.
￼A similar issue happened with Apple Computer. In 2005, Apple changed the architecture (that is, the “language” of instructions needed for the processor) of their personal computers from PowerPC to Intel x86. This means that applications needed to be rewritten, sometimes from the ground up, to work natively on the newer Apple computers. To ease the transition, Apple created Rosetta, a software that dynamically translates PowerPC instructions to x86 instructions, so that buyers of the new Apple computers were still able to open legacy applications that were not yet rewritten for x86 — although this came with a significant performance hit. In 2009 however, support for PowerPC was entirely discontinued [...]
Libraries
Good curating and preservation starts early. Smith (2003, pp. 2–3, cited in Harvey, 2012, p. 32) suggests that “the critical dependency of preservation on good stewardship begins with the act of creation, and the creator has a decisive role in the longevity of the digital object”. Harvey (2012, p. 32) notes that “for most creators of information in any form this is a new role”; therefore publishers of digital content are not necessarily well prepared for preservation, and this is why collaboration with librarians and archivists can be helpful (Ayre and Muir, 2004).
Ayre and Muir (2004) note that if the publisher worries about preservation themselves then they generally already own the rights for preservation, but there are negotiations to be done if an external body helps with preservation.
National libraries
Ayre and Muir (2004) suggest that the complexity and resources needed for digital preservation may mean that individual libraries may not have the possibility to do the preservation themselves like they used to, but instead rely on national libraries and centralised preservation systems to reduce overhead. This system is used by a number of libraries; even here at the University of Dundee, most journals and a selection of books are available through centralised systems (Dawsonera, Elsevier Science) or publishers directly (ACM). A report by Hedstrom and Montgomery (1998, cited by Ayre and Muir, 2004) found that most research librarians in the UK thought that a legal deposit to national archives was the best option, but this was not a
￼clear consensus. The report also highlighted that these librarians thought that “other libraries and publishers would need to, and should, have some involvement in preservation”.
Because all preservation methods involve copying of the original object, it might be illegal to make such a copy without the necessary rights, even if it is for preservation purposes. Certain countries have laws to overcome this and allow national libraries to preserve all materials; Ayre and Muir (2004) list a few of them and their specificities, but the situation has since evolved, as a lot of legislations have enacted changes to specifically include digital materials.
Archivists
New stakeholders
Profits vs nonprofits
Harvey (2012) points out doubts in the real interests of for-profit companies to achieve long-term preservation;
Even given the prevailing market-driven political ethos, it is difficult to envisage a situation where market forces will be sufficient to ensure the preservation of this digital material. The opposite is more likely to apply: ‘in some cases market forces work against long-term preservation by locking customers into proprietary formats and systems’ (Workshop on Research Challenges in Digital Archiving and Long-term Preservation, 2003, pp.x-xi). (Harvey, 2012, p. 32). [link with Google concerns]
How to preserve
Software methods Making hard copies
What appears to be the simplest solution is to make physical copies (i.e. print) of the digital documents. Although this certainly solves some
￼
￼problems in regards to technological obsolescence, Rothenberg (1999) and Granger (2000) pointed out that this cannot be done for more interactive material besides text and images. There is also, obviously, an issue with the amount of content that can be stored in this way — this would not work for archiving websites. Granger (2000) recognises that “if one’s sole concern is with the intellectual content of a document and the document is of a fairly simple nature [...] this at least provides some form of security”.
Standardisation
Standardisation is the transformation of a document’s content to a format that is standardised, open and well documented, ensuring that we will be able to open it with a variety of applications on a variety of platforms and that we can rebuild a program for visualising that document — as opposed to vendor-specific, closed source program that could be discontinued at any point in the future. Rothenberg (1999, p. 10) claims that this is a bad solution because “standardisation sows the seeds of its own destruction by encouraging vendors to implement non-standard features in order to secure market share” (therefore these non-standard features will be lost if opened in a different application). Granger (2000) argues that the blame is not to be put on the method, but on vendors. Standardisation is a type of migration and therefore share some of it cons (notably, this absence of vendor-specific features could imply a loss in the presentation and other characteristics). Standards eventually become obsolete so it is also paramount to preserve the specifications of the format, or to define formats that are unambiguous, such as CSV or XML.
A slightly different approach to this method is rebuilding viewers. In this scenario, the bitstream is not migrated to a standardised format, but kept as is; we simply re-build a viewer that is able to read that format. This is not as straightforward as it sounds, because most of the time, formats for proprietary software are not documented and decrypting them would require reverse engineering. If they are documented, then it is likely that this format is already a standard. There exists cases where reverse engineering is possible, or where a non-standard format is documented; for example, Microsoft Word files are not a standard, but there is plenty of documentation available online to produce code able to read them. An experiment by the VERS project (Thibodeau 2002, p. 22) has shown that it is possible to reengineer a viewer for the PDF file format from the specification available; as long as we also preserve these specifications, we
￼will be able to read PDF files in the future.
UVC
Lorie (2000, in Thibodeau, 2002, p. 22) has also proposed that the use of a portable programming language, such as Java [footnote: Most programming languages include instructions that are specific to a given platform, therefore the same file will need to be adapted to run on another platform. “Portable” languages like Java are called high-level and are more abstract. They are run inside a virtual machine, which worries about transforming Java code into instructions that are specific to that platform; as such, the same file of code can be ran on any platform.]. Lorie developed a software called Universal Virtual Computer (UVC), able to decrypt a variety of file formats, which can run on any platform supporting Java; because Java could be implemented on virtually any architecture, this means that a reader could be run on any platform without any effort. However, Lorie points out, this approach is limited in terms of functionality, and comes at the price of performance (because portable programming languages are platform-agnostic, this means the code is not optimised for any of the platforms). I am also wondering if this would really work on the long term, as we cannot be sure that future implementations of Java will be backward compatible with code written for a previous version.
Migration
Migration is the transformation of a document’s content to a format that is not necessarily standardised, but at least current; or, the newer version of same format. This could be, for example, transferring documents from the WordPerfect 6 format (used in the 80s and late 90s) to a Word 2011 format, or a Word 98 file to Word 2011. This should be done regularly, as after a certain period of time new programs will stop having backward compatibility to import these old formats (Thibodeau, 2002). This method has several downsides, mainly being that it is labour intensive and expensive (Thibodeau, 2002; Rothenberg, 1999); product lines, and therefore the migration path, could be terminated at any point (Thibodeau, 2002); the presentation and other characteristics could be changed (although this can be overcome by migrating to more flexible formats like PDF or LaTeX). Rothenberg (1999) adds that this method is also prone to error, risky (as it could compromise the integrity of the data), not scalable and it requires new solutions for each new format. Granger (2000) argues however that
￼comparatively, other methods (particularly emulation) are potentially much more expensive and labour intensive, and that for most preservation programmes, migration remains the only viable method.
Computer museums
Another approach to preservation, proposed by Swade (1993), would be to preserve a variety of hardware, software and devices to access obsolete media in “computer museums”. This is generally not considered a good solution; Granger (2000) notes that it would be unreasonably expensive to keep old machines running for an extended period of time, that computers chips will eventually decay anyway (and repair or replacement will be impossible), and that storage media not stand the test of time. There are still two possible use cases, identified by Rothenberg (1999): ensuring that emulators work as expected (by comparing them to original platforms), and helping with the recovery of lost data on obsolete media
Emulation
Emulation is the dynamic transformation of instruction sets to allow software and/or operating system designed for an obsolete architecture or platform to run on current systems. It is a highly complex method that offers certain benefits; notably, this is the most conservative method, as it is not destructive of the content or presentation. It allows us to run the original program used to create a digital object without altering that bitstream ; it “keeps the look and feel as well as the interactivity” (Granger, 2000) and as such, it is particularly interesting for objects rich in interaction, such as video games, data visualisations, or formats that are particularly complex (for example, medical imagery, 3D modelisations), for which methods like migration would be hard. It also ensures complete integrity and authenticity, which could be crucial for some data types.
There are numerous criticisms against emulation. A number of legal and intellectual property problems are raised, if the emulation strategy is not supported by the original author [see Nintendo &amp; Apple]. The complexity involved in creating emulators might be not worth it and requires a coherent organisation to reduce overhead (Granger 2000). Additionally, Thibodeau (2002, p. 21) argues that keeping the original functionality is not necessarily a positive thing when it comes to the delivery of the preserved content; “most users in the future will never have encountered—not to
￼mention learned how to use—most of the products they will need to access the preserved information objects”, and “it would cut users off from the possibility of using more advanced technologies for discovery, delivery, and analysis”.
“Emulators themselves become obsolete; therefore, it becomes necessary either to replace the old em- ulator with a new one or to create a new emulator that allows the old emulator to work on new platforms. In fact, if you get into an emula- tion strategy, you have bought into a migration strategy. Either strategy adds complexity over time.” (thibodeau 2002 p.20)
[NB: due to the length limit of this literature review, I have only explained the most popular archiving techniques. See Thibodeau (2002) and Harvey (2012) for a more comprehensive list of methods, including configurable chips, persistent archives, object interchange format, etc.]
Choosing one
Thibodeau (2002, pp. 15–16) suggests four criteria to choose a preservation method: feasibility, sustainability, practicality and appropriateness.
Feasability means that the hardware and the software for implementing a given method must be existing and developed. Sustainability is ensuring that this method can be “applied indefinitely into the future, or that there are credible grounds for asserting that another path will offer a logical sequel to the method, should it cease being sustainable”. This means that the method must be resistant to technological obsolescence, both for storing the data (medium will change) and accessing it (means to access the media ane decode the bitstream will change).
Practicality is establishing that this method will be reasonably difficult and affordable. [This can generally change depending on the resources of the preserving organisation, the available technology at that time and the valuation of the digital objects.]
Finally, appropriateness is using a method that is relevant to the type of material being preserved, and the objectives of the preservation. For example, del Pozo et al. (2012, p. 7) suggests that if the integrity and
￼
￼authenticity of the documents are not paramount, then a migration that keeps the contents but discards the presentation might be the least expensive and simplest option. Inversely, when trying to preserve highly interactive material such as video games or when the specifications for a format are unknown, then emulation is a more pertinent option, because migration would be extremely difficult and inaccurate (Granger, 2000).
Authenticity/integrity and issues with each method
Different preservation methods yield different results in what is called authenticity, that is how faithful the rendition [its presentation] of the preserved object will be. Authenticity is subjective and therefore there is debate around this issue (Del Pozo et al., 2012, p. 7; Cook, 2000); however most agree that while as little difference in the presentation is desirable, it is rarely (if ever) possible to keep a truthful rendition. As an example, del Pozo et al. (2012, p. 8) use a spreadsheet within the migration method:
“For instance, one institution may decide to normalise a spreadsheet into a PDF, which would retain the cell values but destroy any formula used to calculate them. This would favour a particular presentation of that digital object over the information form of the original. On the other hand, another institution might decide to normalise spreadsheets into ODF, which may lose some formatting but retain the formula used to derive each cell. In this case, being able to retain the information form of the file would be seen as more important.”
Thibodeau (2002, p. 28) however points out that “people will want to use the best available technology—or at least technologies they know how to use— for discovery, retrieval, processing, and delivery of preserved information. There is a danger that to the degree that preservation solutions keep things unaltered they will create barriers to satisfying this basic user requirement.” An overly faithful preservation would therefore not be desirable. Again, this is subject to a high number of parameters.
Another important issue is integrity, i.e. “ensuring the object is complete and has not been altered or corrupted” (Harvey, 2012, p. 54; Ross [CITED], 2002, p. 7); that is, the object “is what it purports to be”, and no parts are missing from it. This is particularly important, Harvey (2012, p. 54) notes, for business archiving, where the legal value of a digital object could be lost if we cannot demonstrate that this object hasn’t been tampered with, or that a
￼
￼migration processes have not incurred data losses. However, as Lynch (2000) has argued, the integrity of objects is only a matter of trust. The guidelines of UNESCO (2003, p. 22) recommend that the best protection for authenticity is proper documentation of the archival process; Harvey (2012, p. 54) expands on this idea: “for instance, if it can be established that a digital object has always been kept in the custody of an archive in which every change to it has been recorded (such as changes resulting from migration of the bit-stream – the dates at which migration occurred, to which media, and so on), then we can be more secure about its integrity.”
Emulation / legal rights with videogames Use of standards
Some standards such as OAIS (Open Archival Information System), originally developed for space science, could be used. Sadly [no implementation][source??]
Personal archiving
On a more personal level, Harvey (2012) and Lukesh (1999) note that personal correspondance might be lost if we do not take steps to preserve them:
“The widespread shift from writing letters to the use of email has diminished the likelihood that personal correspondence will remain accessible to future historians. Lukesh asked in 1999, ‘Where will our understandings of today and, more critically, the next century be, if this rich source of information is no longer available?’, as scientists, scholars, historians and almost everyone increasingly use email” (Lukesh, 1999) (in Harvey, 2012 p. 32)
While this is a correct idea, I think that, just like the archiving of Usenet, few people kept their letters in the sole purpose of leaving researchers decades from now understand our epoch’s social habits and ways of speaking. There has to be other motivations for preserving our conversations and our objects.
￼
￼Richard Banks, a researcher at Microsoft Cambridge, suggests that we first preserve artifacts for ourselves. The majority of the time, we simply keep what we consider significant to remind ourselves of particular people, events and places, and to share them with people we care about; we discard what is not meaningful to us by pragmatism (lack of physical space, lack of importance, redundance) (Banks, 2011, p. 6). This selection process contributes to keeping what we want to remember, and what is the legacy we will want to leave to others after our deaths. However, the digital age changes this situation. First, the space to store our objects on computers and online is becoming virtually unlimited; second, storing photos and data on online services mean we may forget about them in a few years (Banks, 2011).
What happens after our death
### [Digital heritage]
Shifts in usages (more photos)
Advantages (sharing, organisation etc.); record part of our lives we never did before
Digital artifacts have certain fundamental advantages over physical objects: the main two being the ease of organising a collection of data through indexing, cataloguing and adding metadata (Kirk and Sellen, 2010, p. 35), and the ability to duplicate and share that object accurately and virtually cost- free (Banks, 2011). This actually provides numerous advantages in terms of preservation.
[[+ tie with “In the past ancestors have signified something is important simply by keeping it” part ]]
The first one, ease of organisation, can be significantly helpful in tackling the abundance of digital data. Banks (2011) suggests that the technologies lying behind the ranking in the result of Web search engines could, eventually, help us in understanding and finding what is our most important data. An attempt has been made by the MUSE (Memories USing
￼Email) project (Hangal et al., 2011), which aims to find patterns in our emails archives to help us make sense of this large data set. As the authors note in the introduction:
“Email has become a de facto medium of record; many people consciously deposit important information into email, knowing they can look it up later, and thereby use their email account as an informal backup device. Therefore, email archives contain or reflect memories that are extremely valuable for the purposes of reminiscence.” (Hangal et al., 2011)
Early users reported that they have been using it to: make a summary of their work progress over the year; extract and organise certain type of data (for instance, the personal out of the professional, or the important out of the more mundane); finding life milestones inside family emails; picking up work that’s been left unfinished and forgotten; renewing with old relationships; and serendipitous discovery. The authors note in the conclusion that the different uses of their program went beyond their original expectations of simply reminiscing: “the stories above include an example of each one of the ‘5R’s’ described by Sellen and Whittaker [2010]: recollection, reminiscing, retrieving, reflecting, and remembering intentions. Further, it suggests that browsing and remembering the past can affect the future.”
The second advantage, the ability to accurately and easily make copies and backups, is fairly obviously an advantage for personal preservation. However, Banks (2011) notes, the process of backing up is troublesome and storage media do not make our data immune. Additionally, the use of online services to store our data creates an abstraction between the storage material and us; about Flickr: “I have no idea what kind of hardware my files are now stored or even where they are geographically. I just expect to have access to them as long as I pay my bills” (Banks, 2011, p. 27).
Keeping memories attached to digital objects
Banks (2011) also suggests that we could blend in the physical and the digital to allow recording of things we never had the opportunity to before. He gives as example the Weather Camera, an experimental object by Kjen Wilkens (2011) that records wind and temperature along with commentary,
￼
￼instead of an image, as an alternative to recall a place; or 3D backups by Héctor Serrano (2009), another experimental project that allows us to make backups of sentimentally important objects through 3D scanning and 3D printing. Similarly, Banks notes tools by Microsoft Research to recreate places in a 3D environment based on photos, allowing to recreate a sentimentally important place at any given point in time provided enough photos are available. A similar technique is used in bigger preservation projects, as described by Zhou et al. (2012), to make high quality 3D models of culturally important places and objects in China.
Kirk and Sellen (2010) found however that ultimately, stories and narrative are more important than the preservation of the artifact itself. They propose that “technology might play a role in capturing and associating stories or narratives with different physical objects”, imagining that we could augment physical objects with RFID tags (and I would add, virtual reality technologies) to pass down stories attached to these objects.
Conclusion
</Text>
        </Document>
        <Document ID="139">
            <Title>Lecture slides/transcript</Title>
            <Text>Transcript and slides available at http://socialdigital.dundee.ac.uk/~vloux/blog/archives/411
</Text>
        </Document>
        <Document ID="27">
            <Title>dissertation</Title>
        </Document>
        <Document ID="32">
            <Title>Draft</Title>
            <Text>Harvey, D. R. 2012. *Preserving digital materials*. Berlin: De Gruyter Saur.

Preserving Digital Materials by Douglas Ross Harvey is a great introduction to the field of digital preservation. Although it does not seek to get into details, it references a lot of other publications that I have used throughout this literature review, and is considered a great reference by archivists, librarians and other recordkeepers. The first chapter is concerned with the definition of digital preservation itself, which is central to the theme of my dissertation. Harvey argues that the most important challenge we are faced now with digital preservation is the changement of paradigm; he says the volume of information, the nature of the data that is deemed valuable for preservation, the means of accessing the data, the abundance of metadata, and the physical supports to hold digital data mean we must completely redefine our paradigms for the preservation of digital data, as traditional approaches from paper-based archiving become irrelevant.

&gt; “In library and recordkeeping practice we are moving rapidly from collection-based models, whose principles and practices have been developed over many centuries, to models where collections are not of paramount importance and where what matters is the extent of access provided to information resources”

## Previous paradigm

(copied p. 10). Called pre-digital preservation paradigm

- When materials are treated, the treatments should, when possible, be re- versible
– Whenever possible or appropriate, the originals should be preserved; only materials that are untreatable should be reformatted
– Library materials should be preserved for as long as possible
– Efforts should be put into preventive conservation, and aimed at providing
appropriate storage and handling of artifacts
– Benign neglect may be the best treatment (derived from Cloonan (1993,
p.596), Harvey (1993, pp.14,140), and Bastian, Cloonan and Harvey (2011, pp.612-613)).

more defs pp 10-11
+ Bastian, Cloonan and Harvey, 2011, p.611

&gt; Change is also apparent in the ways that information professionals define preservation. In a 2002 survey of records managers and archivists who deal with electronic records, Cloonan and Sanett (2002, p.73) posed the questions: ‘What is the meaning of preservation? Does the meaning change when it is applied to electronic rather than paper-based records?’. They noted that:
&gt;&gt; It is clear that professionals are revising their definitions of preservation from a once- and-forever approach for paper-based materials to an all-the-time approach for digital materials. Preservation must now accommodate both media and access systems ... while we once tended to think about preserving materials for a particular period of time – for example, permanent/durable paper was expected to last for five hundred years – we now think about retaining digital media for a period of continuing value (Cloonan and Sanett, 2002, p.93).

&gt; Interviewees expressed dissatisfaction with the term digital preservation, sug- gesting that other terms such as long-term retention are more suitable (Cloonan and Sanett, 2002, p.74). Definitions of preservation provided by survey re- spondents, Cloonan and Sanett (2002, p.85) concluded, ‘demonstrate a shift taking place from defining preservation as a once-and-forever approach for paper-based materials, to an all-the-time approach for digital materials’. This shift implies an acceptance that preservation may even begin before a record has been created.
&gt; Debate has continued about whether digital preservation sufficiently describes what needs to occur for digital materials to be accessible over time, on the grounds that more than just aspects of preservation need to be encompassed by whatever term is used. The terms digital curation and digital stewardship have been proposed and are gaining acceptance because they describe more than just preservation, referring also to ‘the creation, collection, organization [and] dissemination’ of digital objects (Bastian, Cloonan and Harvey 2011, p.609).

# Change of the nature of the data stored

Not only books or journals but also data sets:
&gt; Librarians manage hybrid libraries, consisting of both physical collections and distributed digital information resources, and digital libraries. Other stakeholders with a keen inter- est in digital preservation manage digital information in specific subject areas, such as geospatial data or social science data. In the past this material, where it existed, was maintained as collections of paper and other physical objects. The practices developed and applied in libraries and archives are still largely based on managing physical collections and cannot be applied automatically to managing digital collections.

Use of datasets: visualisation, aggregating, calculation

# Change of structure in physical media.
Benign neglect cannot be applied anymore:
&gt; The phrase benign neglect provides an example of a concept that is helpful in the pre-digital preservation paradigm but is harmful in the new. It refers to the concept that many informa- tion carriers made of organic materials (most notably paper-based artifacts) will not deteriorate rapidly if they are left undisturbed. For digital materials this concept is positively harmful.

&gt; The old paradigm does not, however, engender an understanding of the complexity of copying – which is more than simply preserving a bit-stream, but must take account of a wide range of other attributes of the digital object that also need to be preserved. (p. 12)

pp. 17-18

&gt; Other concepts need to be accommodated by the new definitions. For digital materials ‘their preservation must be an integral element of the initial design of systems and projects’ (Ross, 2000, p.13), but this is not usually the case. Digital materials exist in a bewilderingly large number of formats; there is still little standardization. The most significant concept is that the preservation of digital materials is much more than the preservation of information content or physical carrier:
&gt;&gt; it is about preserving the intellectual integrity of information objects, including capturing information about the various contexts within which information is created, organized, and used; organic relationships with other information objects; and characteristics that provide meaning and evidential value (Gilliland-Swetland, 2000, p.29).

&gt; Preserving the original bit-stream is only one part of the problem; equally im- portant is the requirement to preserve ‘the means of interpreting, reading and utilizing the bit stream’ (Deegan and Tanner, 2002).
-&gt; Deegan, M. and Tanner, S. (2002) The digital dark ages. Update, May
-&gt; Deegan, M. and Tanner, S. (eds) (2006) Digital preservation. London: Facet

# Changes in the availability

‘technology tends to erase distinctions between the separate processes of creation, reproduction and distribution that characterize the classic industrial model of print commodities’ (Nurnberg, 1995, p.21). -&gt; [Nurnberg, G. (1995) The places of books in the age of electronic reproduction. In *Future libraries*, ed. R.H. Bloch and C.A. Hesse, pp.13-37. Berkeley: University of California Press] &gt;&gt;&gt; 
“This has significant implications for preservation, especially in terms of who takes responsibility for it and at what stage preservation actions are first applied. For instance, in the industrial-mode print world, acquiring the artifact – the book – so that it could be preserved occurred by means such as legal deposit legislation, requiring publishers to provide copies to libraries for preservation and other purposes. If the creator is now also the publisher and distributor, as is often the case in the digital world, who has the responsibility of acquiring the information?
[developed later]”

# Why preserve digital material 

&gt; Preservation is based on the notion that, because man learns from the past,  ‘evidence of the past therefore has considerable signifcance to the human race and is worth saving’ (Harvey, 1993, p.6).



—


——

"We don't know where this Internet is going, and once we get there it will be very instructive to look back."

Donald Heath, president of the Internet Society in Reston, Virginia (quoted in https://archive.org/about/)</Text>
        </Document>
        <Document ID="100">
            <Title>Migration</Title>
            <Synopsis>Migration is the transformation of a document’s content to a format that is not necessarily standardised, but at least current; or, the newer version of same format. This could be, for example, transferring documents from the WordPerfect 6 format (used in the 80s and late 90s) to a Word 2011 format, or a Word 98 file to Word 2011</Synopsis>
            <Text>Migration is the transformation of a document’s content to a format that is not necessarily standardised, but at least current; or, the newer version of same format. This could be, for example, transferring documents from the WordPerfect 6 format (used in the 80s and late 90s) to a Word 2011 format, or a Word 98 file to Word 2011. This should be done regularly, as after a certain period of time new programs will stop having backward compatibility to import these old formats (Thibodeau, 2002). This method has several downsides, mainly being that it is labour intensive and expensive (Thibodeau, 2002; Rothenberg, 1999); product lines, and therefore the migration path, could be terminated at any point (Thibodeau, 2002); the presentation and other characteristics could be changed (although this can be overcome by migrating to more flexible formats like PDF or LaTeX). Rothenberg (1999) adds that this method is also prone to error, risky (as it could compromise the integrity of the data), not scalable and it requires new solutions for each new format. Granger (2000) argues however that comparatively, other methods (particularly emulation) are potentially much more expensive and labour intensive, and that for most preservation programmes, migration remains the only viable method.
</Text>
        </Document>
        <Document ID="144">
            <Title>Table of Contents</Title>
        </Document>
        <Document ID="33">
            <Title>Draft2</Title>
            <Text>
# Introduction

As we shift into the Information Age and a knowledge-based economy, a number of concerns are being raised over the long-term preservation of digital data and artifacts. This literature review aims to get an overview of the current problems, techniques and practices in preserving data on digital media, including the definition and understanding of the new paradigms created by digital preservation, the importance of being able to store things on the long term, the processes used to curate and select content to be preserved, and the various methods used to do so. It also encompasses legal and ethical issues arising from it,  and how the paradigms and methods change when applied to different types of archiving (e.g. for researchers, for businesses, and for personal businesses).

# Why digital preservation
[Stats of numbers on digital vs physical]

## Why digital degrades faster
### 	UNESCO’s Threats to digital continuity
UNESCO (2003, pp. 30-31) has identified a list of threats to what they call *digital continuity*. These are:
+ “The carriers used to store these digital materials are usually unstable and deteriorate within a few years or decades at most”. Where, in the traditional preservation paradigms for paper-based artifacts, the main rule is that “benign neglect may be the best treatment” (derived from Cloonan (1993, p.596), Harvey (1993, pp.14,140), and Bastian, Cloonan and Harvey (2011, pp.612-613)),  this cannot be applied to digital storage media because their lifetime is much shorter. This varies highly with the type of medium used and with the storage conditions; for example, samples by the Digital Preservation Coalition (2008, p. 154) show that D3 cartridge tapes deteriorates after 50 years in optimal conditions (25% RH at 10º) but only one year in high humidity and temperatures (50%RH at 28º). Similar results happen for the more commercially available optical media (down to three months and up to 200 years), which are more sensitive to moisture (Harvey, 2012, p.47).

+ “Use of digital materials depends on means of access that work in particular ways: often complex combinations of tools including hardware and software, which typically become obsolete within a few years and are replaced with new tools that work diﬀerently” (UNESCO, 2003). Even if storage media could survive decades or even centuries, technological change means we will not necessarily have the means to access the data later. In other words, “to retain appropriate access to a digital object, we must maintain a mechanism to derive meaning from that object” (del Pozo et al., 2012, p6): for optical media, this includes not only the drives to read the disc, but also a cable to connect the drive to the computer’s motherboard, a motherboard that has a connector for this type of cable, and an operating system that has drivers for this connectivity (Pearson 2009, slide 11). Once we have access to the bitstream from the computer we need to be able to represent it, which means we also need to keep the software in a version that can open the preserved file.
There is a myth, kept by both customers and vendors, that media is long-lived: “[An IT vendor] offered us a special polymer that they guaranteed would preserve a CD-ROM for 100 years. They were unable to answer how we would preserve a CD-ROM player for that length of time.” (Weathley, 2003, cited by Harvey, 2012, p. 37)

+ Poor preservation documentation and metadata means it could be hard for potential users to find the data (UNESCO 2003)
+ + poor preservation (“Critical aspects of functionality, such as formatting of documents or the rules by which databases operate, may not be recognised and may be discarded or damaged in preservation processing.”, UNESCO 2003) + “So much contextual information may be lost that the materials themselves are un- intelligible or not trusted even when they can be accessed” -&gt; poor integrity

+ “Materials may be lost in the event of disasters such as fire, flood, equipment failure, or virus or direct attack that disables stored data and operating systems”
+ “Access barriers such as password protection, encryption, security devices, or hard-coded access paths may prevent ongoing access beyond the very limited circumstances for which they were designed”. [This argument is particularly true for web archiving, where crawlers of national libraries retrieve all sort of files to preserve them but do not necessarily have the permission or know the rights owner] [back that up maybe yo]

## Importance of data

### Library material: for society

&gt; “if we librarians do not rise to the occasion, successive generations will know less and have access to less for the first time in human history. This is not a challenge from which we can shrink or a mission in which we can fail” (Gorman, 1997, cited in Harvey, 2012, p. 27)

### For businesses (records)
### Personal archiving / domestic archiving
### Preserving interactive material inc. video games

In addition to the preservation of “static” documents and images, there is a significant desire to preserve video games, which are getting recognised as a form of art similarly to movies and music. The Library of Congress (2007) gave awards for the preservation of video games; there is a growing interest in being able to play “retro” games, but the consoles are obsolete. A large community of hobbyists reverse engineered consoles and produced emulators of these platforms for PC and other platforms different from their intended one. Because no documentation is available, this is a Herculean task; byuu (2011), the developer of an emulator for Super Nintendo games, recalls this for reengineering certain cartridges:

&gt; [Low-level emulation] is also a very expensive operation, monetarily speaking: to obtain the DSP program code requires melting the integrated circuit with nitric acid, scanning in the surface of a chip with an electron microscope, and then either staining and manually reading out or physically altering and monitoring the traces to extract the program and data ROMs. This kind of work can cost up to millions of dollars to have done professionally, depending upon the chip's complexity, due to the extremely specialized knowledge and equipment involved.  (byuu, 2011)

Obviously this is not necessarily a route many institutions would like to take for preserving digital objects. Because this also causes intellectual property issues, Nintendo and Sony have launched their own preservation programs, allowing players to play “classic” games on the newer generations of consoles; this is a far more accurate emulation (as they do have the internal documentation to create these emulators) and respects their monetary interests (players have to buy the games, whereas in hobbyist emulation, you would have to download the game file).

### Useful for researchers in the future

Beyond the governmental and cultural needs for preservation, there is also great need for preserving scientific data. “Data are the foundation on which scientific, engineering, and medical knowledge is built” (Committee on Ensuring the Utility and Integrity of Research Data in a Digital Age, 2009, p. ix, cited in Harvey, 2012, p. 26); therefore it is crucial in a digital era, with enormous data sets and research papers becoming widely public, to ensure this doesn’t disappear.

#### Understanding our history

There is a need to define what is the purpose of archives. As a tool to learn from the past, or to keep records, but as Cook (2000) eloquently puts it:
&gt; “Archives […] are a source of memory about the past, about history, heritage, and culture, about personal roots and family connections, about who we are as human beings and about glimpses into our common humanity through recorded information in all media, much more than they are about narrow accountabilities or administrative continuity.” (Cook, 2000)

Preservation should, therefore, be considered very important. There is some skepticism to this:

&gt; “It was suggested in 2002 that the last 25 years have been a ‘scenario of data loss and poor records that has dogged our progress’ and that, if this is not reversed, ‘the human record of the early 21st century may be unreadable’ (Deegan and Tanner, 2002).
&gt; Rhetoric of this kind is common in the literature, but is regrettably poorly supported with specifics and evidence. Alarmist descriptions abound: there will be ‘a digital black hole… truly a digital dark age from which information may never reappear’ (Deegan and Taylor, 2002) if we do not address the problem, and we will become an amnesiac society. 
&gt;[…]
&gt; It is only possible to conclude, as the authors of the Digital Preservation Coalition’s handbook (2008, p.32) did for the UK, that the evidence of data loss is ‘as yet only largely anecdotal ... [but] it is certain that many potentially valuable digital materials have already been lost’.” (Harvey, 2012, pp. 33-34)

While it’s difficult to argue that preserving digital objects is necessary, there is certainly a need to define what deserves to be protected, and to what level.

#### Understanding the beginnings of the Internet, as we have shifted in an information society
##### Preserving an accurate + [integrity] image of our society at a given point

###### Twitter &amp; Usenet

Camille Paloque-Bergès (2011; 2013) proposes that we can already find historical and sociological evidence of a given period in Internet time when browsing archives of Usenet [footnote: An Internet discussion system established in the 1980s and that is widely regarded as the predecessor of the Web and online fora]. She analysed how the way of speaking has evolved [footnote: See also http://wwwtxt.org (or @wwwtxt on Twitter), a project collecting short phrases of the Usenet to demonstrate this] and how an self-organised community had built itself, noting that we can “feel [the community’s] informational generativity” and understand that one of the biggest qualities of the early users of the Internet was their “recursivity as a public” (Paloque-Bergès, 2013); that is, how they welcomed newcomers — notably through the use of FAQs [footnote: Frequently Asked Questions], which weren’t a common concept back then. There are also large elements of nostalgia coming back (Paloque-Bergès, 2011).

It is also interesting to come back on the archival of Usenet. The messages were not systematically archived, and obviously not available on the Web as it did not exist and is separate. Today’s collections have mainly been recreated from donations of people who did archived Usenet newsgroup, originally to DejaVu News in 1995, and Google Groups in 2001.
A number of privacy concerns were raised with the publication of these searchable archives; messages who were, 15 to 20 years earlier, thought to be confidential are now considered as digital heritage. There was also a large backlash against the archiving made by for-profit companies, and the credit taken later by Google for its donors, with other critiques like Brad Templeton’s (an Internet veteran and important figure; cited in Paloque-Berges, 2013)  remark that the archive was lacking in integrity. However, the searchability of these archives did bring welcomed technical possibilities for an Internet historiography, including a timeline where “the announcement of the Chernobyl disaster on Usenet stands alongside the first message of Linus Torvalds calling for help on what would become the Linux operating system” (Paloque-Bergès, 2013). 

Similar concerns appeared at the announcement of the archival of Twitter by the Library of Congress in 2010. The two are not really comparable, mostly because the archive is not public and is made by a not-for-profit body. On the mailing list of the Association of Internet Researchers, the idea was met with interest: “The public twitter stream is of historical cultural significance and is an amazing repository of mundane moments in the daily lives of many people and records of what they thought important” (Baym, 2010) [on http://listserv.aoir.org/pipermail/air-l-aoir.org/2010-April/021125.html]”. However, the privacy concerns (notably with public and private feeds) were discussed, and as Michael Zimmer remarked:
&gt; “This is the classic “but the information is already public” argument that, while technically true, presumes a false dichotomy that information is either strictly public or private, ignoring any contextual norms that might have guided the initial release of information or how a person expects that information to flow.” (Zimmer, M. [on http://listserv.aoir.org/pipermail/air-l-aoir.org/2010-April/021136.html]; see also his paper (2010) on Facebook privacy, going further in that argument) 

Paloque-Bergès (2010) theorises this argument, suggesting that “information is not an object, but a statement; it goes through socio-technical processes; it doesn’t have an objective nature”. A message in a tweet can have a very different context when taken apart of the moment it was posted. It also emphasises that privacy could change (the user could, later, switch their account to Private, delete the tweet or their account altogether, but this wouldn’t be reflected in the archive. We cannot know what is the user’s original intent when they posted their message, and, as Paloque-Bergès (2010) noted, we aren’t sure the user themselves know.
&gt; “More often than not, in a technical ecosystem in which making content private is more difficult than sharing broadly, teens choose to share, even if doing so creates the impression that they have given up on privacy. It’s not that every teen is desperate for widespread attention; plenty simply see no reason to take the effort to minimize the visibility of their photos and conversations.” (Boyd, 2014).

## Advantages over physical preservation
### Mass preservation, easier access and organisation

### Recording digitally material that’s not born-digital
## Cost of restoration; case studies of lost data

A number of institutions have already experienced loss of data, or data being corrupted. This can have disastrous consequences for businesses, for example, as noted by McAteer (1996, cited by Ross and Gow, 1999, p. iii): “of the 350 companies unexpectedly relocated by the [1993] World Trade Centre (NYC) bombing 150 [43%] ceased trading,
many because they lost access to key business records held in electronic form”. 

Data can, obviously, be sometimes restored. Ross and Gow (1999, pp. 39-42, referenced in Harvey, 2012, p. 34) provide a number of case studies. Harvey (2012, p. 34) also gives the ironic example of the website for the “Functional Requirements for Evidence in Recordkeeping” project, directed by the University of Pittsburgh which was accidentally deleted; but thankfully was automatically saved by the Internet Archive.

There is still the possibility of restoring data by specialised carriers; however Pearson (2009, slide 15) notes that while this was generally effective, this did not work on some items. He also regrets that the restoration loses all metadata in the process; this can be an issue for sensitive or classified information (for business or governmental archives) as the carrier would be able to see it; finally, it is a prohibitely expensive operation, and so a carefully planned preservation could be much cheaper (and obviously more reliable) than neglect.
However, these services are always useful in certain cases. Harvey (2012, p. 41) cites the statistics by the data recovery giant Kroll Ontrack (2011): “hardware and system problems cause most customers to approach them (29 per cent), followed by human error (27 per cent). Software corruption or program problems, computer viruses and natural disasters are the other reasons listed.”

# Defining paradigms for digital preservation
### Pre-digital paradigms
### Inability to preserve the medium, only the bitstream
### Importance of metadata, relationship &amp; context
### Definition of terms		
		
# What to preserve / Selectivity
## Harvey 5 pillars for selectivity (Harvey 2012, p. 26)
* If **unique information** objects that are vulnerable and sensitive and therefore subject to risks can be preserved and protected
* If preservation ensures long-term accessibility for researchers and the public;
* If preservation fosters the accountability of governments and organisations
* If there is an economic or societal advantage in re-using information
* If there is a legal requirement to keep it (NSF-DELOS Working Group on Digital Archiving and Preservation, 2003, p.3).

### Banks reasons for domestic archiving
In *The Future of Looking Back*, Richard Banks (2011) analyses the reasons that lead us to keep or discard physical objects at home. He argues that while making a selection is a positive thing because it forces us to filter our belongings and focus on what is really meaningful, we also run the risk of ridding ourselves of things that we wish we would have kept in retrospect. He points out that the organisation of digital files make this trickier: because we we can keep pretty much everything on virtual space, it is easier to accidentally delete meaningful things alongside the less important. (Banks, 2011 p. 22).

In the context of personal archiving, Banks notes that in the past, the legacy we leave behind us was a highly selective one. Because our possessions are limited by physical space, our ancestors until now “have implicitly indicated that an item is significant through the simple act of keeping it. Although thousands of objects likely crossed their path over their lifetime, the constraints of money and space meant that only a few were retained, often the most significant.” (Banks, 2011, p. 78). He does however argue that this is no longer true in a digital world, because new possibilities mean we simply accumulate digital objects without necessarily ordering them by importance.

### Technical possibility of archiving

### Pros/cons of archiving the entire internet

There are various controversies around whether we should automate the archiving of the Internet, or if we should operate a manual selection. UNESCO (2003, p. 73) notes that those who value a more comprehensive collection (i.e. archiving “all” the Internet) argue that “any information may turn out to have long-term value, and that the costs of detailed selection are greater than the costs of collecting and storing everything”. This argument is particularly true because of the digital media: “unlike non-digital material such as paper-based artifacts, where there is a period of time in which to make selection decisions before deterioration of materials becomes an issue, the time frame for deciding whether or not to preserve digital materials is very short” (Harvey, 2012, p. 57). UNESCO (2003, p.71) backs that claim: “it may not be possible to wait for evidence of enduring value to emerge before making selection decisions”.

Other people [Cook?] think that it would be preferable to keep a curated collection because of a higher technical quality, and this also allow us to settle [negotiate?] preservation rights with the content producers. Harvey adds that we also do not have the technical capabilities to find all the pages on the Internet, and even if we had we would not have the technical possibility to store the whole of the Internet (Harvey 2012, p. 58); again, this is also something recognised by UNESCO: “‘there are usually more things – more information, more records, more publications, more data – than we have the means to keep” (2003, p.70).

The UNESCO (2003) and Cook (2000) both however recognise that a middle ground should be found and that both approaches are valid depending on the goal of the preservation. For instance, the Internet Archive[IA] is going through an automated process (archive as many pages as possible) but, because of technical limitation, only preserves the text and dismisses all other images, stylesheets and external media, which could be critical in some instances. It seems evident that for valued documents these should be retained. The British Library’s Web Archive[ukwa] [http://www.webarchive.org.uk/ukwa/] 

###	Dangers of selection (Cook)
However, Cook (2000) points out the dangers of manual selection for what is worth archiving: “With memory comes the inevitable privileging of certain records and records creators, and the marginalizing or silencing of others” (Cook, 2000, cited in Harvey p. 58). He asserts that the archivists must be impartial and objective, and ensure all points of view are kept to ensure our shared memory does not “becomes counterfeit, or at least transformed into forgery, manipulation, or imagination”. This is something that is clear with manual archiving, but could also be a problem, maybe unintentionally, with the automated discovery of content when archiving Web pages.
		
### Automatic selection (Reputation model)
In order to keep the most valuable content, algorithms could be used. Kawano (2008) proposes to use reputation models, that attempts to determine the relative value of a Web page by considering its “importance, fairness, trustiness, uniqueness and valuation”. This is done using a variety of ways; the algorithm suggested by Kawano relies on the consistency of a web page, that is the amount of time it existed, the number of links it contains and the number of external links to this page  (Kawano, 2008, p. 291).

#### Crawling
#### Manual curating
	
### Special Collections for events (eg before elections, see webarchive.org.uk for other ideas)
Another interesting selection process that has emerged is the one of *specific archiving* when certain events are happening. For instance, the UK Web Archive[ukwa] has, on top of their topical-based collections (Energy, Health and Social Care…), a few event-based collections (Queen Jubilee, Olympic and Paralympic Games, London 2005 terrorist attacks…) which reflect the state of the Web at that given moment in history. Masanès (2006, p.83) notes that this type of archiving must be manually curated and thoroughly preparated. He gives a methodology for this, using presidential elections as an example: a time frame for the collection must be defined (“3 months before the election + 1 month after”), what type of sites will be archived and when (“the political parties’ blogs and websites, every week; analysis, commentary and humourous/satire websites, every month; online newspaper articles, once”).

[Opposing to automated archiving]: Masanès (2006) also describes the technique used for crawling a set of websites automatically. For example, when attempting to save all sites located in France for the French National Library, the robots were instructed to start navigating from 12 existing directories of websites, and restrict what they save to sites with a .fr domain name, or, for more generic top-level domain names (.com, .net, .org…), check if the phone number used for DNS[footnote explanation] registration is a French number (starting with +33). (Masanès 2006, p. 83)

### Selection: by genre [web]
There are different genres of websites: institutional website, blogs, personal pages, forums. [[Genre have been studied in the context of the Web to see how they replicate or diverge from the genre in the print- ing world (see Crowston and Williams 1997) - in Masanès p88]]. An interesting point about this is that often, robots (crawlers, or later data analysis) can automatically identify their type [Rehm 2002 -&gt; check reference, Masanès:88] which permits the automatic generation of metadata, or simply to restrict a collection of web pages to certain Web genres.

# Web archiving: what stands as heritage on the web

# Who should preserve

Digital media changes the original preservation paradigm for deciding who takes the responsibility for archiving content, and how. The processes of creation, fabrication and publication used to be distinct in the “analog” era; but as Nurnberg (1995, p. 21, cited in Harvey, 2012, p. 9) said, “technology tends to erase distinctions between the separate processes of creation, reproduction and distribution that characterize the classic industrial model of print commodities”. Self-publication (particularly on websites) is much more common, and this is also true for businesses and personal data.
The Task Force on Archiving of Digital Information (1996, pp. 19-20) suggests that when, in general, “stakeholders disseminate, use, reuse, recreate and re-disseminate various kinds of digital information, they can easily, even inadvertently, destroy valuable information, corrupt the cultural record, [and ultimately thwart the pursuit of knowledge that is their common end]. Against such a danger, a safety net is needed to ensure that digital information objects with long-term cultural and intellectual value survive the expressions of stakeholder interest with their integrity intact.

[&gt; Society has always created objects and records describing its activities, and it has consciously preserved them in a permanent way… Cultural institutions are recognised custodians of this collective memory: archives, librar[ies] and museums play a vital role in organizing, preserving and providing access to the cultural, intellectual and historical resources of society. They have established formal preservation programs for traditional materials and they understand how to safeguard both the contextual circumstances and the authenticity and integrity of the objects and information placed in their care… It is now evident that the computer has changed forever the way information is created, managed, archives and accessed, and that digital information is now an integral part of our cultural and intellectual heritage. However the institutions that have traditionally been responsible for preserving information now face major technical, organisational, resource, and legal challenges in taking on the preservation of digital holdings (B. Smith, 2002, pp.133-134) [cited in Harvey p. 29].]

A variety of new stakeholders are now required to collaborate more for a good preservation practice to take place. Lavoie and Dempsey (2004, cited in Harvey, 2012, p. 30) argues that cooperation “can enhance the productive capacity of a limited supply of digital preservation funds, by building shared resources, eliminating redundancies, and exploiting economies of scale”. Chapter 11 of the UNESCO guidelines (2003) give more pratical details on how this can take place, giving structural models and tips on what to share (information, standards, division of labour…). [CLOCKSS cooperation to preserve journals (see Harvey p32 + chapter 9); + Harvey p32 National Library of Netherlands &amp; Elsevier Science + most uni libraries access with publishers, even UoD with eg Dawsonera/ACM]

## Legal issues
### (see later) special case for emulation

In certain cases, the archiving process is not made in accordance with the original publisher. This is often the case for the automated archiving of the Internet (e.g. Wayback Machine), which automatically crawl pages, but as a website publisher, it is easily possible to opt out of these schemes [see http://archive.org/about/exclude.php [Accessed 17th Apr 2014]].
In the case of emulation, this is what happens with *reverse engineering*, that is, disassembling the original program to understand how it works and recreate a similar version which can operate from a different hardware or software architecture. This is a common process for proprietary software or hardware, whose source code is not available to the general public and no documentation is provided for developers. Often, this is developed by amateurs and hobbyists, but is illegal as it breaks the intellectual property of the original creator; Nintendo, for example, attacks reverse engineered emulators[http://www.nintendo.com/corp/legal.jsp [Accessed 17th Apr 2014]] and the sharing of ROMs [Read Only Memory; the bitstream of cartridges or discs containing the games]. There is however popular demand for playing these, and Nintendo has developed their own emulators to play legacy games on their new consoles (see the Virtual Console [http://en.wikipedia.org/wiki/Virtual_Console]), while still retaining their property rights (you need to buy the games again on an e-commerce platform). This however causes issue for the longer term preservation: there is no certainty that Nintendo will continue to provide this service on their future generations of consoles, or if they ever cease operations.



A similar issue happened with Apple Computer. In 2005, Apple changed the architecture (that is, the “language” of instructions needed for the processor) of their personal computers from PowerPC to Intel x86. This means that applications needed to be rewritten, sometimes from the ground up, to work natively on the newer Apple computers. To ease the transition, Apple created **Rosetta**, a software that dynamically translates PowerPC instructions to x86 instructions, so that buyers of the new Apple computers were still able to open legacy applications that were not yet rewritten for x86 — although this came with a significant performance hit. In 2009 however, support for PowerPC was entirely discontinued […]

## Libraries

Good curating and preservation starts early. Smith (2003, pp. 2-3, cited in Harvey, 2012, p. 32) suggests that “the critical dependency of preservation on good stewardship begins with the act of creation, and the creator has a decisive role in the longevity of the digital object”. Harvey (2012, p. 32) notes that “for most creators of information in any form this is a new role”; therefore publishers of digital content are not necessarily well prepared for preservation, and this is why collaboration with librarians and archivists can be helpful (Ayre and Muir, 2004). 

Ayre and Muir (2004) note that if the publisher worries about preservation themselves then they generally already own the rights for preservation, but there are negotiations to be done if an external body helps with preservation. 

## National libraries

Ayre and Muir (2004) suggest that the complexity and resources needed for digital preservation may mean that individual libraries may not have the possibility to do the preservation themselves like they used to, but instead rely on national libraries and centralised preservation systems to reduce overhead. This system is used by a number of libraries; even here at the University of Dundee, most journals and a selection of books are available through centralised systems (Dawsonera, Elsevier Science) or publishers directly (ACM).
A report by Hedstrom and Montgomery (1998, cited by Ayre and Muir, 2004) found that most research librarians in the UK thought that a legal deposit to national archives was the best option, but this was not a clear consensus. The report also highlighted that these librarians thought that “other libraries and publishers would need to, and should, have some involvement in preservation”.

Because all preservation methods involve copying of the original object, it might be illegal to make such a copy without the necessary rights, even if it is for preservation purposes. Certain countries have laws to overcome this and allow national libraries to preserve all materials; Ayre and Muir (2004) list a few of them and their specificities, but the situation has since evolved, as a lot of legislations have enacted changes to specifically include digital materials.

## Archivists
## New stakeholders
## Profits vs nonprofits

Harvey (2012) points out doubts in the real interests of for-profit companies to achieve long-term preservation;
&gt; Even given the prevailing market-driven political ethos, it is difficult to envisage a situation where market forces will be sufficient to ensure the preservation of this digital material. The opposite is more likely to apply: ‘in some cases market forces work against long-term preservation by locking customers into proprietary formats and systems’ (Workshop on Research Challenges in Digital Archiving and Long-term Preservation, 2003, pp.x-xi). (Harvey, 2012, p. 32). [link with Google concerns]

# How to preserve
## Software methods
### Making hard copies
What appears to be the simplest solution is to make physical copies (i.e. print) of the digital documents. Although this certainly solves some problems in regards to technological obsolescence, Rothenberg (1999) and Granger (2000) pointed out that this cannot be done for more interactive material besides text and images. There is also, obviously, an issue with the amount of content that can be stored in this way — this would not work for archiving websites. Granger (2000) recognises that “if one’s sole concern is with the intellectual content of a document and the document is of a fairly simple nature […] this at least provides some form of security”.

### Standardisation

Standardisation is the transformation of a document’s content to a format that is standardised, open and well documented, ensuring that we will be able to open it with a variety of applications on a variety of platforms and that we can rebuild a program for visualising that document — as opposed to vendor-specific, closed source program that could be discontinued at any point in the future. Rothenberg (1999, p. 10) claims that this is a bad solution because “standardisation sows the seeds of its own destruction by encouraging vendors to implement non-standard features in order to secure market share” (therefore these non-standard features will be lost if opened in a different application). Granger (2000) argues that the blame is not to be put on the method, but on vendors.
Standardisation is a type of migration and therefore share some of it cons (notably, this absence of vendor-specific features could imply a loss in the presentation and other characteristics). Standards eventually become obsolete so it is also paramount to preserve the specifications of the format, or to define formats that are unambiguous, such as CSV or XML.

A slightly different approach to this method is rebuilding viewers. In this scenario, the bitstream is not migrated to a standardised format, but kept as is; we simply re-build a viewer that is able to read that format. This is not as straightforward as it sounds, because most of the time, formats for proprietary software are not documented and decrypting them would require reverse engineering. If they are documented, then it is likely that this format is already a standard. There exists cases where reverse engineering is possible, or where a non-standard format is documented; for example, Microsoft Word files are not a standard, but there is plenty of documentation available online to produce code able to read them. An experiment by the VERS project (Thibodeau 2002, p. 22) has shown that it is possible to reengineer a viewer for the PDF file format from the specification available; as long as we also preserve these specifications, we will be able to read PDF files in the future.

#### UVC

Lorie (2000, in Thibodeau, 2002, p. 22) has also proposed that the use of a portable programming language, such as Java [footnote: Most programming languages include instructions that are specific to a given platform, therefore the same file will need to be adapted to run on another platform. “Portable” languages like Java are called high-level and are more abstract. They are run inside a virtual machine, which worries about transforming Java code into instructions that are specific to that platform; as such, the same file of code can be ran on any platform.]. Lorie developed a software called Universal Virtual Computer (UVC), able to decrypt a variety of file formats, which can run on any platform supporting Java; because Java could be implemented on virtually any architecture, this means that a reader could be run on any platform without any effort. However, Lorie points out, this approach is limited in terms of functionality, and comes at the price of performance (because portable programming languages are platform-agnostic, this means the code is not optimised for any of the platforms). I am also wondering if this would really work on the long term, as we cannot be sure that future implementations of Java will be backward compatible with code written for a previous version.

### Migration

Migration is the transformation of a document’s content to a format that is not necessarily standardised, but at least current; or, the newer version of same format. This could be, for example, transferring documents from the WordPerfect 6 format (used in the 80s and late 90s) to a Word 2011 format, or a Word 98 file to Word 2011. This should be done regularly, as after a certain period of time new programs will stop having backward compatibility to import these old formats (Thibodeau, 2002). This method has several downsides, mainly being that it is labour intensive and expensive (Thibodeau, 2002; Rothenberg, 1999); product lines, and therefore the migration path, could be terminated at any point (Thibodeau, 2002); the presentation and other characteristics could be changed (although this can be overcome by migrating to more flexible formats like PDF or LaTeX).
Rothenberg (1999) adds that this method is also prone to error, risky (as it could compromise the integrity of the data), not scalable and it requires new solutions for each new format. Granger (2000) argues however that comparatively, other methods (particularly emulation) are potentially much more expensive and labour intensive, and that for most preservation programmes, migration remains the only viable method.

### Computer museums

Another approach to preservation, proposed by Swade (1993), would be to preserve a variety of hardware, software and devices to access obsolete media in “computer museums”. This is generally not considered a good solution; Granger (2000) notes that it would be unreasonably expensive to keep old machines running for an extended period of time, that computers chips will eventually decay anyway (and repair or replacement will be impossible), and that storage media not stand the test of time. There are still two possible use cases, identified by Rothenberg (1999): ensuring that emulators work as expected (by comparing them to original platforms), and helping with the recovery of lost data on obsolete media

### Emulation

Emulation is the dynamic transformation of instruction sets to allow software and/or operating system designed for an obsolete architecture or platform to run on current systems. It is a highly complex method that offers certain benefits; notably, this is the most conservative method, as it is not destructive of the content or presentation. It allows us to run the original program used to create a digital object without altering that bitstream ; it “keeps the look and feel as well as the interactivity” (Granger, 2000) and as such, it is particularly interesting for objects rich in interaction, such as video games, data visualisations, or formats that are particularly complex (for example, medical imagery, 3D modelisations), for which methods like migration would be hard. It also ensures complete integrity and authenticity, which could be crucial for some data types.

There are numerous criticisms against emulation. A number of legal and intellectual property problems are raised, if the emulation strategy is not supported by the original author [see Nintendo &amp; Apple]. The complexity involved in creating emulators might be not worth it and requires a coherent organisation to reduce overhead (Granger 2000). Additionally, Thibodeau (2002, p. 21) argues that keeping the original functionality is not necessarily a positive thing when it comes to the delivery of the preserved content; “most users in the future will never have encountered—not to mention learned how to use—most of the products they will need to access the preserved information objects”, and “it would cut users off from the possibility of using more advanced technologies for discovery, delivery, and analysis”.

&gt; “Emulators themselves become obsolete; therefore, it becomes necessary either to replace the old em- ulator with a new one or to create a new emulator that allows the old emulator to work on new platforms. In fact, if you get into an emula- tion strategy, you have bought into a migration strategy. Either strategy adds complexity over time.” (thibodeau 2002 p.20)

[NB: due to the length limit of this literature review, I have only explained the most popular archiving techniques. See Thibodeau (2002) and Harvey (2012) for a more comprehensive list of methods, including configurable chips, persistent archives, object interchange format, etc.]

## Choosing one

Thibodeau (2002, pp. 15-16) suggests four criteria to choose a preservation method: *feasibility*, *sustainability*, *practicality* and *appropriateness*. 

Feasability means that the hardware and the software for implementing a given method must be existing and developed. 
Sustainability is ensuring that this method can be “applied indefinitely into the future, or that there are credible grounds for asserting that another path will offer a logical sequel to the method, should it cease being sustainable”. This means that the method must be resistant to technological obsolescence, both for storing the data (medium will change) and accessing it (means to access the media ane decode the bitstream will change).

Practicality is establishing that this method will be reasonably difficult and affordable. [This can generally change depending on the resources of the preserving organisation, the available technology at that time and the valuation of the digital objects.]

Finally, appropriateness is using a method that is relevant to the type of material being preserved, and the objectives of the preservation. For example, del Pozo et al. (2012, p. 7) suggests that if the integrity and authenticity of the documents are not paramount, then a migration that keeps the contents but discards the presentation might be the least expensive and simplest option. Inversely, when trying to preserve highly interactive material such as video games or when the specifications for a format are unknown, then emulation is a more pertinent option, because migration would be extremely difficult and inaccurate (Granger, 2000).

### Authenticity/integrity and issues with each method

Different preservation methods yield different results in what is called **authenticity**, that is how faithful the rendition [its presentation] of the preserved object will be. Authenticity is subjective and therefore there is debate around this issue (Del Pozo et al., 2012, p. 7; Cook, 2000); however most agree that while as little difference in the presentation is desirable, it is rarely (if ever) possible to keep a truthful rendition. As an example, del Pozo et al. (2012, p. 8) use a spreadsheet within the migration method:
&gt; “For instance, one institution may decide to normalise a spreadsheet into a PDF, which would retain the cell values but destroy any formula used to calculate them. This would favour a particular presentation of that digital object over the information form of the original. On the other hand, another institution might decide to normalise spreadsheets into ODF, which may lose some formatting but retain the formula used to derive each cell. In this case, being able to retain the information form of the file would be seen as more important.”

Thibodeau (2002, p. 28) however points out that “people will want to use the best available technology—or at least technologies they know how to use—for discovery, retrieval, processing, and delivery of preserved information. There is a danger that to the degree that preservation solutions keep things unaltered they will create barriers to satisfying this basic user requirement.” An overly faithful preservation would therefore not be desirable. Again, this is subject to a high number of parameters.

Another important issue is *integrity*, i.e. “ensuring the object is complete and has not been altered or corrupted” (Harvey, 2012, p. 54; Ross [CITED], 2002, p. 7); that is, the object “is what it purports to be”, and no parts are missing from it. This is particularly important, Harvey (2012, p. 54) notes, for business archiving, where the legal value of a digital object could be lost if we cannot demonstrate that this object hasn’t been tampered with, or that a migration processes have not incurred data losses.
However, as Lynch (2000) has argued, the integrity of objects is only a matter of trust. The guidelines of UNESCO (2003, p. 22) recommend that the best protection for authenticity is proper documentation of the archival process; Harvey (2012, p. 54) expands on this idea: “for instance, if it can be established that a digital object has always been kept in the custody of an archive in which every change to it has been recorded (such as changes resulting from migration of the bit-stream – the dates at which migration occurred, to which media, and so on), then we can be more secure about its integrity.” 

### Emulation / legal rights with videogames



## Use of standards

Some standards such as OAIS (Open Archival Information System), originally developed for space science, could be used. Sadly [no implementation] [source??]

# Personal archiving

On a more personal level, Harvey (2012) and Lukesh (1999) note that personal correspondance might be lost if we do not take steps to preserve them:

&gt; “The widespread shift from writing letters to the use of email has diminished the likelihood that personal correspondence will remain accessible to future historians. Lukesh asked in 1999, ‘Where will our understandings of today and, more critically, the next century be, if this rich source of information is no longer available?’, as scientists, scholars, historians and almost everyone increasingly use email” (Lukesh, 1999) (in Harvey, 2012 p. 32)

While this is a correct idea, I think that, just like the archiving of Usenet, few people kept their letters in the sole purpose of leaving researchers decades from now understand our epoch’s social habits and ways of speaking. There has to be other motivations for preserving our conversations and our objects.

Richard Banks, a researcher at Microsoft Cambridge, suggests that we first preserve artifacts for ourselves. The majority of the time, we simply keep what we consider significant to remind ourselves of particular people, events and places, and to share them with people we care about; we discard what is not meaningful to us by pragmatism (lack of physical space, lack of importance, redundance) (Banks, 2011, p. 6). This selection process contributes to keeping what we want to remember, and what is the legacy we will want to leave to others after our deaths.
However, the digital age changes this situation. First, the space to store our objects on computers and online is becoming virtually unlimited; second, storing photos and data on online services mean we may forget about them in a few years (Banks, 2011).

## What happens after our death
### [Digital heritage]
## Shifts in usages (more photos)
## Advantages (sharing, organisation etc.); record part of our lives we never did before

Digital artifacts have certain fundamental advantages over physical objects: the main two being the ease of organising a collection of data through indexing, cataloguing and adding metadata (Kirk and Sellen, 2010, p. 35), and the ability to duplicate and share that object accurately and virtually cost-free (Banks, 2011). This actually provides numerous advantages in terms of preservation. 

[[+ tie with “In the past ancestors have signified something is important simply by keeping it” part ]]

The first one, ease of organisation, can be significantly helpful in tackling the abundance of digital data. Banks (2011) suggests that the technologies lying behind the ranking in the result of Web search engines could, eventually, help us in understanding and finding what is our most important data. An attempt has been made by the MUSE (Memories USing Email) project (Hangal et al., 2011), which aims to find patterns in our emails archives to help us make sense of this large data set. As the authors note in the introduction:

&gt; “Email has become a de facto medium of record; many people consciously deposit important information into email, knowing they can look it up later, and thereby use their email account as an informal backup device. Therefore, email archives contain or reflect memories that are extremely valuable for the purposes of reminiscence.” (Hangal et al., 2011)

Early users reported that they have been using it to: make a summary of their work progress over the year; extract and organise certain type of data (for instance, the personal out of the professional, or the important out of the more mundane); finding life milestones inside family emails; picking up work that’s been left unfinished and forgotten; renewing with old relationships; and serendipitous discovery. The authors note in the conclusion that the different uses of their program went beyond their original expectations of simply reminiscing: “the stories above include an example of each one of the ‘5R’s’ described by Sellen and Whittaker [2010]: recollection, reminiscing, retrieving, reflecting, and remembering intentions. Further, it suggests that browsing and remembering the past can affect the future.”

The second advantage, the ability to accurately and easily make copies and backups, is fairly obviously an advantage for personal preservation. However, Banks (2011) notes, the process of backing up is troublesome and storage media do not make our data immune. Additionally, the use of online services to store our data creates an abstraction between the storage material and us; about Flickr: “I have no idea what kind of hardware my files are now stored or even where they are geographically. I just expect to have access to them as long as I pay my bills” (Banks, 2011, p. 27).

## Keeping memories attached to digital objects

Banks (2011) also suggests that we could blend in the physical and the digital to allow recording of things we never had the opportunity to before. He gives as example the Weather Camera, an experimental object by Kjen Wilkens (2011) that records wind and temperature along with commentary, instead of an image, as an alternative to recall a place; or 3D backups by Héctor Serrano (2009), another experimental project that allows us to make backups of sentimentally important objects through 3D scanning and 3D printing. 
Similarly, Banks notes tools by Microsoft Research to recreate places in a 3D environment based on photos, allowing to recreate a sentimentally important place at any given point in time provided enough photos are available. A similar technique is used in bigger preservation projects, as described by Zhou et al. (2012), to make high quality 3D models of culturally important places and objects in China.

Kirk and Sellen (2010) found however that ultimately, stories and narrative are more important than the preservation of the artifact itself. They propose that “technology might play a role in capturing and associating stories or narratives with different physical objects”, imagining that we could augment physical objects with RFID tags (and I would add, virtual reality technologies) to pass down stories attached to these objects.

Conclusion</Text>
        </Document>
        <Document ID="29">
            <Title>LiteratureReview</Title>
        </Document>
        <Document ID="34">
            <Title>draft3-annotated</Title>
            <Text>The need for digital preservation
With an exponential growth in data being produced digitally (SINTEF, 2013), comes a growth in the volume of data worth preserving over the long term (Harvey, 2012, p. 33). However, the preservation of digital materials cause much more issues than traditional preservation paradigms; where we can consider that for paper and physical artifacts, “benign neglect may be the best treatment” (Harvey, 2012, p. 10; derived from Bastian, Cloonan and Harvey, 1993; 2011), the rapid changes in technology and the fragility of digital media make this rule dangerous for the survival of data.
First, we should define what are archives, and what they are useful for. As Cook eloquently puts it:
“Archives [...] are a source of memory about the past, about history, heritage, and culture, about personal roots and family connections, about who we are as human beings and about glimpses into our common humanity through recorded information in all media, much more than they are about narrow accountabilities or administrative continuity.” (Cook, 2000)
It is certainly difficult to understand which data was already lost, and what is at stake now. Harvey emitted some criticism of pessimistic views for archiving:
“It was suggested in 2002 that the last 25 years have been a ‘scenario of data loss and poor records that has dogged our progress’ and that, if this is not reversed, ‘the human record of the early 21st century may be unreadable’ (Deegan and Tanner, 2002). Rhetoric of this kind is common in the literature, but is regrettably poorly supported with specifics and evidence. Alarmist descriptions abound: there will be ‘a digital black hole... truly a digital dark age from which information may never reappear’ (Deegan and Taylor, 2002) if we do not address the problem, and we will become an amnesiac society. [...] It is only possible to conclude, as the authors of the Digital Preservation Coalition’s handbook (2008, p.32) did for the UK, that the evidence of data loss is ‘as yet only largely anecdotal ... [but] it is certain that many potentially valuable digital materials have already been lost’.” (Harvey, 2012, pp. 33–34)
￼￼￼
While it is difficult to argue that preserving digital objects is necessary, there is certainly a need to make a selection of what deserves to be protected, and to what level. It is clear that the long term preservation of legal and governmental records, scientific publications (Harvey, 2012, p. 26) and business records (Ross and Gow, 1999, p. iii) on digital form is crucial; but the archival of the wider Internet, cultural artifacts and our personal heritage is subject to discussion.
UNESCO note that those who value a more comprehensive collection argue that “any information may turn out to have long-term value, and that the costs of detailed selection are greater than the costs of collecting and storing everything” (UNESCO, 2003, p. 73). Additionally, digital storage media degrade so rapidly that the content might disappear before a decision about its value has been made (UNESCO, 2003, p. 71; Harvey, 2012, p. 57). Proponents of a more selective collection argue that we could obtain a higher content quality, settle the preservation rights with content producers, and that we do not have the physical resources to store everything anyway (UNESCO, 2003, pp. 70–73; Harvey, 2012, pp. 26, 58). Both UNESCO (2003) and Cook (2000) recognise that a middle ground must be found, and that both approaches are valid depending on the goal of the preservation intent. For example, the Internet Archive (Kahle, 1996) stores copies of as many websites as possible, but discards all images, stylesheets and multimedia objects in the process due to technical limitations; evidently this is not appropriate for valuable websites. The British Library’s Web Archive [footnote: http://www.webarchive.org.uk/ukwa/] curates collections of websites by topic and by events (Queen Jubilee, Olympic and Paralympic Games, London 2005 terrorist attacks, General Elections ...) to reflect the state and opinion of the Web at these given points in history; because fewer sites are archived and their content is deemed important, they have archived full quality versions of these sites. See Masanès (2006, p.83) for a methodology for this type of selection. Cook reminds us however that archivists have a duty of impartiality and objectivity during the selection and must ensure all points of view are being kept, to ensure our shared memory does not “becomes counterfeit, or at least transformed into forgery, manipulation, or imagination” (Cook, 2000).
Hiroyuki Kawano also proposes to use reputation models to let crawling robots consider the “importance, fairness, trustiness, uniqueness and valuation” of a Web page (Kawano, 2008) based on numerous parameters, to
￼
decide whether a page is worth archiving. The automated nature of this selection can, by nature, be inaccurate and inadvertently subjective, therefore a manual selection is still sometimes required.
Threats to digital continuity
UNESCO (2003, pp. 30–31) has compiled a list of threats to what they call digital continuity. The two principal ones are the short lifetime of digital materials carriers (i.e. storage media), and the obsolescence of the means to access these digital materials (i.e. the software to read files and the hardware to read discontinued storage media).
The Digital Preservation Coalition (2008, p. 154) show that the reliability of storage material varies highly depending on the type of material and the storage conditions; for example D3 cartridges tapes start to deteriorate after 50 years in optimal conditions (low temperature and humidity) but only a year in warm and humid atmospheres. The range is even greater with optical media like DVDs, going from 3 months to a theoretical 200 years.
But the media are only part of the equation; because these media become obsolete, we will not necessarily be able to read them again even if they are not deteriorated. Weathley (2003, cited by Harvey, 2012, p. 37) denounced that myth of long-lived media: “[an IT vendor] offered us a special polymer that they guaranteed would preserve a CD-ROM for 100 years. They were unable to answer how we would preserve a CD-ROM player for that length of time.” And as Pearson adds, not only is the CD-ROM drive needed, but also the type of cable to plug that drive to the computer, a computer motherboard that has a connector for this cable, an operating system with drivers for these types of connectors, and a program able to open the file itself (Pearson, 2009, slide 11). All of these elements become rapidly obsolete themselves, which mean that media may not be easily accessible on current technology after just one or two decades, even if the storage media last longer than that. Therefore it is needed to periodically migrate (a process also called “refreshing”) the bitstreams from a media type to a more contemporary one in order to ensure that this bitstream will remain easily accessible (Harvey, 2012). As the bitstream is preserved, only the question of software is left.
￼
Who must preserve, and what we should keep
Digital media changes the original preservation paradigm for deciding who takes the responsibility for archiving content, and how. The processes of creation, fabrication and publication used to be distinct in the “analog” era. But as Nurnberg (1995, p. 21, cited in Harvey, 2012, p. 9) said, “technology tends to erase distinctions between the separate processes of creation, reproduction and distribution that characterise the classic industrial model of print commodities”. Self-publication (particularly on websites) is much more common, and for many publishers the task of preservation is new to them (Harvey, 2012, p. 32); they might not have the awareness, knowledge or resources to do so themselves it is therefore essential that they collaborate with preservation organisations, such as libraries and archives (Ayre and Muir, 2004). See Harvey (2012, chapter 9), Ayre and Muir (2004) and UNESCO (2003, chapter 11) for more resources and case studies. Ayre and Muir (2004) suggest that the complexity and resources needed for digital preservation may mean that individual libraries may not have the possibility to do the preservation themselves like they used to, but instead rely on national libraries and centralised preservation systems to reduce overhead. This system is used by a number of libraries; even here at the University of Dundee, most journals and a selection of books are available through centralised systems (Dawsonera, Ebrary) or publishers directly (ACM, Elsevier Science, Springer).
How to preserve
There are multiple methods for preservation. Each has advantages and issues, although one parameter is particularly influential: authenticity. This refers to the faithfulness of the rendition of the preserved object in the future, compared to when it was created; while the content should not be altered, its presentation may change (Cook, 2000). What is considered appropriately authentic is subject to debate (del Pozo et al., 2012, p. 7; Cook, 2000) and may vary depending on the preservation intent; del Pozo et al. (2012, p. 8) give, as example, the normalisation of a spreadsheet to formats that may accurately retain either the formatting, or the formulae used in each cell. Retaining integrity, that is ensuring the object is “what it purports
￼
to be” and is “complete and has not been altered or corrupted” (Ross, 2002, p. 7, cited in Harvey, 2012, p. 54), can also be crucially important, particularly for proving that a business or legal document has not been tampered with (Harvey, 2012, p. 54). But as Lynch (2000), UNESCO (2003, p. 22) and Harvey (2012, p. 54) argue, integrity is mostly a matter of trust and can only be ensured with a thorough documentation of the archiving process.
Methods for software preservation
NB: due to the length limit of this literature review, I have only explained the most popular archiving techniques. See Thibodeau (2002) and Harvey (2012) for a comprehensive list of methods, including configurable chips, persistent archives, object interchange format, etc.
Making hard copies
What appears to be the simplest solution is to make physical copies of the digital documents (i.e. print them). Rothenberg (1999) and Granger (2000) both note that this obviously isn’t a viable option for large data sets and interactive content, but Granger recognises it could provide a form of security for simple documents.
Computer Museums
Swade (1993) proposed to preserve a variety of hardware and software to access legacy media and files in centralised places. Granger (2000) critiques this idea, thinking that it would be prohibitely expensive and would only postpone the issue, as computer chips will decay anyway. Rothenberg (1999) has the same arguments but noted that such museums could be helpful for recovering data found on obsolete media, and testing emulators.
Migration
The most popular option is migration, that is, the transformation of a document’s content to a current format — either a different format of the same type (e.g. image type, text type) or a newer version of the same format. This could be, for example, transferring documents from the WordPerfect 6
￼
format (used in the 80s and late 90s) to a Word 2011 format, or a Word 98 file to Word 2011. This should be done regularly, as after a certain period of time new programs will stop having backward compatibility to import these old formats (Thibodeau, 2002). This method has several downsides, mainly being that it is labour intensive and expensive (Thibodeau, 2002; Rothenberg, 1999); product lines, and therefore the migration path, could be terminated at any point (Thibodeau, 2002); the presentation and other characteristics could be changed. Rothenberg (1999) adds that this method is also prone to error, risky (as it could compromise the integrity of the data), not scalable and it requires new solutions for each new format. Granger (2000) argues however that comparatively, other methods (particularly emulation) are potentially much more expensive and labour intensive, and that for most preservation programmes, migration remains the only viable method.
Standardisation
This option is similar to migration, but it preconises switching to a software-agnostic standardised format, which must be well documented or unambiguous, to allow the re-creation of editors and viewers later (Granger 2000). This solution avoids some of the drawbacks with migration (it avoids the need to migrate periodically and the format/software cannot be discontinued) but, because no vendor-specific formatting is allowed, it could lose some specificities of the document and as such is only usable for certain simpler data types (Rothenberg 1999).
Universal Virtual Computer (UVC)
This model proposed by Lorie (2000, in Thibodeau, 2002, p. 22) uses a portable programming language [footnote: Most programming languages include instructions that are specific to a given platform, therefore the same file will need to be adapted to run on another platform. “Portable” languages like Java are called high-level and are more abstract. They are run inside a virtual machine (itself low-level, different for every platform) which worries about transforming Java code into instructions that are specific to that platform; as such, the same file of code can be ran on any platform.] to create an implementation of a viewer which could then theoretically be ran on any later platform supporting that programming language. However, Lorie notes that this approach restricts functionality
￼
and performance, and I am wondering if future implementations of these portable programming languages will be backward compatible with that code.
Emulation
Emulation is the most conservative model; it is not destructive of the original bitstream (unlike migration) and proposes to emulate the characteristics of a legacy platform on a current computer, in order to run the original software used to create it. Therefore it “keeps the look and feel as well as the interactivity” (Granger, 2000) and is particularly useful for highly interactive material like video games and scientific visualisations. It also guarantes integrity and authenticity, which could be crucial in some preservation scenarios.
However, it is highly complex and is criticised for this; Granger (2000) thinks that the amount of work involved in creating emulators would rarely be justified, and if it is, it requires a coherent global organisation to reduce overhead. Emulators also become obsolete (Thibodeau, 2002, p. 20) and so still require a high maintenance cost. Additionally, Thibodeau (2002, p. 21) argues that keeping the original functionality is not necessarily a positive thing when it comes to delivery: it would deprive new users of future technological advances for discovery and analysis, and force them into learning to use a dated interface they might never have seen before.
It might, also, raise a number of legal and intellectual property issues from the software provider. For software that is proprietary or discontinued, the source code is not available, and being able to emulate it requires to do reverse engineering [footnote: Disassembling the original program to understand how it works and recreate a similar version which can operate from a different hardware or software architecture], which breaks intellectual property of the publishers.
Emulation is highly used for video games, as other methods are unsuitable for accurately recreating them. The Library of Congress (2007) gave awards for the preservation of video games; there is a growing interest in being able to play “retro” games, but the consoles are obsolete or discontinued and so are the games. There has been a thriving community of hobbyists reverse engineering console games; this requires considerable efforts (byuu, 2011)
￼
and is generally considered illegal. In response to the growing interest, and to protect their rights and interests[footnote: See http://www.nintendo.com/corp/legal.jsp [Accessed 17th Apr 2014]], Nintendo started to offer their own emulation solution to play classic games on their current generation consoles. This however causes issue for the longer term preservation: there is no certainty that Nintendo will continue to provide this service on their future generations of consoles, or if they ever cease operations. This has happened in the past with Apple, who provided emulation software to run legacy software on their new architectures to ease transition (MacInsider, 2011; Mesa, A. F., 1997) but was later discontinued, effectively removing support for any pre–2005 application on their current models.
Choosing a method
Thibodeau (2002, pp. 15–16) suggests four criteria to choose a preservation method: * Feasibility: means that the hardware and the software for implementing a given method must be existing and developed. * Sustainability: ensuring that the method is resistant to technological obsolescence and that it can be “applied indefinitely into the future, or that there are credible grounds for asserting that another path will offer a logical sequel to the method, should it cease being sustainable” (Thibodeau, 2002). * Practicality: establishing that the method is reasonably easy and affordable, in line with the preserving organisation’s resources. * Appropriateness: the method must be relevant to the type of material being preserved, and the objectives of the preservation. For instance, del Pozo et al. (2012, p. 7) suggest that if the integrity and authenticity of the documents are not paramount, then a migration that keeps the contents but discards its presentation might be the least expensive and simplest option. Inversely, when trying to preserve highly interactive material such as video games or when the specifications for a format are unknown, then emulation is a more pertinent option, because migration would be extremely difficult and inaccurate (Granger, 2000).
Web Archiving
Archiving the Internet and preserving privacy
￼
Camille Paloque-Bergès (2011; 2013) suggests that we can already find historical and sociological evidence of a given period in Internet time when browsing archives of Usenet [footnote: An Internet discussion protocol established in the 1980s and that is widely regarded as the predecessor of the Web and online fora]. She analysed how the way of speaking has evolved [footnote: See also http://wwwtxt.org (or @wwwtxt on Twitter), a project collecting short phrases of the Usenet to demonstrate this] and how a self-organised community had built itself, noting that we can “feel [the community’s] informational generativity” and understand that one of the biggest qualities of the early users of the Internet was their “recursivity as a public” (Paloque-Bergès, 2013); that is, how they welcomed newcomers — notably through the use of FAQs [footnote: Frequently Asked Questions], which weren’t a common concept back then. There are also large elements of nostalgia coming back (Paloque-Bergès, 2011).
It is also interesting to come back on the archival of Usenet. The messages were not systematically archived, and obviously not available on the Web as we know it, as it did not exist and is a separate network (albeit functioning on the same Internet). A number of privacy concerns were caused with the publication of searchable archives (first by DejaVu News in 1995, then Google Groups in 2001); messages who were, 15 to 20 years earlier, thought to be confidential are now considered as our common digital heritage.
Similar concerns were raised at the announcement of the archival of Twitter by the Library of Congress in 2010, but two significant elements make it different: the archive is not public and is made by a not-for-profit body. On the mailing list of the Association of Internet Researchers (2010), the idea was met with interest: “The public twitter stream is of historical cultural significance and is an amazing repository of mundane moments in the daily lives of many people and records of what they thought important” (Baym, 2010) [footnote: Direct link: http://listserv.aoir.org/pipermail/air-l- aoir.org/2010- April/021125.html]. However, the privacy concerns (notably with public and private feeds) were discussed, and as Michael Zimmer remarked:
￼
“This is the classic “but the information is already public” argument that, while technically true, presumes a false dichotomy that information is either strictly public or private, ignoring any contextual norms that might have guided the initial release of information or how a person expects that information to flow.” (Zimmer, 2010 [footnote: Direct link: http://listserv.aoir.org/pipermail/air-l-aoir.org/2010- April/021136.html]; see also his paper (2010) on Facebook privacy, expanding on that argument).
Personal archiving
It is interesting to ask ourselves if the paradigms for digital preservation used by librarians and archivists are still valid for preservation of our personal data. Harvey (2012) and Lukesh (1999, cited in Harvey, 2012, p. 32) note that our personal corespondance and sentimental artifacts stored digitally might be lost if we do not take steps to preserve them, and wonder how we will understand our modern life without these exchanges. While this is a correct idea, I think that, just like the archiving of Usenet, few people kept their letters in the sole purpose of leaving researchers decades from now understand our epoch’s social habits and ways of speaking. There has to be other motivations for preserving our conversations and our objects.
Richard Banks, a researcher at Microsoft Cambridge, suggests that we first preserve artifacts for ourselves. The majority of the time, we simply keep what we consider significant to remind ourselves of particular people, events and places, and to share them with people we care about; we discard what is not meaningful to us by pragmatism (lack of physical space, lack of importance, redundance) (Banks, 2011, p. 6). This selection process contributes to keeping what we want to remember, and what is the legacy we will want to leave to others after our deaths.
However, the digital age changes this situation. First, the space to store our objects on computers and online is becoming virtually unlimited, and so we tend to accumulate data instead of selecting (Banks, 2011); second, the abundance of online services makes it harder to keep track of everything we create. Fortunately, digital objects have fundamental properties that physical artifacts don’t have, which actually provide numerous advantages
￼￼
in terms of preservation: the ease of organising a collection of data through indexing, cataloguing and adding metadata (Kirk and Sellen, 2010, p. 35), and the ability to duplicate and share that object accurately and virtually cost- free (Banks, 2011).
The cataloguing advantage allows us to process it using computer science. Hangal et al. (2011) have developed MUSE (Memories USing Email), a system finding patterns in email archives to help us make sense of this large data set. As the authors note in the introduction, “email has become a de facto medium of record; many people consciously deposit important information into email, knowing they can look it up later”, and so our inboxes contain highly valuable information.
Early users reported that they have been using it to: make a summary of their work progress; extract and organise certain type of data (for instance, the personal out of the professional, or the important out of the more mundane); finding life milestones inside family emails; picking up work that’s been left unfinished and forgotten; or renewing with old relationships. These uses went beyond the authors’ original expectations of simply reminiscing:
“the stories above include an example of each one of the ‘5R’s’ described by Sellen and Whittaker [2010]: recollection, reminiscing, retrieving, reflecting, and remembering intentions. Further, it suggests that browsing and remembering the past can affect the future.” (Hangal et al., 2011)
The copying advantage, while useful for sharing, has issues for backing up. Banks (2011) notes the process is still troublesome, and still feels insecure because of cheap looking media, and nowadays, the abstraction with cloud storage (about Flickr: “I have no idea what kind of hardware my files are now stored on or even where they are geographically. I just expect to have access to them as long as I pay my bills” Banks, 2011, p. 27).
Kirk and Sellen (2010) found however that ultimately, stories and narrative are more important than the preservation of the artifact itself. They propose that “technology might play a role in capturing and associating stories or narratives with different physical objects”, imagining we could also digitally augment physical objects or places. Banks (2011) gives as
￼￼
example the Weather Camera (Wilkens, 2011), which records wind force and temperature along with commentary; a system of physicals backup of sentimental items by Serrano (2009) using 3D scanning and 3D printing; and tools by Microsoft Research to recreate an environment in 3D just using photos. A similar technique is used in bigger preservation projects to make high quality 3D models of culturally important places and objects in China (Zhou et al., 2012).
￼</Text>
        </Document>
        <Document ID="101">
            <Title>Emulation</Title>
            <Synopsis>Does it make sense for social networks? (emulate interface?)
Talk about R. Banks retrieval of old computers</Synopsis>
            <Text>
Emulation is the dynamic transformation of instruction sets to allow software and/or operating system designed for an obsolete architecture or platform to run on current systems. It is a highly complex method that offers certain benefits; notably, this is the most conservative method, as it is not destructive of the content or presentation. It allows us to run the original program used to create a digital object without altering that bitstream ; it “keeps the look and feel as well as the interactivity” (Granger, 2000) and as such, it is particularly interesting for objects rich in interaction, such as video games, data visualisations, or formats that are particularly complex (for example, medical imagery, 3D modelisations), for which methods like migration would be hard. It also ensures complete integrity and authenticity, which could be crucial for some data types.
There are numerous criticisms against emulation. A number of legal and intellectual property problems are raised, if the emulation strategy is not supported by the original author [see Nintendo &amp; Apple]. The complexity involved in creating emulators might be not worth it and requires a coherent organisation to reduce overhead (Granger 2000). Additionally, Thibodeau (2002, p. 21) argues that keeping the original functionality is not necessarily a positive thing when it comes to the delivery of the preserved content; “most users in the future will never have encountered—not to mention learned how to use—most of the products they will need to access the preserved information objects”, and “it would cut users off from the possibility of using more advanced technologies for discovery, delivery, and analysis”.
“Emulators themselves become obsolete; therefore, it becomes necessary either to replace the old emulator with a new one or to create a new emulator that allows the old emulator to work on new platforms. In fact, if you get into an emulation strategy, you have bought into a migration strategy. Either strategy adds complexity over time.” (Thibodeau 2002 p.20)</Text>
        </Document>
        <Document ID="145">
            <Title>Front matter</Title>
        </Document>
        <Document ID="35">
            <Title>Draft3</Title>
            <Text># The need for digital preservation

With an exponential growth in data being produced digitally (SINTEF, 2013), comes a growth in the volume of data worth preserving over the long term (Harvey, 2012, p. 33). However, the preservation of digital materials cause much more issues than traditional preservation paradigms; where we can consider that for paper and physical artifacts, “benign neglect may be the best treatment” (Harvey, 2012, p. 10; derived from Bastian, Cloonan and Harvey, 1993; 2011), the rapid changes in technology and the fragility of digital media make this rule dangerous for the survival of data.

First, we should define what are archives, and what they are useful for. As Cook eloquently puts it:

&gt; “Archives [...] are a source of memory about the past, about history, heritage, and culture, about personal roots and family connections, about who we are as human beings and about glimpses into our common humanity through recorded information in all media, much more than they are about narrow accountabilities or administrative continuity.” (Cook, 2000)

It is certainly difficult to understand which data was already lost, and what is at stake now. Harvey emitted some criticism of pessimistic views for archiving:

&gt; “It was suggested in 2002 that the last 25 years have been a ‘scenario of data loss and poor records that has dogged our progress’ and that, if this is not reversed, ‘the human record of the early 21st century may be unreadable’ (Deegan and Tanner, 2002). Rhetoric of this kind is common in the literature, but is regrettably poorly supported with specifics and evidence. Alarmist descriptions abound: there will be ‘a digital black hole... truly a digital dark age from which information may never reappear’ (Deegan and Taylor, 2002) if we do not address the problem, and we will become an amnesiac society. [...] It is only possible to conclude, as the authors of the Digital Preservation Coalition’s handbook (2008, p.32) did for the UK, that the evidence of data loss is ‘as yet only largely anecdotal ... [but] it is certain that many potentially valuable digital materials have already been lost’.” (Harvey, 2012, pp. 33–34)

While it is difficult to argue that preserving digital objects is necessary, there is certainly a need to make a selection of what deserves to be protected, and to what level. It is clear that the long term preservation of legal and governmental records, scientific publications (Harvey, 2012, p. 26) and business records (Ross and Gow, 1999, p. iii) on digital form is crucial; but the archival of the wider Internet, cultural artifacts and our personal heritage is subject to discussion.

UNESCO note that those who value a more comprehensive collection argue that “any information may turn out to have long-term value, and that the costs of detailed selection are greater than the costs of collecting and storing everything” (UNESCO, 2003, p. 73). Additionally, digital storage media degrade so rapidly that the content might disappear before a decision about its value has been made (UNESCO, 2003, p. 71; Harvey, 2012, p. 57. Proponents of a more selective collection argue that we could obtain a higher content quality, settle the preservation rights with content producers, and that we do not have the physical resources to store everything anyway (UNESCO, 2003, pp. 70-73; Harvey, 2012, pp. 26, 58).
Both UNESCO (2003) and Cook (2000) recognise that a middle ground must be found, and that both approaches are valid depending on the goal of the preservation intent. For example, the Internet Archive (Kahle, 1996) stores copies of as many websites as possible, but discards all images, stylesheets and multimedia objects in the process due to technical limitations; evidently this is not appropriate for valuable websites. The British Library’s Web Archive [footnote: http://www.webarchive.org.uk/ukwa/] curates collections of websites by topic and by events (Queen Jubilee, Olympic and Paralympic Games, London 2005 terrorist attacks, General Elections …) to reflect the state and opinion of the Web at these given points in history; because fewer sites are archived and their content is deemed important, they have archived full quality versions of these sites. See Masanès (2006, p.83) for a methodology for this type of selection. Cook reminds us however that archivists have a duty of impartiality and objectivity during the selection and must ensure all points of view are being kept, to ensure our shared memory does not “becomes counterfeit, or at least transformed into forgery, manipulation, or imagination” (Cook, 2000).

Hiroyuki Kawano also proposes to use reputation models to let crawling robots consider the “importance, fairness, trustiness, uniqueness and valuation” of a Web page (Kawano, 2008) based on numerous parameters, to decide whether a page is worth archiving. The automated nature of this selection can, by nature, be inaccurate and inadvertently subjective, therefore a manual selection is still sometimes required.


## Threats to digital continuity 

UNESCO (2003, pp. 30–31) has compiled a list of threats to what they call *digital continuity*. The two principal ones are the short lifetime of digital materials carriers (i.e. storage media), and the obsolescence of the means to access these digital materials (i.e. the software to read files and the hardware to read discontinued storage media).

The Digital Preservation Coalition (2008, p. 154) show that the reliability of storage material varies highly depending on the type of material and the storage conditions; for example D3 cartridges tapes start to deteriorate after 50 years in optimal conditions (low temperature and humidity) but only a year in warm and humid atmospheres. The range is even greater with optical media like DVDs, going from 3 months to a theoretical 200 years.

But the media are only part of the equation; because these media become obsolete, we will not necessarily be able to read them again even if they are not deteriorated. Weathley (2003, cited by Harvey, 2012, p. 37) denounced that myth of long-lived media: “[an IT vendor] offered us a special polymer that they guaranteed would preserve a CD-ROM for 100 years. They were unable to answer how we would preserve a CD-ROM player for that length of time.” And as Pearson adds, not only is the CD-ROM drive needed, but also the type of cable to plug that drive to the computer, a computer motherboard that has a connector for this cable, an operating system with drivers for these types of connectors, and a program able to open the file itself (Pearson, 2009, slide 11). All of these elements become rapidly obsolete themselves, which mean that media may not be easily accessible on current technology after just one or two decades, even if the storage media last longer than that. Therefore it is needed to periodically migrate (a process also called “refreshing”) the bitstreams from a media type to a more contemporary one in order to ensure that this bitstream will remain easily accessible (Harvey, 2012). As the bitstream is preserved, only the question of software is left.

# Who must preserve, and what we should keep

Digital media changes the original preservation paradigm for deciding who takes the responsibility for archiving content, and how. The processes of creation, fabrication and publication used to be distinct in the “analog” era. But as Nurnberg (1995, p. 21, cited in Harvey, 2012, p. 9) said, “technology tends to erase distinctions between the separate processes of creation, reproduction and distribution that characterise the classic industrial model of print commodities”. Self-publication (particularly on websites) is much more common, and for many publishers the task of preservation is new to them (Harvey, 2012, p. 32); they might not have the awareness, knowledge or resources to do so themselves it is therefore essential that they collaborate with preservation organisations, such as libraries and archives (Ayre and Muir, 2004). See Harvey (2012, chapter 9), Ayre and Muir (2004) and UNESCO (2003, chapter 11) for more resources and case studies. 
Ayre and Muir (2004) suggest that the complexity and resources needed for digital preservation may mean that individual libraries may not have the possibility to do the preservation themselves like they used to, but instead rely on national libraries and centralised preservation systems to reduce overhead. This system is used by a number of libraries; even here at the University of Dundee, most journals and a selection of books are available through centralised systems (Dawsonera, Ebrary) or publishers directly (ACM, Elsevier Science, Springer).

# How to preserve

There are multiple methods for preservation. Each has advantages and issues, although one parameter is particularly influential: **authenticity**. This refers to the faithfulness of the rendition of the preserved object in the future, compared to when it was created; while the content should not be altered, its presentation may change (Cook, 2000). What is considered appropriately authentic is subject to debate (del Pozo et al., 2012, p. 7; Cook, 2000) and may vary depending on the preservation intent; del Pozo et al. (2012, p. 8) give, as example, the normalisation of a spreadsheet to formats that may accurately retain either the formatting, or the formulae used in each cell.
Retaining integrity, that is ensuring the object is “what it purports to be” and is “complete and has not been altered or corrupted” (Ross, 2002, p. 7, cited in Harvey, 2012, p. 54), can also be crucially important, particularly for proving that a business or legal document has not been tampered with (Harvey, 2012, p. 54). But as Lynch (2000), UNESCO (2003, p. 22) and Harvey (2012, p. 54) argue, integrity is mostly a matter of trust and can only be ensured with a thorough documentation of the archiving process.


## Methods for software preservation

*NB: due to the length limit of this literature review, I have only explained the most popular archiving techniques. See Thibodeau (2002) and Harvey (2012) for a comprehensive list of methods, including configurable chips, persistent archives, object interchange format, etc.*

## Making hard copies

What appears to be the simplest solution is to make physical copies of the digital documents (i.e. print them). Rothenberg (1999) and Granger (2000) both note that this obviously isn’t a viable option for large data sets and interactive content, but Granger recognises it could provide a form of security for simple documents.

## Computer Museums
Swade (1993) proposed to preserve a variety of hardware and software to access legacy media and files in centralised places. Granger (2000) critiques this idea, thinking that it would be prohibitely expensive and would only postpone the issue, as computer chips will decay anyway. Rothenberg (1999) has the same arguments but noted that such museums could be helpful for recovering data found on obsolete media, and testing emulators.

## Migration 
The most popular option is migration, that is, the transformation of a document’s content to a current format — either a different format of the same type (e.g. image type, text type) or a newer version of the same format. This could be, for example, transferring documents from the WordPerfect 6 format (used in the 80s and late 90s) to a Word 2011 format, or a Word 98 file to Word 2011. This should be done regularly, as after a certain period of time new programs will stop having backward compatibility to import these old formats (Thibodeau, 2002). This method has several downsides, mainly being that it is labour intensive and expensive (Thibodeau, 2002; Rothenberg, 1999); product lines, and therefore the migration path, could be terminated at any point (Thibodeau, 2002); the presentation and other characteristics could be changed. Rothenberg (1999) adds that this method is also prone to error, risky (as it could compromise the integrity of the data), not scalable and it requires new solutions for each new format. Granger (2000) argues however that comparatively, other methods (particularly emulation) are potentially much more expensive and labour intensive, and that for most preservation programmes, migration remains the only viable method.

## Standardisation

This option is similar to migration, but it preconises switching to a software-agnostic standardised format, which must be well documented or unambiguous, to allow the re-creation of editors and viewers later (Granger 2000). This solution avoids some of the drawbacks with migration (it avoids the need to migrate periodically and the format/software cannot be discontinued) but, because no vendor-specific formatting is allowed, it could lose some specificities of the document and as such is only usable for certain simpler data types (Rothenberg 1999).

## Universal Virtual Computer (UVC)
This model proposed by Lorie (2000, in Thibodeau, 2002, p. 22) uses a portable programming language [footnote: Most programming languages include instructions that are specific to a given platform, therefore the same file will need to be adapted to run on another platform. “Portable” languages like Java are called high-level and are more abstract. They are run inside a virtual machine (itself low-level, different for every platform) which worries about transforming Java code into instructions that are specific to that platform; as such, the same file of code can be ran on any platform.] to create an implementation of a viewer which could then theoretically be ran on any later platform supporting that programming language. However, Lorie notes that this approach restricts functionality and performance, and I am wondering if future implementations of these portable programming languages will be backward compatible with that code.

## Emulation

Emulation is the most conservative model; it is not destructive of the original bitstream (unlike migration) and proposes to emulate the characteristics of a legacy platform on a current computer, in order to run the original software used to create it. Therefore it “keeps the look and feel as well as the interactivity” (Granger, 2000) and is particularly useful for highly interactive material like video games and scientific visualisations. It also guarantes integrity and authenticity, which could be crucial in some preservation scenarios.

However, it is highly complex and is criticised for this; Granger (2000) thinks that the amount of work involved in creating emulators would rarely be justified, and if it is, it requires a coherent global organisation to reduce overhead. Emulators also become obsolete (Thibodeau, 2002, p. 20) and so still require a high maintenance cost. Additionally, Thibodeau (2002, p. 21) argues that keeping the original functionality is not necessarily a positive thing when it comes to delivery: it would deprive new users of future technological advances for discovery and analysis, and force them into learning to use a dated interface they might never have seen before.

It might, also, raise a number of legal and intellectual property issues from the software provider. For software that is proprietary or discontinued, the source code is not available, and being able to emulate it requires to do reverse engineering [footnote: Disassembling the original program to understand how it works and recreate a similar version which can operate from a different hardware or software architecture], which breaks intellectual property of the publishers.

Emulation is highly used for video games, as other methods are unsuitable for accurately recreating them. The Library of Congress (2007) gave awards for the preservation of video games; there is a growing interest in being able to play “retro” games, but the consoles are obsolete or discontinued and so are the games. There has been a thriving community of hobbyists reverse engineering console games; this requires considerable efforts (byuu, 2011) and is generally considered illegal. In response to the growing interest, and to protect their rights and interests[footnote: See http://www.nintendo.com/corp/legal.jsp [Accessed 17th Apr 2014]], Nintendo started to offer their own emulation solution to play classic games on their current generation consoles. This however causes issue for the longer term preservation: there is no certainty that Nintendo will continue to provide this service on their future generations of consoles, or if they ever cease operations. This has happened in the past with Apple, who provided emulation software to run legacy software on their new architectures to ease transition (MacInsider, 2011; Mesa, A. F., 1997) but was later discontinued, effectively removing support for any pre-2005 application on their current models.

## Choosing a method

Thibodeau (2002, pp. 15–16) suggests four criteria to choose a preservation method:
* **Feasibility**: means that the hardware and the software for implementing a given method must be existing and developed.
* **Sustainability**: ensuring that the method is resistant to technological obsolescence and that it can be “applied indefinitely into the future, or that there are credible grounds for asserting that another path will offer a logical sequel to the method, should it cease being sustainable” (Thibodeau, 2002).
* **Practicality**: establishing that the method is reasonably easy and affordable, in line with the preserving organisation’s resources.
* **Appropriateness**: the method must be relevant to the type of material being preserved, and the objectives of the preservation. For instance, del Pozo et al. (2012, p. 7) suggest that if the integrity and authenticity of the documents are not paramount, then a migration that keeps the contents but discards its presentation might be the least expensive and simplest option. Inversely, when trying to preserve highly interactive material such as video games or when the specifications for a format are unknown, then emulation is a more pertinent option, because migration would be extremely difficult and inaccurate (Granger, 2000).

# Web Archiving

## Archiving the Internet and preserving privacy

Camille Paloque-Bergès (2011; 2013) suggests that we can already find historical and sociological evidence of a given period in Internet time when browsing archives of Usenet [footnote: An Internet discussion protocol established in the 1980s and that is widely regarded as the predecessor of the Web and online fora]. She analysed how the way of speaking has evolved [footnote: See also http://wwwtxt.org (or @wwwtxt on Twitter), a project collecting short phrases of the Usenet to demonstrate this] and how a self-organised community had built itself, noting that we can “feel [the community’s] informational generativity” and understand that one of the biggest qualities of the early users of the Internet was their “recursivity as a public” (Paloque-Bergès, 2013); that is, how they welcomed newcomers — notably through the use of FAQs [footnote: Frequently Asked Questions], which weren’t a common concept back then. There are also large elements of nostalgia coming back (Paloque-Bergès, 2011).

It is also interesting to come back on the *archival* of Usenet. The messages were not systematically archived, and obviously not available on the Web as we know it, as it did not exist and is a separate network (albeit functioning on the same Internet). A number of privacy concerns were caused with the publication of searchable archives (first by DejaVu News in 1995, then Google Groups in 2001); messages who were, 15 to 20 years earlier, thought to be confidential are now considered as our common digital heritage.

Similar concerns were raised at the announcement of the archival of Twitter by the Library of Congress in 2010, but two significant elements make it different: the archive is not public and is made by a not-for-profit body. On the mailing list of the Association of Internet Researchers (2010), the idea was met with interest: “The public twitter stream is of historical cultural significance and is an amazing repository of mundane moments in the daily lives of many people and records of what they thought important” (Baym, 2010) [footnote: Direct link: http://listserv.aoir.org/pipermail/air-l-aoir.org/2010- April/021125.html]. However, the privacy concerns (notably with public and private feeds) were discussed, and as Michael Zimmer remarked:
&gt; “This is the classic “but the information is already public” argument that, while technically true, presumes a false dichotomy that information is either strictly public or private, ignoring any contextual norms that might have guided the initial release of information or how a person expects that information to flow.” (Zimmer, 2010 [footnote: Direct link: http://listserv.aoir.org/pipermail/air-l-aoir.org/2010-April/021136.html]; see also his paper (2010) on Facebook privacy, expanding on that argument).

# Personal archiving

It is interesting to ask ourselves if the paradigms for digital preservation used by librarians and archivists are still valid for preservation of our personal data. Harvey (2012) and Lukesh (1999, cited in Harvey, 2012, p. 32) note that our personal corespondance and sentimental artifacts stored digitally might be lost if we do not take steps to preserve them, and wonder how we will understand our modern life without these exchanges. 
While this is a correct idea, I think that, just like the archiving of Usenet, few people kept their letters in the sole purpose of leaving researchers decades from now understand our epoch’s social habits and ways of speaking. There has to be other motivations for preserving our conversations and our objects.

Richard Banks, a researcher at Microsoft Cambridge, suggests that we first preserve artifacts for ourselves. The majority of the time, we simply keep what we consider significant to remind ourselves of particular people, events and places, and to share them with people we care about; we discard what is not meaningful to us by pragmatism (lack of physical space, lack of importance, redundance) (Banks, 2011, p. 6). This selection process contributes to keeping what we want to remember, and what is the legacy we will want to leave to others after our deaths.

However, the digital age changes this situation. First, the space to store our objects on computers and online is becoming virtually unlimited, and so we tend to accumulate data instead of selecting (Banks, 2011); second, the abundance of online services makes it harder to keep track of everything we create. Fortunately, digital objects have fundamental properties that physical artifacts don’t have, which actually provide numerous advantages in terms of preservation: the ease of organising a collection of data through indexing, cataloguing and adding metadata (Kirk and Sellen, 2010, p. 35), and the ability to duplicate and share that object accurately and virtually cost-free (Banks, 2011).

The cataloguing advantage allows us to process it using computer science. Hangal et al. (2011) have developed MUSE (Memories USing Email), a system finding patterns in email archives to help us make sense of this large data set. As the authors note in the introduction, “email has become a de facto medium of record; many people consciously deposit important information into email, knowing they can look it up later”, and so our inboxes contain highly valuable information.

Early users reported that they have been using it to: make a summary of their work progress; extract and organise certain type of data (for instance, the personal out of the professional, or the important out of the more mundane); finding life milestones inside family emails; picking up work that’s been left unfinished and forgotten; or renewing with old relationships. These uses went beyond the authors’ original expectations of simply reminiscing:
&gt; “the stories above include an example of each one of the ‘5R’s’ described by Sellen and Whittaker [2010]: recollection, reminiscing, retrieving, reflecting, and remembering intentions. Further, it suggests that browsing and remembering the past can affect the future.” (Hangal et al., 2011)

The copying advantage, while useful for sharing, has issues for backing up. Banks (2011) notes the process is still troublesome, and still *feels* insecure because of cheap looking media, and nowadays, the abstraction with cloud storage (about Flickr: “I have no idea what kind of hardware my files are now stored on or even where they are geographically. I just expect to have access to them as long as I pay my bills” Banks, 2011, p. 27).

Kirk and Sellen (2010) found however that ultimately, stories and narrative are more important than the preservation of the artifact itself. They propose that “technology might play a role in capturing and associating stories or narratives with different physical objects”, imagining we could also digitally augment physical objects or places. Banks (2011) gives as example the Weather Camera (Wilkens, 2011), which records wind force and temperature along with commentary; a system of physicals backup of sentimental items by Serrano (2009) using 3D scanning and 3D printing; and tools by Microsoft Research to recreate an environment in 3D just using photos. A similar technique is used in bigger preservation projects to make high quality 3D models of culturally important places and objects in China (Zhou et al., 2012).</Text>
        </Document>
        <Document ID="102">
            <Title>Choosing a method</Title>
            <Text>Thibodeau (2002, pp. 15–16) suggests four criteria to choose a preservation method: feasibility, sustainability, practicality and appropriateness.
Feasibility means that the hardware and the software for implementing a given method must be existing and developed. Sustainability is ensuring that this method can be “applied indefinitely into the future, or that there are credible grounds for asserting that another path will offer a logical sequel to the method, should it cease being sustainable”. This means that the method must be resistant to technological obsolescence, both for storing the data (medium will change) and accessing it (means to access the media and decode the bitstream will change).
Practicality is establishing that this method will be reasonably difficult and affordable. This can generally change depending on the resources of the preserving organisation, the available technology at that time and the valuation of the digital objects.
Finally, appropriateness is using a method that is relevant to the type of material being preserved, and the objectives of the preservation. For example, del Pozo et al. (2012, p. 7) suggests that if the integrity and authenticity of the documents are not paramount, then a migration that keeps the contents but discards the presentation might be the least expensive and simplest option. Inversely, when trying to preserve highly interactive material such as video games or when the specifications for a format are unknown, then emulation is a more pertinent option, because migration would be extremely difficult and inaccurate (Granger, 2000).</Text>
        </Document>
        <Document ID="150">
            <Title>Process-related</Title>
            <Text>Finally, we can wonder whether trying to represent time is relevant at all. Haynes (2014) encourages us to stop trying to use units, and points out the historical and anthropological works of Sasha Stern and Evans-Pritchard, which respectively analyse the ancient Jews’ and the Nuers’ (now South Sudan) ways of constructing time in their day-to-day thinking. Instead of using time units and points of reference such as calendar dates, relative dates (“two days ago”)  and hours of the day, they used events, processes and activities as points of reference for describing time.
Stern believed that in ancient Judaism, for those speaking Hebrew and Aramaic, “the notion of time as an entity in itself, a human resource, a continuous flow, or a structure of dimension of the created world, [is] simply non-existent” (Stern, 2003, p.127, cited in Lipton, 2005). While many considered that this belief was naïve (Lipton, 2005), [it would be hard to argue that their vision of time was clearly different].

——
Haynes, 2014: &amp;&amp; Stern, 2003

Why? Let us consider what Time means. We think of it as a neutral entity without a history of its own, a pristine backdrop against which the messy action of life takes place. But this idea is human-made and historically specific, rather than a natural and universal fact. It is most likely isolated to Indo-European cultures, amplified by the ancient Greeks and given its modern thrust by Newtonians. In short, to paraphrase Sacha Stern, the majority of cultures over history have not had a word for Time as a thing in itself.

&gt;&gt; “In antiquity, the world-view of Hebrew and Aramaic-speaking Jews remained completely process-related. Reality was seen as a succession of objects and events, whereas the notion of time as an entity in itself, a human resource, a continuous flow, or a structure of dimension of the created world, were simply non-existent.” (Stern, 2003, p.127, cited in Lipton, 2005)
&gt; Stern believes that ancient Judaism did not have any concept of time whatsoever; only events and processes. (Lipton, 2005)
+ The Neuer / cattle clock thingy: (Haynes 2014, Evans-Pritchard 1940)


What’s more, as Stern reminds us, we might live more comfortably without it. Consider the anthropologist E Evans-Pritchard’s comparison of his own sense of Time with that of the Nuer herders he lived with in the 1930s in what is now South Sudan:

"I do not think that [the Nuer] ever experience the same feeling of fighting against time or having to coordinate activities with an abstract passage of time, because their points of reference are mainly the activities themselves."

The Nuer herders lived by the cattle clock, he said, translating it into an abstract metaphor to aid his Western readers’ understanding. On Stereochron we have encountered something close to this in the ways the campaign secretary’s family judge when to harvest crops or take cattle to market.

The cattle clock does not display Time as a succession of identical units. Rather its irregular, multiple, parallel phases are defined by the events they describe. Vestiges of this are left in our language: the blink of an eye, the shake of a lamb’s tale, moons ago, the crack of dawn. On Stereochron we have expanded those measures to changes in the sheep’s eye, the blooming of the evening primrose, the colour-shift of the sky, the crescendo of the dawn chorus, and the length and orientation of shadows.
As we learnt on our Midsummer field trip, standard clock-time is not an objective model for physics any more than it is for anthropology. In as far as we must use it for certain essential tasks on Stereochron that require accurate synchrony, let’s remember that the standard clock’s tick-tock is just another form of event against which to measure other events, as Stern points out.

“In my experience Nuer do not to any great extent use the names of the months to indicate the time of an event, but generally refer instead to some outstanding activity in process at the time of its occurrence, e.g. at the time of early camps, at the time of weeding, at the time of harvesting, &amp;c., and it is easily understandable that they do so, since time is to them a relation between activities. During the rains the stages in the growth of millet and the steps taken in its culture are often used as points of reference. Pastoral activities, being largely un- differentiated throughout the months and seasons, do not provide suitable points. There are no units of time between the month and day and night. People indicate the occurrence of an event more than a day or two ago by reference to some other event which took place at the same time or by counting the number of intervening 'sleeps' or, less commonly, 'suns’.”
(Evans-Pritchard 1940)</Text>
        </Document>
        <Document ID="36">
            <Title>LiteratureReview-light</Title>
            <Text>DJ32004 RESEARCH &amp; CREATIVE PRACTICE PART 1 LITERATURE REVIEW
DIGITAL PRESERVATION
Victor Loux April 2014
Digital Interaction Design University of Dundee
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼
Table of contents
2 Table of contents
2 List of figures
3 Introduction
4 The need for digital preservation
6 Threats to digital continuity
7 Who must preserve, and what we should keep
7 How to preserve
8 Methods for software preservation
9 Choosing a method
9 Personal archiving
12 Conclusion
13 Bibliography
17 Project Proposal Form
List of figures
1.1 and 1.2 Kjen Wilkens, Sensor Poetics / Weather Camera http://www.kjenwilkens.com/projects/sensor-poetics
2.1 Héctor Seranno, Back Up Objects
http://www.hectorserrano.com/index.php?id=41&amp;m=lab&amp;grupo=backup
￼￼2
Introduction
As we shift into the Information Age and a knowledge-based economy, a number of concerns are being raised over the long-term preservation of digital data and artifacts.
This literature review aims to get an overview of the current problems, techniques and prac- tices in preserving data on digital media, including the definition and understanding of the new paradigms created by digital preservation, the importance of being able to store things on the long term, the processes used to curate and select content to be preserved, and the var- ious methods used to do so. It also encompasses personal archiving which will be the focus of my own research.
3
The need for digital preservation
With an exponential growth in data being produced digitally (SINTEF, 2013), comes a signif- icant growth in the volume of data worth archiving over the long term (Harvey, 2012, p. 33). However, the preservation of digital materials cause much more issues than traditional pres- ervation paradigms; where we can consider that for paper and physical artefacts, “benign ne- glect may be the best treatment” (Harvey, 2012, p. 10; derived from Bastian, Cloonan and Har- vey, 1993; 2011), the rapid changes in technology and the fragility of digital media make this rule dangerous for the survival of data.
First, we should define what are archives, and what they are useful for. As Cook eloquently puts it:
“Archives [...] are a source of memory about the past, about history, heritage, and culture, about personal roots and family connections, about who we are as human beings and about glimpses into our common humanity through recorded information in all media, much more than they are about narrow accountabilities or administrative continuity.” (Cook, 2000)
It is certainly difficult to understand which data was already lost, and what is at stake now. Harvey emitted some criticism of pessimistic views for archiving:
“It was suggested in 2002 that the last 25 years have been a ‘scenario of data loss and poor records that has dogged our progress’ and that, if this is not reversed, ‘the human record of the early 21st century may be unreadable’ (Deegan and Tanner, 2002). Rhetoric of this kind is common in the literature, but is regrettably poorly supported with specifics and evidence. Alarmist descriptions abound: there will be ‘a digital black hole... truly a digital dark age from which information may never reappear’ (Deegan and Taylor, 2002) if we do not address the problem, and we will become an amnesiac society. [...] It is only possible to conclude, as the authors of the Digital Preservation Coalition’s handbook (2008, p. 32) did for the UK, that the evidence of data loss is ‘as yet only largely anecdotal ... [but] it is certain that many potentially valuable digital materials have already been lost’.” (Harvey, 2012, pp. 33–34)
4
While it is difficult to argue that preserving digital objects is necessary, there is certainly a need to make a selection of what deserves to be protected, and to what level. It is clear that the long term preservation of legal and governmental records, scientific publications (Harvey, 2012, p. 26) and business records (Ross and Gow, 1999, p. iii) on digital form is crucial; but the archival of the wider Internet, cultural artefacts and our personal heritage is subject to discus- sion, as fewer items could be considered valuable.
UNESCO note that those who value a more comprehensive collection argue that “any informa- tion may turn out to have long-term value, and that the costs of detailed selection are greater than the costs of collecting and storing everything” (UNESCO, 2003, p. 73). Additionally, digi- tal storage media degrade so rapidly that the content might disappear before a decision about its value has been made (UNESCO, 2003, p. 71; Harvey, 2012, p. 57). Proponents of a more selective collection argue that we could obtain a higher content quality, settle the preserva- tion rights with content producers, and that we do not have the physical resources to store everything anyway (UNESCO, 2003, pp. 70-73; Harvey, 2012, pp. 26, 58).
Both UNESCO (2003) and Cook (2000) recognise that a middle ground must be found, and that both approaches are valid depending on the goal of the preservation intent. To give an ex- ample in archiving the Web, the Internet Archive (Kahle, 1996) has chosen to store copies of as many websites as possible, but due to technical limitations discards all images, stylesheets and multimedia objects; evidently this is not appropriate for valuable websites. The British Library’s Web Archive 1 curates collections of websites by topic and by events (Queen Jubi- lee, London 2005 terrorist attacks, General Elections, Olympic and Paralympic Games, ...) to reflect the state and opinion of the Web at these given points in history; because fewer sites are archived and their content is deemed important, they have archived full quality versions of these sites. See Masanès (2006, p. 83) for a methodology for this type of selection. Cook reminds us however that archivists have a duty of impartiality and objectivity during the se- lection and must ensure all points of view are being kept, to ensure our shared memory does not “becomes counterfeit, or at least transformed into forgery, manipulation, or imagination” (Cook, 2000).
Hiroyuki Kawano also proposes to use reputation models to let crawling robots consider the
￼1 http://www.webarchive.org.uk/ukwa/ [Accessed: 11 Apr 2014]
￼5
“importance, fairness, trustiness, uniqueness and valuation” of a Web page (Kawano, 2008) based on numerous parameters, to decide whether a page is worth archiving. The automated nature of this selection can, by nature, be inaccurate and inadvertently subjective, therefore a manual selection is still sometimes required.
Threats to digital continuity
UNESCO (2003, pp. 30–31) has compiled a list of threats to what they call digital continuity. The two principal ones are the short lifetime of digital materials carriers (i.e. storage media), and the obsolescence of the means to access these digital materials (i.e. the software to read files and the hardware to read discontinued storage media). Research by the Digital Preserva- tion Coalition (2008, p. 154) show that the reliability of storage material varies highly depend- ing on the type of material and the storage conditions, going from a few months to decades or even centuries.
But the media are only part of the equation; because all storage media eventually become obsolete, we will not necessarily be able to read them again even if they are not deteriorat- ed. Weathley (2003, cited by Harvey, 2012, p. 37) denounced that myth of long-lived media: “[an IT vendor] offered us a special polymer that they guaranteed would preserve a CD-ROM for 100 years. They were unable to answer how we would preserve a CD-ROM player for that length of time.” And as Pearson adds, not only is the CD-ROM drive needed, but also the type of cable to plug that drive to the computer, a computer motherboard that has a connector for this cable, an operating system with drivers for these types of connectors, and a program able to open the file itself (Pearson, 2009, slide 11). All of these elements become rapidly obsolete themselves, which mean that media may not be easily accessible on current technology after just one or two decades, even if the storage media last longer than that. Therefore it is needed to periodically migrate (a process also called “refreshing”) the bitstreams from a media type to a more contemporary one in order to ensure that this bitstream will remain easily accessi- ble (Harvey, 2012). As the bitstream is preserved, only the question of preserving software to read it is left.
6
Who must preserve, and what we should keep
Digital media also changes the original preservation paradigm for deciding who takes the re- sponsibility for archiving content, and how. Publishing processes used to be distinct in the “analogue” era. But as Nurnberg (1995, p. 21, cited in Harvey, 2012, p. 9) said, “technology tends to erase distinctions between the separate processes of creation, reproduction and dis- tribution that characterise the classic industrial model of print commodities”. Self-publication (particularly on websites) is much more common, and for many publishers the task of preser- vation is new to them (Harvey, 2012, p. 32); they might not have the awareness, knowledge or resources to do so themselves it is therefore essential that they collaborate with preservation organisations, such as libraries and archives (Ayre and Muir, 2004). See Harvey (2012, chapter 9), Ayre and Muir (2004) and UNESCO (2003, chapter 11) for more resources and case studies.
Ayre and Muir (2004) suggest that the complexity and resources needed for digital preser- vation may mean that individual libraries may not have the possibility to do the preservation themselves like they used to, but instead rely on national libraries and centralised preserva- tion systems to reduce overhead. This system is used by a number of libraries; even here at the University of Dundee, most journals and a selection of books are available through centralised systems (Dawsonera, Ebrary) or publishers directly (ACM, Elsevier Science, Springer).
How to preserve
There are multiple methods for preservation. Each has advantages and issues, although one parameter is particularly influential: authenticity. This refers to the faithfulness of the rendi- tion of the preserved object in the future, compared to when it was created; while the content should not be altered, its presentation may change (Cook, 2000). What is considered appro- priately authentic is subject to debate (del Pozo et al., 2012, p. 7; Cook, 2000) and may vary depending on the preservation intent; del Pozo et al. (2012, p. 8) give, as example, the nor- malisation of a spreadsheet to formats that may accurately retain either the formatting, or the formulae used in each cell.
7
Retaining integrity, that is ensuring the object is “what it purports to be” and is “complete and has not been altered or corrupted” (Ross, 2002, p. 7, cited in Harvey, 2012, p. 54), can also be crucially important, particularly for proving that a business or legal document has not been tampered with (Harvey, 2012, p. 54). But as Lynch (2000), UNESCO (2003, p. 22) and Harvey (2012, p. 54) argue, integrity is mostly a matter of trust and can only be ensured with a thor- ough documentation of the archiving process.
Methods for software preservation
[NB: due to the length limit of this literature review, I only give a simplified overview of the main archiving techniques. See Granger (2000), Thibodeau (2002) and Harvey (2012) for a comprehensive list of methods, including configurable chips, persistent archives, object inter- change format, etc.]
• Hard copies: print documents to archive them. Rarely considered a viable option for large data sets and interactive documents (Granger, 2000; Rothenberg, 1998).
• Computer Museums: Swade (1993) proposed to preserve hardware and software central- ised places. Considered short-term and prohibitively expensive by Granger (2000) but can be useful to recover old media or testing emulators (Rothenberg, 1998).
• Migration: the most popular option, consisting of periodically saving files to a current file type (newer version of a format, different format, or a standardised format) (Granger, 2000; Thibodeau, 2002) to avoid technological obsolescence. It is labour intensive and risky, as it compromises authenticity (Rothenberg, 1998).
• Universal Virtual Computer (UVC): proposed by Lorie (2000, in Thibodeau, 2002, p. 22). It consists of implementing a format viewer in a portable programming language like Java 2 which could then theoretically run on any future platform supporting that pro- gramming language. It is however slow and functionally limited (Lorie 2000), and runs the risk of the portable language itself becoming obsolete.
• Emulation is the most conservative model; it simulates a legacy architec-
2 Most programming languages include instructions that are specific to a given platform, therefore the same file will need to be adapted to run on another platform. “Portable” languages like Java are called high-level and are more abstract. They are run inside a virtual machine (itself low-level, different for every platform) which worries about transforming Java code into instructions that are specific to that platform; as such, the same file of code can be ran on any platform.
￼8
ture on current platforms, allowing to run the original software used to create a file, therefore not modifying them. It “keeps the look and feel as well as the in- teractivity” (Granger, 2000) therefore is highly relevant for video games (Gut- tenbrunner et al., 2010) and interactive systems, and it guarantees authenticity. It is however highly complex to implement (Granger, 2000; byuu, 2011) and need renew- al, since emulators themselves become obsolete (Thibodeau, 2002), as exemplified by em- ulators provided by Apple for their architecture switches (Mesa, A. F., 1997; AppleInsider, 2011) that were later discontinued, effectively removing the ability to run pre-2005 appli- cations on their current models.
Choosing a method
Thibodeau (2002, pp. 15–16) suggests four criteria to choose a preservation method. It must be:
• technically feasible
• sustainable and resistant to technological obsolescence
• practical (the preserving organisation must have the resources to do it)
• appropriate to the type of material being preserved, and the objectives of the preservation
(see del Pozo et al., 2012, p. 7 and Granger, 2000 for examples).
Personal archiving
It is interesting to ask ourselves if the paradigms for digital preservation used by librarians and archivists are still valid for preservation of our personal data. Harvey (2012) and Lukesh (1999, cited in Harvey, 2012, p. 32) note that our personal correspondence and sentimental ar- tefacts stored digitally might be lost if we do not take steps to preserve them, and wonder how we will be able to understand our modern life without these exchanges.
While this is a correct idea, I think that few people kept their letters in the sole purpose of leav- ing researchers decades from now understand our epoch’s social habits and ways of speaking. There has to be other motivations for preserving our conversations and our objects.
9
Richard Banks, a researcher at Microsoft Cambridge, suggests that we first preserve artefacts for ourselves. The majority of the time, we simply keep what we consider significant to re- mind ourselves of particular people, events and places, and to share them with people we care about; we discard what is not meaningful to us by pragmatism (lack of physical space, lack of importance, redundancy) (Banks, 2011, p. 6). This selection process contributes to keeping what we want to remember, and what is the legacy we will want to leave to others after our deaths.
However, the digital age changes this situation. First, the space to store our objects on com- puters and online is becoming virtually unlimited, and so we tend to accumulate data instead of selecting (Banks, 2011); second, the abundance of online services makes it harder to keep track of everything we create. Fortunately, digital objects have fundamental properties that physical artefacts don’t have, which actually provide numerous advantages in terms of pres- ervation: the ease of organising a collection of data through indexing, cataloguing and adding metadata (Kirk and Sellen, 2010, p. 35), and the ability to duplicate and share that object accu- rately and virtually cost-free (Banks, 2011).
The cataloguing advantage allows us to process it using computer science. Hangal et al. (2011) have developed MUSE (Memories USing Email), a system finding patterns in email archives to help us make sense of this large data set. As the authors note in the introduction, “email has become a de facto medium of record; many people consciously deposit important information into email, knowing they can look it up later”, and so our inboxes contain highly valuable in- formation.
Early users reported that they have been using it to: make a summary of their work progress; extract and organise certain type of data (for instance, the personal out of the professional, or the important out of the more mundane); finding life milestones inside family emails; picking up work that has been left unfinished and forgotten; or renewing with old relationships. These uses went beyond the authors’ original expectations of simply reminiscing:
“The stories above include an example of each one of the ‘5R’s’ described by Sellen and Whittaker [2010]: recollection, reminiscing, retrieving, reflecting, and remembering in- tentions. Further, it suggests that browsing and remembering the past can affect the fu- ture.” (Hangal et al., 2011)
10
The copying advantage, while useful for sharing, has issues for backing up. Banks (2011) notes the process is still troublesome, and still feels insecure because of cheap looking media, and nowadays, the abstraction with cloud storage (about Flickr: “I have no idea what kind of hard- ware my files are now stored on or even where they are geographically. I just expect to have access to them as long as I pay my bills” Banks, 2011, p. 27).
Kirk and Sellen (2010) found however that ultimately, stories and narrative are more impor- tant than the preservation of the artefact itself. They propose that “technology might play a role in capturing and associating stories or narratives with different physical objects”, imagin- ing we could also digitally augment physical objects or places. Banks (2011) gives as example the Weather Camera (Wilkens, 2011), which records wind force and temperature along with commentary; a system of physicals backup of sentimental items by Serrano (2009) using 3D scanning and 3D printing; and tools by Microsoft Research to recreate an environment in 3D just using photos. A similar technique is used in bigger preservation projects to make high quality 3D models of culturally important places and objects in China (Zhou et al., 2012).
Fig. 1.1: The Weather Camera, by Jens Wilkens. Fig. 1.2: Associating stories and weather to a timeline with the Weather Camera.
Fig. 2: backing up physical sentimental items using a 3D scanner and 3D printing, 11 with Back Up Objects by Héctor Serrano
￼￼￼
Conclusion
This literature review, although shallow considering the breadth of the topic, allowed me to gain an understanding of the current practices and debates in the digital archiving communi- ty, and to gather relevant sources for any future research in each sub-topic. I found the theme of personal archiving captivating but not explored as intensely as large scale preservation for our common knowledge and data; I would like to do deeper research in this particular area of digital preservation, notably on our use of social media and Internet services on the long term.
12
Bibliography
AppleInsider. (2011) ‘Inside Mac OS X 10.7 Lion: Missing Front Row, Rosetta and Java runtime’. AppleInsider. [web page] Accessible at http://appleinsider.com/articles/11/02/26/ mac_os_x_lion_drops_front_row_java_runtime_rosetta.html [Accessed: 19th Apr 2014]
Ayre, C. and Muir, A. (2004) ‘The right to preserve’. D-Lib Magazine, vol. 10 (No. 3). March. Available at http://www.dlib.org/dlib/march04/ayre/03ayre.html [Accessed: 11th Apr 2014]
Banks, R. (2011) The Future of Looking Back. Sebastopol: Microsoft Press.
Bearman, D. (1999) ‘Reality and Chimeras in the Preservation of Electronic Records’. D-Lib Magazine, vol. 5 (No. 4). April. Available at http://www.dlib.org/dlib/april99/ bearman/04bearman.html [Accessed: 7th Apr 2014]
Boyd, D. (2014) It’s Complicated: The Social Lives of Networked Teens. New Haven and London: Yale University Press.
byuu.1 (2011) ‘Accuracy takes power: one man’s 3 GHz quest to build a perfect SNES emulator’. Ars Technica. [web page] Available at: http://arstechnica.com/gaming/2011/08/ accuracy-takes-power-one-mans-3ghz-quest-to-build-a-perfect-snes-emulator/ [Accessed: 8th Apr 2014]
Cook, T. (2000) ‘Beyond the screen: the records continuum and archival cultural heritage.’ Presented at the Australian Society of Archivists Conference, Melbourne, 18 August. Available at http://www.mybestdocs.com/cook-t-beyondthescreen-000818.htm [Accessed: 9th Apr 2014]
Granger, S. (2000) ‘Emulation as a Digital Preservation Strategy’. D-Lib Magazine, vol. 6 (No. 10). October. Available at http://www.dlib.org/dlib/october00/granger/10granger.html [Accessed: 7th Apr 2014]
Guttenbrunner, M., Becker, C. and Rauber, A. (2010) ‘Keeping the game alive: Evaluating strategies for the preservation of console video games’. International Journal of Digital Curation, vol. 5 (Iss. 1), pp. 64-90.
￼￼￼￼￼￼￼￼￼￼1 The article is only credited to the developer’s pseudonym.
13
Bibliography (cont.)
Hangal, S., Lam, M. S. and Heer, J. (2011) ‘MUSE: reviving memories using email archives’, in
Proceedings of the 24th annual ACM symposium on User interface software and technology (UIST ‘11). New York, NY: ACM, pp. 75-84. doi:10.1145/2047196.2047206
Harvey, D. R. (2012) Preserving digital materials. Berlin: De Gruyter Saur.
Kahle, B. (1996) ‘Archiving the Internet’. Scientific American, Iss. March 1997. Available at http://web.archive.org/web/19971211123138/www.archive.org/sciam_article.html [Accessed: 27th Feb 2014]
Kawano, H. (2008) ‘Strategy of Digital Contents Archive Based on Reputation Model’, in ICSENG ‘08. Proceedings of 19th International Conference on Systems Engineering, 19-21 Aug, pp. 288-293. doi: 10.1109/ICSEng.2008.75
Kirk, D. S. and Sellen, A. (2010) ‘On human remains: Values and practice in the home archiving of cherished objects’. ACM Transactions on Human-Computer Interaction, vol. 17 (Iss. 3), article 10. doi:0.1145/1806923.1806924
Library of Congress. (2007) Digital Preservation Program Makes Awards to Preserve American Creative Works. [press release] Available at http://www.loc.gov/today/ pr/2007/07-156.html [Accessed: 14th Apr 2014]
Lubar, S. (1999) ‘Information Culture and the Archival Record’, in American Archivist, vol. 62 (Iss. 1), pp. 10-22.
Lynch, C. (2000) “Authenticity and Integrity in the Digital Environment: An Exploratory Analysis of the Central Role of Trust’, in Authenticity in a Digital Environment. Washington, DC: Council on Library and Information Resources. Available at http://www. clir.org/pubs/abstract/pub92abst.html [Accessed: 16th Apr 2014]
Masanès, J. (2006) Web archiving. Berlin: Springer.
Mesa, A. F. (1997) ‘The PowerPC Triumph’. The Apple Museum. [web page] Available at: http://applemuseum.bott.org/sections/ppc.html [Accessed: 19th Apr 2014]
￼￼￼￼￼￼￼￼￼14
Bibliography (cont.)
Paloque-Bergès, C. (2011) Entre trivialité et culture : une histoire de l’Internet vernaculaire. Émergence et médiations d’un folklore de réseau [Between Triviality and Culture: a History of the Vernacular Internet. Emergence and mediations in network folklore]. PhD thesis, self-published. Available at http://camillepaloqueberges.wordpress.com/phd-thesis/ [Accessed: 6th Apr 2014] 2
Paloque-Bergès, C. (2013) ‘Un patrimoine composite : le public Internet face à l’archivage
de sa matière culturelle’ [A composite heritage: the Internet public confronted to the archiving of its cultural matter], in I. Dragan, P. Stefanescu, N. Pelissier, J.-F. Tétu and
L. Idjeroui-Ravez (eds.) Traces, mémoire, communication. Bucharest: Editura Universităţii din Bucureşti. 2
Pearson, D. (2009) Preserve or Preserve Not, There is No Try: some dilemmas relating
to Personal Digital Archiving. [PowerPoint slides]. National Library of Australia Staff Papers. Available at http://www.nla.gov.au/openpublish/index.php/nlasp/article/view/1388 [Accessed: 9th Apr 2014]
del Pozo, N., Long, A. S. and Pearson, D. (2012) ‘Land of the Lost: a discussion of what can be preserved through digital preservation’. Library Hi Tech, vol. 28 (Iss. 2), pp. 290-300. Available from: doi: 10.1108/07378831011047686. [Accessed: 6th Apr 2014]
Ross, S. and Gow, A. (1999) Digital archaeology: rescuing neglected and damaged data resources: a JISC/NPO study within Electronic Libraries (eLib) Programme on the Preservation of Electronic Materials. London: Library Information Technology Centre. Available at http://www.ukoln.ac.uk/services/elib/papers/supporting/pdf/p2.pdf [Accessed: 17th Apr 2014]
Rothenberg, J. (1998) Avoiding Technological Quicksand: Finding a Viable Technical Foundation for Digital Preservation. [report] Washington, DC: Council on Library and Information Resources.
￼￼￼￼￼2 In French.
15
Bibliography (cont.)
Serrano, H. (2009) Back Up Objects, Héctor Serrano. [web page] Available at: http://www. hectorserrano.com/index.php?id=41&amp;m=lab&amp;grupo=backup [Accessed: 17th Apr 2014].
SINTEF. (2013) ‘Big Data, for better or worse: 90% of world’s data generated over last two years.’ ScienceDaily. [web page] Available at: http://www.sciencedaily.com/ releases/2013/05/130522085217.htm [Accessed: 16th Apr 2014]
Swade, D. (1993) ‘The problems of software conservation”, in Computer Resurrection, vol. 7. Available at: http://www.cs.man.ac.uk/CCS/res/res07.htm#f [Accessed: 12th Apr 2014]
Task Force on Archiving of Digital Information. (1996) Preserving digital information, Report of the Task Force on Archiving of Digital Information. Washington, DC: Council on Library and Information Resources.
Thibodeau, K. (2002) ‘Overview of technological approaches to digital preservation
and challenges in coming years’, in The state of digital preservation: an international perspective. Washington, DC: Council on Library and Information Resources, pp. 4-31.
UNESCO. (2003) Guidelines for the preservation of digital heritage, prepared by the National Library of Australia. Paris: UNESCO. Available at http://unesdoc.unesco.org/ images/0013/001300/130071e.pdf [Accessed: 9 Apr 2014]
Wilkens, K. (2011) Sensor Poetics, Kjenwilkens.com. [web page] Available at http://www. kjenwilkens.com/projects/sensor-poetics [Accessed: 18th Apr 2014]
Zimmer, M. (2010) ‘“But the data is already public”: on the ethics of research in Facebook’, in Ethics and information technology, vol. 12 (Iss. 4), pp. 313-325. doi: 10.1007/s10676-010- 9227-5
Zhou, M., Geng, G. and Wu, Z. (2012) Digital preservation technology for cultural heritage. Berlin: Springer.
￼￼￼￼￼￼￼￼￼￼￼￼16
Project Proposal Form
￼￼Name Programme/Course Project/working title
Project Aims (i.e. what you want to learn about through further investigation. E.g. I want toa learn more about the creative process of animators)
Objectives (i.e. the particular things you propose to do
in order to carry out your investigation. E.g. Make contact with professional animators and arrange
to meet them. Possibly Interview them or observe their creative activities to gather data)
Methods (i.e. the different ways in which you will achieve these objectives. E.g. Open ended interview techniques and grounded theory analysis or video observation of working animators)
Victor Loux
Digital Interaction Design
Digital preservation for the masses
I would like to look at further research papers on personal preservation and focus on ways to apply “large-scale” paradigms to our personal heritage while trying to be respectful of people’s privacy and unintrusive.
I first intend to do further reading in this area, as until now I have been exploring digital preservation as a whole with no particular focus, to get a broader understanding of the issues
and solutions. I would like to interview several people of different age ranges to understand their relationship to technology and their personal digital heritage, notably concerning photographs and conversations.
Using open ended interview techniques, further literature review and possibly research through practice as I intend to explore this theme for my final year project.
￼￼￼￼￼￼￼￼￼￼￼￼17
</Text>
        </Document>
        <Document ID="146">
            <Title>Back matter</Title>
        </Document>
        <Document ID="40">
            <Title>mindmap</Title>
        </Document>
        <Document ID="37">
            <Title>LiteratureReview</Title>
            <Text># The need for digital preservation

With an exponential growth in data being produced digitally (SINTEF, 2013), comes a significant growth in the volume of data worth archiving over the long term (Harvey, 2012, p. 33). However, the preservation of digital materials cause much more issues than traditional preservation paradigms; where we can consider that for paper and physical artifacts, “benign neglect may be the best treatment” (Harvey, 2012, p. 10; derived from Bastian, Cloonan and Harvey, 1993; 2011), the rapid changes in technology and the fragility of digital media make this rule dangerous for the survival of data.

First, we should define what are archives, and what they are useful for. As Cook eloquently puts it:

&gt; “Archives [...] are a source of memory about the past, about history, heritage, and culture, about personal roots and family connections, about who we are as human beings and about glimpses into our common humanity through recorded information in all media, much more than they are about narrow accountabilities or administrative continuity.” (Cook, 2000)

It is certainly difficult to understand which data was already lost, and what is at stake now. Harvey emitted some criticism of pessimistic views for archiving:

&gt; “It was suggested in 2002 that the last 25 years have been a ‘scenario of data loss and poor records that has dogged our progress’ and that, if this is not reversed, ‘the human record of the early 21st century may be unreadable’ (Deegan and Tanner, 2002). Rhetoric of this kind is common in the literature, but is regrettably poorly supported with specifics and evidence. Alarmist descriptions abound: there will be ‘a digital black hole... truly a digital dark age from which information may never reappear’ (Deegan and Taylor, 2002) if we do not address the problem, and we will become an amnesiac society. [...] It is only possible to conclude, as the authors of the Digital Preservation Coalition’s handbook (2008, p.32) did for the UK, that the evidence of data loss is ‘as yet only largely anecdotal ... [but] it is certain that many potentially valuable digital materials have already been lost’.” (Harvey, 2012, pp. 33–34)

While it is difficult to argue that preserving digital objects is necessary, there is certainly a need to make a selection of what deserves to be protected, and to what level. It is clear that the long term preservation of legal and governmental records, scientific publications (Harvey, 2012, p. 26) and business records (Ross and Gow, 1999, p. iii) on digital form is crucial; but the archival of the wider Internet, cultural artifacts and our personal heritage is subject to discussion, as fewer items could be considered valuable.

UNESCO note that those who value a more comprehensive collection argue that “any information may turn out to have long-term value, and that the costs of detailed selection are greater than the costs of collecting and storing everything” (UNESCO, 2003, p. 73). Additionally, digital storage media degrade so rapidly that the content might disappear before a decision about its value has been made (UNESCO, 2003, p. 71; Harvey, 2012, p. 57). Proponents of a more selective collection argue that we could obtain a higher content quality, settle the preservation rights with content producers, and that we do not have the physical resources to store everything anyway (UNESCO, 2003, pp. 70-73; Harvey, 2012, pp. 26, 58).
Both UNESCO (2003) and Cook (2000) recognise that a middle ground must be found, and that both approaches are valid depending on the goal of the preservation intent. To give an example in archiving the Web, the Internet Archive (Kahle, 1996) has chosen to store copies of as many websites as possible, but due to technical limitations discards all images, stylesheets and multimedia objects; evidently this is not appropriate for valuable websites. The British Library’s Web Archive [footnote: http://www.webarchive.org.uk/ukwa/] curates collections of websites by topic and by events (Queen Jubilee, London 2005 terrorist attacks, General Elections, Olympic and Paralympic Games, …) to reflect the state and opinion of the Web at these given points in history; because fewer sites are archived and their content is deemed important, they have archived full quality versions of these sites. See Masanès (2006, p.83) for a methodology for this type of selection. Cook reminds us however that archivists have a duty of impartiality and objectivity during the selection and must ensure all points of view are being kept, to ensure our shared memory does not “becomes counterfeit, or at least transformed into forgery, manipulation, or imagination” (Cook, 2000).

Hiroyuki Kawano also proposes to use reputation models to let crawling robots consider the “importance, fairness, trustiness, uniqueness and valuation” of a Web page (Kawano, 2008) based on numerous parameters, to decide whether a page is worth archiving. The automated nature of this selection can, by nature, be inaccurate and inadvertently subjective, therefore a manual selection is still sometimes required.


## Threats to digital continuity 

UNESCO (2003, pp. 30–31) has compiled a list of threats to what they call *digital continuity*. The two principal ones are the short lifetime of digital materials carriers (i.e. storage media), and the obsolescence of the means to access these digital materials (i.e. the software to read files and the hardware to read discontinued storage media). Research by the Digital Preservation Coalition (2008, p. 154) show that the reliability of storage material varies highly depending on the type of material and the storage conditions, going from a few months to decades or even centuries.

But the media are only part of the equation; because all storage media eventually become obsolete, we will not necessarily be able to read them again even if they are not deteriorated. Weathley (2003, cited by Harvey, 2012, p. 37) denounced that myth of long-lived media: “[an IT vendor] offered us a special polymer that they guaranteed would preserve a CD-ROM for 100 years. They were unable to answer how we would preserve a CD-ROM player for that length of time.” And as Pearson adds, not only is the CD-ROM drive needed, but also the type of cable to plug that drive to the computer, a computer motherboard that has a connector for this cable, an operating system with drivers for these types of connectors, and a program able to open the file itself (Pearson, 2009, slide 11). All of these elements become rapidly obsolete themselves, which mean that media may not be easily accessible on current technology after just one or two decades, even if the storage media last longer than that. Therefore it is needed to periodically migrate (a process also called “refreshing”) the bitstreams from a media type to a more contemporary one in order to ensure that this bitstream will remain easily accessible (Harvey, 2012). As the bitstream is preserved, only the question of preserving software to read it is left.

# Who must preserve, and what we should keep

Digital media also changes the original preservation paradigm for deciding who takes the responsibility for archiving content, and how. The processes of creation, fabrication and publication used to be distinct in the “analog” era. But as Nurnberg (1995, p. 21, cited in Harvey, 2012, p. 9) said, “technology tends to erase distinctions between the separate processes of creation, reproduction and distribution that characterise the classic industrial model of print commodities”. Self-publication (particularly on websites) is much more common, and for many publishers the task of preservation is new to them (Harvey, 2012, p. 32); they might not have the awareness, knowledge or resources to do so themselves it is therefore essential that they collaborate with preservation organisations, such as libraries and archives (Ayre and Muir, 2004). See Harvey (2012, chapter 9), Ayre and Muir (2004) and UNESCO (2003, chapter 11) for more resources and case studies. 
Ayre and Muir (2004) suggest that the complexity and resources needed for digital preservation may mean that individual libraries may not have the possibility to do the preservation themselves like they used to, but instead rely on national libraries and centralised preservation systems to reduce overhead. This system is used by a number of libraries; even here at the University of Dundee, most journals and a selection of books are available through centralised systems (Dawsonera, Ebrary) or publishers directly (ACM, Elsevier Science, Springer).

# How to preserve

There are multiple methods for preservation. Each has advantages and issues, although one parameter is particularly influential: **authenticity**. This refers to the faithfulness of the rendition of the preserved object in the future, compared to when it was created; while the content should not be altered, its presentation may change (Cook, 2000). What is considered appropriately authentic is subject to debate (del Pozo et al., 2012, p. 7; Cook, 2000) and may vary depending on the preservation intent; del Pozo et al. (2012, p. 8) give, as example, the normalisation of a spreadsheet to formats that may accurately retain either the formatting, or the formulae used in each cell.
Retaining integrity, that is ensuring the object is “what it purports to be” and is “complete and has not been altered or corrupted” (Ross, 2002, p. 7, cited in Harvey, 2012, p. 54), can also be crucially important, particularly for proving that a business or legal document has not been tampered with (Harvey, 2012, p. 54). But as Lynch (2000), UNESCO (2003, p. 22) and Harvey (2012, p. 54) argue, integrity is mostly a matter of trust and can only be ensured with a thorough documentation of the archiving process.


## Methods for software preservation

*NB: due to the length limit of this literature review, I only give a simplified overview of the main archiving techniques. See Granger (2000), Thibodeau (2002) and Harvey (2012) for a comprehensive list of methods, including configurable chips, persistent archives, object interchange format, etc.*

* **Hard copies**: print documents to archive them. Rarely considered a viable option for large data sets and interactive documents (Granger, 2000; Rothenberg, 1999).
* **Computer Museums**: Swade (1993) proposed to preserve hardware and software centralised places. Considered short-term and prohibitely expensive by Granger (2000) but can be useful to recover old media or testing emulators (Rothenberg, 1999).
* **Migration**: the most popular option, consisting of periodically saving files to a current file type (newer version of a format, different format, or a standardised format) (Granger, 2000; Thibodeau, 2002) to avoid technological obsolescence. It is labour intensive and risky, as it compromises authenticity (Rothenberg, 1999).
* **Universal Virtual Computer (UVC)**: proposed by Lorie (2000, in Thibodeau, 2002, p. 22). It consists of implementing a format viewer in a portable programming language like Java [footnote: Most programming languages include instructions that are specific to a given platform, therefore the same file will need to be adapted to run on another platform. “Portable” languages like Java are called high-level and are more abstract. They are run inside a virtual machine (itself low-level, different for every platform) which worries about transforming Java code into instructions that are specific to that platform; as such, the same file of code can be ran on any platform.] which could then theoretically run on any future platform supporting that programming language. It is however slow and functionally limited (Lorie 2000), and runs the risk of the portable language itself becoming obsolete.
* **Emulation** is the most conservative model; it simulates a legacy architecture on current platforms, allowing to run the original software used to create a file, therefore not modifying them. It “keeps the look and feel as well as the interactivity” (Granger, 2000) therefore is highly relevant for video games (Guttenbrunner et al., 2010) and interactive systems, and it guarantees authenticity.
It is however highly complex to implement (Granger, 2000; byuu, 2011) and need renewal, since emulators themselves become obsolete (Thibodeau, 2002), as exemplified by emulators provided by Apple for their architecture switches (Mesa, A. F., 1997; AppleInsider, 2011) that were later discontinued, effectively removing the ability to run pre-2005 applications on their current models.

## Choosing a method

Thibodeau (2002, pp. 15–16) suggests four criteria to choose a preservation method. It must be:
* technically feasible
* sustainable and resistant to technological obsolescence
* practical (the preserving organisation must have the resources to do it)
* appropriate to the type of material being preserved, and the objectives of the preservation (see del Pozo et al., 2012, p. 7 and Granger, 2000 for examples).

# Personal archiving

It is interesting to ask ourselves if the paradigms for digital preservation used by librarians and archivists are still valid for preservation of our personal data. Harvey (2012) and Lukesh (1999, cited in Harvey, 2012, p. 32) note that our personal corespondance and sentimental artifacts stored digitally might be lost if we do not take steps to preserve them, and wonder how we will be able to understand our modern life without these exchanges. 
While this is a correct idea, I think that, just like the archiving of Usenet, few people kept their letters in the sole purpose of leaving researchers decades from now understand our epoch’s social habits and ways of speaking. There has to be other motivations for preserving our conversations and our objects.

Richard Banks, a researcher at Microsoft Cambridge, suggests that we first preserve artifacts for ourselves. The majority of the time, we simply keep what we consider significant to remind ourselves of particular people, events and places, and to share them with people we care about; we discard what is not meaningful to us by pragmatism (lack of physical space, lack of importance, redundance) (Banks, 2011, p. 6). This selection process contributes to keeping what we want to remember, and what is the legacy we will want to leave to others after our deaths.

However, the digital age changes this situation. First, the space to store our objects on computers and online is becoming virtually unlimited, and so we tend to accumulate data instead of selecting (Banks, 2011); second, the abundance of online services makes it harder to keep track of everything we create. Fortunately, digital objects have fundamental properties that physical artifacts don’t have, which actually provide numerous advantages in terms of preservation: the ease of organising a collection of data through indexing, cataloguing and adding metadata (Kirk and Sellen, 2010, p. 35), and the ability to duplicate and share that object accurately and virtually cost-free (Banks, 2011).

The cataloguing advantage allows us to process it using computer science. Hangal et al. (2011) have developed MUSE (Memories USing Email), a system finding patterns in email archives to help us make sense of this large data set. As the authors note in the introduction, “email has become a de facto medium of record; many people consciously deposit important information into email, knowing they can look it up later”, and so our inboxes contain highly valuable information.

Early users reported that they have been using it to: make a summary of their work progress; extract and organise certain type of data (for instance, the personal out of the professional, or the important out of the more mundane); finding life milestones inside family emails; picking up work that’s been left unfinished and forgotten; or renewing with old relationships. These uses went beyond the authors’ original expectations of simply reminiscing:
&gt; “the stories above include an example of each one of the ‘5R’s’ described by Sellen and Whittaker [2010]: recollection, reminiscing, retrieving, reflecting, and remembering intentions. Further, it suggests that browsing and remembering the past can affect the future.” (Hangal et al., 2011)

The copying advantage, while useful for sharing, has issues for backing up. Banks (2011) notes the process is still troublesome, and still *feels* insecure because of cheap looking media, and nowadays, the abstraction with cloud storage (about Flickr: “I have no idea what kind of hardware my files are now stored on or even where they are geographically. I just expect to have access to them as long as I pay my bills” Banks, 2011, p. 27).

Kirk and Sellen (2010) found however that ultimately, stories and narrative are more important than the preservation of the artifact itself. They propose that “technology might play a role in capturing and associating stories or narratives with different physical objects”, imagining we could also digitally augment physical objects or places. Banks (2011) gives as example the Weather Camera (Wilkens, 2011), which records wind force and temperature along with commentary; a system of physicals backup of sentimental items by Serrano (2009) using 3D scanning and 3D printing; and tools by Microsoft Research to recreate an environment in 3D just using photos. A similar technique is used in bigger preservation projects to make high quality 3D models of culturally important places and objects in China (Zhou et al., 2012).</Text>
        </Document>
        <Document ID="41">
            <Title>sensor-poetics-screenshot</Title>
        </Document>
        <Document ID="103">
            <Title>Difficulty of keeping track of what we create</Title>
            <Synopsis>Banks, 2011, p. 124</Synopsis>
            <Text>Banks (2011, pp. 124—136) thinks that the multiplication of online services is a good thing in general, allowing users to try several services until you try one that fits your need. However he notes that this is an issue for digital preservation as it scatters our data all over the network with few means to keep track of it:
While I played around on Instagram I posted a number of images to their site, including a few of my daughter. I have a few more images posted up on Twitpic.com, a service that you can use for sharing images when publishing to Twitter. I may never use either of these services again, yet they have both got content on them that is a part of my personal history. (Banks, 2011, p. 128)]
This problem is to be combined with the transience of such services (Twitpic, mentioned here, shut down in September 2014, after being around since 2008). As Banks reminds us, services that were highly popular a decade ago (MySpace, Geocities) have suffered from an “exodus” of their users towards new services, for new competitors that are seen as more attractive, or due to a decline in quality of the platforms (he cites MySpace becoming inundated with advertising as a possible reason). Such a loss of users would hit the company financially, and as such they may be forced to close down or reinvent their model, erasing their users’ data as a result.</Text>
            <Comments>http://blog.twitpic.com/2014/09/twitpic-is-shutting-down/ [Accessed 20th October 2014]
Bebo, which once was the most popular service in the UK, has shut down in 2013 and is reopening with different services, having completely erased the original website. http://bebo.com [Accessed 20th October 2014]</Comments>
        </Document>
        <Document ID="147">
            <Title>Cultural differences</Title>
            <Text>For example, Tversky et al. (1991) demonstrated that the origin of the direction can stem in the reading direction. When we read, the upcoming (future) information is on the right; we make progress by going right in a sentence or book. Speakers of languages that are read from right to left actually think of time going from right to left, the opposite direction of how Western European cultures see it. Participants were asked to put various sets of stickers in the chronological order (for example breakfast, lunch and dinner, or pictures from a person from younger to older); Arabic and Hebrew speakers ordered them from right to left, whereas English speakers did the reverse. In every day life, it is possible to see this difference, for example, in computer software; progress bars are “reversed” in these languages, showing that progress is thought of going from the right to the left (see fig. Timeline/progressBarRTL). 
[Fig. Timeline/progressBarWestern]
[Fig. Timeline/progressBarRTL]
But as Boroditsky (2010) explains, it is not simply the reading direction that influences our perception of time. The study has been done with the Pormpuraaw community of Australian aboriginals. Interestingly, their languages do not have words for left or right; instead they use absolute directions (North, South, West, East) for everything. They have a great knowledge of the world around them to know which way they are facing, and so they would say things like  “move your cup to the north-west a little bit”, or “the boy standing on the south of Mary is my brother”. And so when they have been asked to order stickers, instead of putting them from right to left in front of them, they lay the stickers down and have been ordering them from East to West — just like the sun, from morning to evening; proving that time can be visualised in other spatial models.  
Mandarin speakers tend to think of time vertically, more often than we do, even though modern Chinese is read horizontally. Their conception of movement is also different — they see a moving timeline and a static observer, whereas we tend to visualise ourselves moving in the static timeline (Santiago et al., 2007, p. 512). Speakers of the Aymara language, in South America, think of time from left to right, like Arabic speakers, even though the language reads right-to-left; this is because they see the past as being in front of them, something that they can look at, whereas the future is in their back, it is the unknown, what you cannot see. (Boroditsky, 2010) </Text>
            <Comments>Correct references - they are actually cross-referenced from other work in Santiago et al.’s paper (Times also flies from R to L) </Comments>
        </Document>
        <Document ID="38">
            <Title>LiteratureReviewLayout</Title>
        </Document>
        <Document ID="42">
            <Title>Structure</Title>
            <Text>STRUCTURE

Introduction
Why digital preservation
	Stats of numbers on digital vs physical
	Why digital degrades faster
		UNESCO’s Threats to digital continuity
		Fast decay of preservation medium
		Technological change of hardware and software
		Issues for long term storage
	Importance of data
		Library material: for society
		For businesses (records)
		Personal archiving / domestic archiving
		Preserving interactive material inc. video games
		Useful for researchers in the future
			Understanding our history
			Understanding the beginnings of the Internet, as we have shifted in an information society
			Preserving an accurate and integrity image of our society at a given point
				Twitter &amp; Usenet
	Advantages over physical preservation
		Mass preservation, easier access and organisation
		Recording digitally material that’s not born-digital
	Cost of restoration; case studies of lost data
		
Defining paradigms for digital preservation
	Pre-digital paradigms
	Inability to preserve the medium, only the bitstream
	Importance of metadata, relationship &amp; context
	Definition of terms		
		
What to preserve / Selectivity
	Harvey 5 pillars for selectivity
	Banks reasons for domestic archiving
	Technical possibility of archiving
	Pros/cons of archiving the entire internet
	Dangers of selection (Cook)
	Automatic selection (Reputation model)
	Crawling
	Manual curating
	Special Collections for events (eg before elections, see webarchive.org.uk for other ideas)
	Web archiving: what stands as heritage on the web

Who should preserve
	Legal issues
		(see later) special case for emulation
	Depends on the type of content
	Libraries
	National libraries
	Archivists
	New: how the creator/publisher is now fully responsible for it
	New stakeholders
	Profits vs nonprofits
	Personal archiving
		Yourself?
		Online services

How to preserve
	Preparation
	Software methods and choosing one
	+ Authenticity/integrity and issues with each method
	+ Emulation / legal rights with videogames
	Use of standards

Domestic archiving
    Archiving photos
    	What we store online
    		Personal media &amp; teenagers; privacy options
    		Data already public
    What happens after our deaths
    [Digital heritage]
    Shifts in usages (more photos)
    Advantages (sharing, organisation etc.); record part of our lives we never did before
    Keeping memories attached to digital objects
    or storing physical objects digitally
    -&gt; (larger scale?)


Conclusion</Text>
        </Document>
        <Document ID="39">
            <Title>LiteratureReviewLayout</Title>
            <Text>DJ32004 RESEARCH &amp; CREATIVE PRACTICE PART 1 LITERATURE REVIEW
DIGITAL PRESERVATION
Victor Loux April 2014
Digital Interaction Design University of Dundee
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼
Table of contents
2 Table of contents
2 List of figures
3 Introduction
4 The need for digital preservation
6 Threats to digital continuity
7 Who must preserve, and what we should keep
7 How to preserve
8 Methods for software preservation
9 Choosing a method
9 Personal archiving
12 Conclusion
13 Bibliography
17 Project Proposal Form
List of figures
1.1 and 1.2 Kjen Wilkens, Sensor Poetics / Weather Camera http://www.kjenwilkens.com/projects/sensor-poetics
2.1 Héctor Seranno, Back Up Objects
http://www.hectorserrano.com/index.php?id=41&amp;m=lab&amp;grupo=backup
￼￼2
Introduction
As we shift into the Information Age and a knowledge-based economy, a number of concerns are being raised over the long-term preservation of digital data and artifacts.
This literature review aims to get an overview of the current problems, techniques and prac- tices in preserving data on digital media, including the definition and understanding of the new paradigms created by digital preservation, the importance of being able to store things on the long term, the processes used to curate and select content to be preserved, and the var- ious methods used to do so. It also encompasses personal archiving which will be the focus of my own research.
3
The need for digital preservation
With an exponential growth in data being produced digitally (SINTEF, 2013), comes a signif- icant growth in the volume of data worth archiving over the long term (Harvey, 2012, p. 33). However, the preservation of digital materials cause much more issues than traditional pres- ervation paradigms; where we can consider that for paper and physical artefacts, “benign ne- glect may be the best treatment” (Harvey, 2012, p. 10; derived from Bastian, Cloonan and Har- vey, 1993; 2011), the rapid changes in technology and the fragility of digital media make this rule dangerous for the survival of data.
First, we should define what are archives, and what they are useful for. As Cook eloquently puts it:
“Archives [...] are a source of memory about the past, about history, heritage, and culture, about personal roots and family connections, about who we are as human beings and about glimpses into our common humanity through recorded information in all media, much more than they are about narrow accountabilities or administrative continuity.” (Cook, 2000)
It is certainly difficult to understand which data was already lost, and what is at stake now. Harvey emitted some criticism of pessimistic views for archiving:
“It was suggested in 2002 that the last 25 years have been a ‘scenario of data loss and poor records that has dogged our progress’ and that, if this is not reversed, ‘the human record of the early 21st century may be unreadable’ (Deegan and Tanner, 2002). Rhetoric of this kind is common in the literature, but is regrettably poorly supported with specifics and evidence. Alarmist descriptions abound: there will be ‘a digital black hole... truly a digital dark age from which information may never reappear’ (Deegan and Taylor, 2002) if we do not address the problem, and we will become an amnesiac society. [...] It is only possible to conclude, as the authors of the Digital Preservation Coalition’s handbook (2008, p. 32) did for the UK, that the evidence of data loss is ‘as yet only largely anecdotal ... [but] it is certain that many potentially valuable digital materials have already been lost’.” (Harvey, 2012, pp. 33–34)
4
While it is difficult to argue that preserving digital objects is necessary, there is certainly a need to make a selection of what deserves to be protected, and to what level. It is clear that the long term preservation of legal and governmental records, scientific publications (Harvey, 2012, p. 26) and business records (Ross and Gow, 1999, p. iii) on digital form is crucial; but the archival of the wider Internet, cultural artefacts and our personal heritage is subject to discus- sion, as fewer items could be considered valuable.
UNESCO note that those who value a more comprehensive collection argue that “any informa- tion may turn out to have long-term value, and that the costs of detailed selection are greater than the costs of collecting and storing everything” (UNESCO, 2003, p. 73). Additionally, digi- tal storage media degrade so rapidly that the content might disappear before a decision about its value has been made (UNESCO, 2003, p. 71; Harvey, 2012, p. 57). Proponents of a more selective collection argue that we could obtain a higher content quality, settle the preserva- tion rights with content producers, and that we do not have the physical resources to store everything anyway (UNESCO, 2003, pp. 70-73; Harvey, 2012, pp. 26, 58).
Both UNESCO (2003) and Cook (2000) recognise that a middle ground must be found, and that both approaches are valid depending on the goal of the preservation intent. To give an ex- ample in archiving the Web, the Internet Archive (Kahle, 1996) has chosen to store copies of as many websites as possible, but due to technical limitations discards all images, stylesheets and multimedia objects; evidently this is not appropriate for valuable websites. The British Library’s Web Archive 1 curates collections of websites by topic and by events (Queen Jubi- lee, London 2005 terrorist attacks, General Elections, Olympic and Paralympic Games, ...) to reflect the state and opinion of the Web at these given points in history; because fewer sites are archived and their content is deemed important, they have archived full quality versions of these sites. See Masanès (2006, p. 83) for a methodology for this type of selection. Cook reminds us however that archivists have a duty of impartiality and objectivity during the se- lection and must ensure all points of view are being kept, to ensure our shared memory does not “becomes counterfeit, or at least transformed into forgery, manipulation, or imagination” (Cook, 2000).
Hiroyuki Kawano also proposes to use reputation models to let crawling robots consider the
￼1 http://www.webarchive.org.uk/ukwa/ [Accessed: 11 Apr 2014]
￼5
“importance, fairness, trustiness, uniqueness and valuation” of a Web page (Kawano, 2008) based on numerous parameters, to decide whether a page is worth archiving. The automated nature of this selection can, by nature, be inaccurate and inadvertently subjective, therefore a manual selection is still sometimes required.
Threats to digital continuity
UNESCO (2003, pp. 30–31) has compiled a list of threats to what they call digital continuity. The two principal ones are the short lifetime of digital materials carriers (i.e. storage media), and the obsolescence of the means to access these digital materials (i.e. the software to read files and the hardware to read discontinued storage media). Research by the Digital Preserva- tion Coalition (2008, p. 154) show that the reliability of storage material varies highly depend- ing on the type of material and the storage conditions, going from a few months to decades or even centuries.
But the media are only part of the equation; because all storage media eventually become obsolete, we will not necessarily be able to read them again even if they are not deteriorat- ed. Weathley (2003, cited by Harvey, 2012, p. 37) denounced that myth of long-lived media: “[an IT vendor] offered us a special polymer that they guaranteed would preserve a CD-ROM for 100 years. They were unable to answer how we would preserve a CD-ROM player for that length of time.” And as Pearson adds, not only is the CD-ROM drive needed, but also the type of cable to plug that drive to the computer, a computer motherboard that has a connector for this cable, an operating system with drivers for these types of connectors, and a program able to open the file itself (Pearson, 2009, slide 11). All of these elements become rapidly obsolete themselves, which mean that media may not be easily accessible on current technology after just one or two decades, even if the storage media last longer than that. Therefore it is needed to periodically migrate (a process also called “refreshing”) the bitstreams from a media type to a more contemporary one in order to ensure that this bitstream will remain easily accessi- ble (Harvey, 2012). As the bitstream is preserved, only the question of preserving software to read it is left.
6
Who must preserve, and what we should keep
Digital media also changes the original preservation paradigm for deciding who takes the re- sponsibility for archiving content, and how. Publishing processes used to be distinct in the “analogue” era. But as Nurnberg (1995, p. 21, cited in Harvey, 2012, p. 9) said, “technology tends to erase distinctions between the separate processes of creation, reproduction and dis- tribution that characterise the classic industrial model of print commodities”. Self-publication (particularly on websites) is much more common, and for many publishers the task of preser- vation is new to them (Harvey, 2012, p. 32); they might not have the awareness, knowledge or resources to do so themselves it is therefore essential that they collaborate with preservation organisations, such as libraries and archives (Ayre and Muir, 2004). See Harvey (2012, chapter 9), Ayre and Muir (2004) and UNESCO (2003, chapter 11) for more resources and case studies.
Ayre and Muir (2004) suggest that the complexity and resources needed for digital preser- vation may mean that individual libraries may not have the possibility to do the preservation themselves like they used to, but instead rely on national libraries and centralised preserva- tion systems to reduce overhead. This system is used by a number of libraries; even here at the University of Dundee, most journals and a selection of books are available through centralised systems (Dawsonera, Ebrary) or publishers directly (ACM, Elsevier Science, Springer).
How to preserve
There are multiple methods for preservation. Each has advantages and issues, although one parameter is particularly influential: authenticity. This refers to the faithfulness of the rendi- tion of the preserved object in the future, compared to when it was created; while the content should not be altered, its presentation may change (Cook, 2000). What is considered appro- priately authentic is subject to debate (del Pozo et al., 2012, p. 7; Cook, 2000) and may vary depending on the preservation intent; del Pozo et al. (2012, p. 8) give, as example, the nor- malisation of a spreadsheet to formats that may accurately retain either the formatting, or the formulae used in each cell.
7
Retaining integrity, that is ensuring the object is “what it purports to be” and is “complete and has not been altered or corrupted” (Ross, 2002, p. 7, cited in Harvey, 2012, p. 54), can also be crucially important, particularly for proving that a business or legal document has not been tampered with (Harvey, 2012, p. 54). But as Lynch (2000), UNESCO (2003, p. 22) and Harvey (2012, p. 54) argue, integrity is mostly a matter of trust and can only be ensured with a thor- ough documentation of the archiving process.
Methods for software preservation
[NB: due to the length limit of this literature review, I only give a simplified overview of the main archiving techniques. See Granger (2000), Thibodeau (2002) and Harvey (2012) for a comprehensive list of methods, including configurable chips, persistent archives, object inter- change format, etc.]
• Hard copies: print documents to archive them. Rarely considered a viable option for large data sets and interactive documents (Granger, 2000; Rothenberg, 1998).
• Computer Museums: Swade (1993) proposed to preserve hardware and software central- ised places. Considered short-term and prohibitively expensive by Granger (2000) but can be useful to recover old media or testing emulators (Rothenberg, 1998).
• Migration: the most popular option, consisting of periodically saving files to a current file type (newer version of a format, different format, or a standardised format) (Granger, 2000; Thibodeau, 2002) to avoid technological obsolescence. It is labour intensive and risky, as it compromises authenticity (Rothenberg, 1998).
• Universal Virtual Computer (UVC): proposed by Lorie (2000, in Thibodeau, 2002, p. 22). It consists of implementing a format viewer in a portable programming language like Java 2 which could then theoretically run on any future platform supporting that pro- gramming language. It is however slow and functionally limited (Lorie 2000), and runs the risk of the portable language itself becoming obsolete.
• Emulation is the most conservative model; it simulates a legacy architec-
2 Most programming languages include instructions that are specific to a given platform, therefore the same file will need to be adapted to run on another platform. “Portable” languages like Java are called high-level and are more abstract. They are run inside a virtual machine (itself low-level, different for every platform) which worries about transforming Java code into instructions that are specific to that platform; as such, the same file of code can be ran on any platform.
￼8
ture on current platforms, allowing to run the original software used to create a file, therefore not modifying them. It “keeps the look and feel as well as the in- teractivity” (Granger, 2000) therefore is highly relevant for video games (Gut- tenbrunner et al., 2010) and interactive systems, and it guarantees authenticity. It is however highly complex to implement (Granger, 2000; byuu, 2011) and need renew- al, since emulators themselves become obsolete (Thibodeau, 2002), as exemplified by em- ulators provided by Apple for their architecture switches (Mesa, A. F., 1997; AppleInsider, 2011) that were later discontinued, effectively removing the ability to run pre-2005 appli- cations on their current models.
Choosing a method
Thibodeau (2002, pp. 15–16) suggests four criteria to choose a preservation method. It must be:
• technically feasible
• sustainable and resistant to technological obsolescence
• practical (the preserving organisation must have the resources to do it)
• appropriate to the type of material being preserved, and the objectives of the preservation
(see del Pozo et al., 2012, p. 7 and Granger, 2000 for examples).
Personal archiving
It is interesting to ask ourselves if the paradigms for digital preservation used by librarians and archivists are still valid for preservation of our personal data. Harvey (2012) and Lukesh (1999, cited in Harvey, 2012, p. 32) note that our personal correspondence and sentimental ar- tefacts stored digitally might be lost if we do not take steps to preserve them, and wonder how we will be able to understand our modern life without these exchanges.
While this is a correct idea, I think that few people kept their letters in the sole purpose of leav- ing researchers decades from now understand our epoch’s social habits and ways of speaking. There has to be other motivations for preserving our conversations and our objects.
9
Richard Banks, a researcher at Microsoft Cambridge, suggests that we first preserve artefacts for ourselves. The majority of the time, we simply keep what we consider significant to re- mind ourselves of particular people, events and places, and to share them with people we care about; we discard what is not meaningful to us by pragmatism (lack of physical space, lack of importance, redundancy) (Banks, 2011, p. 6). This selection process contributes to keeping what we want to remember, and what is the legacy we will want to leave to others after our deaths.
However, the digital age changes this situation. First, the space to store our objects on com- puters and online is becoming virtually unlimited, and so we tend to accumulate data instead of selecting (Banks, 2011); second, the abundance of online services makes it harder to keep track of everything we create. Fortunately, digital objects have fundamental properties that physical artefacts don’t have, which actually provide numerous advantages in terms of pres- ervation: the ease of organising a collection of data through indexing, cataloguing and adding metadata (Kirk and Sellen, 2010, p. 35), and the ability to duplicate and share that object accu- rately and virtually cost-free (Banks, 2011).
The cataloguing advantage allows us to process it using computer science. Hangal et al. (2011) have developed MUSE (Memories USing Email), a system finding patterns in email archives to help us make sense of this large data set. As the authors note in the introduction, “email has become a de facto medium of record; many people consciously deposit important information into email, knowing they can look it up later”, and so our inboxes contain highly valuable in- formation.
Early users reported that they have been using it to: make a summary of their work progress; extract and organise certain type of data (for instance, the personal out of the professional, or the important out of the more mundane); finding life milestones inside family emails; picking up work that has been left unfinished and forgotten; or renewing with old relationships. These uses went beyond the authors’ original expectations of simply reminiscing:
“The stories above include an example of each one of the ‘5R’s’ described by Sellen and Whittaker [2010]: recollection, reminiscing, retrieving, reflecting, and remembering in- tentions. Further, it suggests that browsing and remembering the past can affect the fu- ture.” (Hangal et al., 2011)
10
The copying advantage, while useful for sharing, has issues for backing up. Banks (2011) notes the process is still troublesome, and still feels insecure because of cheap looking media, and nowadays, the abstraction with cloud storage (about Flickr: “I have no idea what kind of hard- ware my files are now stored on or even where they are geographically. I just expect to have access to them as long as I pay my bills” Banks, 2011, p. 27).
Kirk and Sellen (2010) found however that ultimately, stories and narrative are more impor- tant than the preservation of the artefact itself. They propose that “technology might play a role in capturing and associating stories or narratives with different physical objects”, imagin- ing we could also digitally augment physical objects or places. Banks (2011) gives as example the Weather Camera (Wilkens, 2011), which records wind force and temperature along with commentary; a system of physicals backup of sentimental items by Serrano (2009) using 3D scanning and 3D printing; and tools by Microsoft Research to recreate an environment in 3D just using photos. A similar technique is used in bigger preservation projects to make high quality 3D models of culturally important places and objects in China (Zhou et al., 2012).
Fig. 1.1: The Weather Camera, by Jens Wilkens. Fig. 1.2: Associating stories and weather to a timeline with the Weather Camera.
Fig. 2: backing up physical sentimental items using a 3D scanner and 3D printing, 11 with Back Up Objects by Héctor Serrano
￼￼￼
Conclusion
This literature review, although shallow considering the breadth of the topic, allowed me to gain an understanding of the current practices and debates in the digital archiving communi- ty, and to gather relevant sources for any future research in each sub-topic. I found the theme of personal archiving captivating but not explored as intensely as large scale preservation for our common knowledge and data; I would like to do deeper research in this particular area of digital preservation, notably on our use of social media and Internet services on the long term.
12
Bibliography
AppleInsider. (2011) ‘Inside Mac OS X 10.7 Lion: Missing Front Row, Rosetta and Java runtime’. AppleInsider. [web page] Accessible at http://appleinsider.com/articles/11/02/26/ mac_os_x_lion_drops_front_row_java_runtime_rosetta.html [Accessed: 19th Apr 2014]
Ayre, C. and Muir, A. (2004) ‘The right to preserve’. D-Lib Magazine, vol. 10 (No. 3). March. Available at http://www.dlib.org/dlib/march04/ayre/03ayre.html [Accessed: 11th Apr 2014]
Banks, R. (2011) The Future of Looking Back. Sebastopol: Microsoft Press.
Bearman, D. (1999) ‘Reality and Chimeras in the Preservation of Electronic Records’. D-Lib Magazine, vol. 5 (No. 4). April. Available at http://www.dlib.org/dlib/april99/ bearman/04bearman.html [Accessed: 7th Apr 2014]
Boyd, D. (2014) It’s Complicated: The Social Lives of Networked Teens. New Haven and London: Yale University Press.
byuu.1 (2011) ‘Accuracy takes power: one man’s 3 GHz quest to build a perfect SNES emulator’. Ars Technica. [web page] Available at: http://arstechnica.com/gaming/2011/08/ accuracy-takes-power-one-mans-3ghz-quest-to-build-a-perfect-snes-emulator/ [Accessed: 8th Apr 2014]
Cook, T. (2000) ‘Beyond the screen: the records continuum and archival cultural heritage.’ Presented at the Australian Society of Archivists Conference, Melbourne, 18 August. Available at http://www.mybestdocs.com/cook-t-beyondthescreen-000818.htm [Accessed: 9th Apr 2014]
Granger, S. (2000) ‘Emulation as a Digital Preservation Strategy’. D-Lib Magazine, vol. 6 (No. 10). October. Available at http://www.dlib.org/dlib/october00/granger/10granger.html [Accessed: 7th Apr 2014]
Guttenbrunner, M., Becker, C. and Rauber, A. (2010) ‘Keeping the game alive: Evaluating strategies for the preservation of console video games’. International Journal of Digital Curation, vol. 5 (Iss. 1), pp. 64-90.
￼￼￼￼￼￼￼￼￼￼1 The article is only credited to the developer’s pseudonym.
13
Bibliography (cont.)
Hangal, S., Lam, M. S. and Heer, J. (2011) ‘MUSE: reviving memories using email archives’, in
Proceedings of the 24th annual ACM symposium on User interface software and technology (UIST ‘11). New York, NY: ACM, pp. 75-84. doi:10.1145/2047196.2047206
Harvey, D. R. (2012) Preserving digital materials. Berlin: De Gruyter Saur.
Kahle, B. (1996) ‘Archiving the Internet’. Scientific American, Iss. March 1997. Available at http://web.archive.org/web/19971211123138/www.archive.org/sciam_article.html [Accessed: 27th Feb 2014]
Kawano, H. (2008) ‘Strategy of Digital Contents Archive Based on Reputation Model’, in ICSENG ‘08. Proceedings of 19th International Conference on Systems Engineering, 19-21 Aug, pp. 288-293. doi: 10.1109/ICSEng.2008.75
Kirk, D. S. and Sellen, A. (2010) ‘On human remains: Values and practice in the home archiving of cherished objects’. ACM Transactions on Human-Computer Interaction, vol. 17 (Iss. 3), article 10. doi:0.1145/1806923.1806924
Library of Congress. (2007) Digital Preservation Program Makes Awards to Preserve American Creative Works. [press release] Available at http://www.loc.gov/today/ pr/2007/07-156.html [Accessed: 14th Apr 2014]
Lubar, S. (1999) ‘Information Culture and the Archival Record’, in American Archivist, vol. 62 (Iss. 1), pp. 10-22.
Lynch, C. (2000) “Authenticity and Integrity in the Digital Environment: An Exploratory Analysis of the Central Role of Trust’, in Authenticity in a Digital Environment. Washington, DC: Council on Library and Information Resources. Available at http://www. clir.org/pubs/abstract/pub92abst.html [Accessed: 16th Apr 2014]
Masanès, J. (2006) Web archiving. Berlin: Springer.
Mesa, A. F. (1997) ‘The PowerPC Triumph’. The Apple Museum. [web page] Available at: http://applemuseum.bott.org/sections/ppc.html [Accessed: 19th Apr 2014]
￼￼￼￼￼￼￼￼￼14
Bibliography (cont.)
Paloque-Bergès, C. (2011) Entre trivialité et culture : une histoire de l’Internet vernaculaire. Émergence et médiations d’un folklore de réseau [Between Triviality and Culture: a History of the Vernacular Internet. Emergence and mediations in network folklore]. PhD thesis, self-published. Available at http://camillepaloqueberges.wordpress.com/phd-thesis/ [Accessed: 6th Apr 2014] 2
Paloque-Bergès, C. (2013) ‘Un patrimoine composite : le public Internet face à l’archivage
de sa matière culturelle’ [A composite heritage: the Internet public confronted to the archiving of its cultural matter], in I. Dragan, P. Stefanescu, N. Pelissier, J.-F. Tétu and
L. Idjeroui-Ravez (eds.) Traces, mémoire, communication. Bucharest: Editura Universităţii din Bucureşti. 2
Pearson, D. (2009) Preserve or Preserve Not, There is No Try: some dilemmas relating
to Personal Digital Archiving. [PowerPoint slides]. National Library of Australia Staff Papers. Available at http://www.nla.gov.au/openpublish/index.php/nlasp/article/view/1388 [Accessed: 9th Apr 2014]
del Pozo, N., Long, A. S. and Pearson, D. (2012) ‘Land of the Lost: a discussion of what can be preserved through digital preservation’. Library Hi Tech, vol. 28 (Iss. 2), pp. 290-300. Available from: doi: 10.1108/07378831011047686. [Accessed: 6th Apr 2014]
Ross, S. and Gow, A. (1999) Digital archaeology: rescuing neglected and damaged data resources: a JISC/NPO study within Electronic Libraries (eLib) Programme on the Preservation of Electronic Materials. London: Library Information Technology Centre. Available at http://www.ukoln.ac.uk/services/elib/papers/supporting/pdf/p2.pdf [Accessed: 17th Apr 2014]
Rothenberg, J. (1998) Avoiding Technological Quicksand: Finding a Viable Technical Foundation for Digital Preservation. [report] Washington, DC: Council on Library and Information Resources.
￼￼￼￼￼2 In French.
15
Bibliography (cont.)
Serrano, H. (2009) Back Up Objects, Héctor Serrano. [web page] Available at: http://www. hectorserrano.com/index.php?id=41&amp;m=lab&amp;grupo=backup [Accessed: 17th Apr 2014].
SINTEF. (2013) ‘Big Data, for better or worse: 90% of world’s data generated over last two years.’ ScienceDaily. [web page] Available at: http://www.sciencedaily.com/ releases/2013/05/130522085217.htm [Accessed: 16th Apr 2014]
Swade, D. (1993) ‘The problems of software conservation”, in Computer Resurrection, vol. 7. Available at: http://www.cs.man.ac.uk/CCS/res/res07.htm#f [Accessed: 12th Apr 2014]
Task Force on Archiving of Digital Information. (1996) Preserving digital information, Report of the Task Force on Archiving of Digital Information. Washington, DC: Council on Library and Information Resources.
Thibodeau, K. (2002) ‘Overview of technological approaches to digital preservation
and challenges in coming years’, in The state of digital preservation: an international perspective. Washington, DC: Council on Library and Information Resources, pp. 4-31.
UNESCO. (2003) Guidelines for the preservation of digital heritage, prepared by the National Library of Australia. Paris: UNESCO. Available at http://unesdoc.unesco.org/ images/0013/001300/130071e.pdf [Accessed: 9 Apr 2014]
Wilkens, K. (2011) Sensor Poetics, Kjenwilkens.com. [web page] Available at http://www. kjenwilkens.com/projects/sensor-poetics [Accessed: 18th Apr 2014]
Zimmer, M. (2010) ‘“But the data is already public”: on the ethics of research in Facebook’, in Ethics and information technology, vol. 12 (Iss. 4), pp. 313-325. doi: 10.1007/s10676-010- 9227-5
Zhou, M., Geng, G. and Wu, Z. (2012) Digital preservation technology for cultural heritage. Berlin: Springer.
￼￼￼￼￼￼￼￼￼￼￼￼16
Project Proposal Form
￼￼Name Programme/Course Project/working title
Project Aims (i.e. what you want to learn about through further investigation. E.g. I want toa learn more about the creative process of animators)
Objectives (i.e. the particular things you propose to do
in order to carry out your investigation. E.g. Make contact with professional animators and arrange
to meet them. Possibly Interview them or observe their creative activities to gather data)
Methods (i.e. the different ways in which you will achieve these objectives. E.g. Open ended interview techniques and grounded theory analysis or video observation of working animators)
Victor Loux
Digital Interaction Design
Digital preservation for the masses
I would like to look at further research papers on personal preservation and focus on ways to apply “large-scale” paradigms to our personal heritage while trying to be respectful of people’s privacy and unintrusive.
I first intend to do further reading in this area, as until now I have been exploring digital preservation as a whole with no particular focus, to get a broader understanding of the issues
and solutions. I would like to interview several people of different age ranges to understand their relationship to technology and their personal digital heritage, notably concerning photographs and conversations.
Using open ended interview techniques, further literature review and possibly research through practice as I intend to explore this theme for my final year project.
￼￼￼￼￼￼￼￼￼￼￼￼17
</Text>
        </Document>
        <Document ID="43">
            <Title>weather-camera-wilkins</Title>
        </Document>
        <Document ID="104">
            <Title>Finding patterns to make sense</Title>
            <Synopsis>Hangal et al. (2011) have developed MUSE (Memories USing Email), a system finding patterns in email archives to help us make sense of this large data set. As the authors note in the introduction, “email has become a de facto medium of record; many people consciously deposit important information into email, knowing they can look it up later”, and so our inboxes contain highly valuable in- formation.
Early users reported that they have been using it to: make a summary of their work progress; extract and organise certain type of data (for instance, the personal out of the professional, or the important out of the more mundane); finding life milestones inside family emails; picking up work that has been left unfinished and forgotten; or renewing with old relationships. These uses went beyond the authors’ original expectations of simply reminiscing:
“The stories above include an example of each one of the ‘5R’s’ described by Sellen and Whittaker [2010]: recollection, reminiscing, retrieving, reflecting, and remembering in- tentions. Further, it suggests that browsing and remembering the past can affect the fu- ture.” (Hangal et al., 2011)</Synopsis>
        </Document>
        <Document ID="148">
            <Title>Circular representation</Title>
            <Text>[Fig. Timelines/Ouroboros]
This linear timeline, however, is not the only way of representing time; it can also be seen as a recurring cycle. On our day-to-day life, we mostly use this representation for analog clock dials; this is borrowed from sundials, and makes sense when we think of the rotation of the Earth. It really is a continuous cycle, which doesn’t really start nor end happening, but that we can still quantify — a day has passed when a full rotation has been done, and we can map time units to it, from midnight to 11:59pm. This representation, however, is rarely used nowadays when talking about months or years, even though the same astronomical principles apply. Would it however be relevant for representing history, or individual lives?
The symbol of the Ouroboros, a snake or a dragon eating its tail, is extremely ancient; its first known representation can be found in the Enigmatic Book of the Netherworld, a funerary text written in the 14th century BC in ancient Egypt (Hornung, 1999). It has however been used by many cultures across the globes, in different epochs and contexts (mythological, religious, alchemical) and for various purposes. In many cases, for example Mayan and Pagan traditions, life itself was represented as an ouroboros; birth wasn’t seen as a starting point, and death not as an end point, but it was rather a continuation of someone else’s life, with each cycle being an incarnation. Again, languages and expressions reflect these ways of visualising time spans or lives; in Balinese, the word for grandparent and grandchildren is the same (kumpi), as they would be the same person ultimately.</Text>
        </Document>
        <Document ID="3">
            <Title>Title</Title>
            <Text>Victor Loux / 110017909
Digital Interaction Design
Research &amp; Creative Pratice
Dissertation
 Long-term backups and retrieval of digital memories posted on social networks
</Text>
        </Document>
        <Document ID="44">
            <Title>README</Title>
            <Text>dissertation
============

Archiving the Internet, and Digital Preservation for the masses
</Text>
        </Document>
        <Document ID="4">
            <Title>Context</Title>
        </Document>
        <Document ID="45">
            <Title>LiteratureReview-light</Title>
            <Text>DJ32004 RESEARCH &amp; CREATIVE PRACTICE PART 1 LITERATURE REVIEW
DIGITAL PRESERVATION
Victor Loux April 2014
Digital Interaction Design University of Dundee
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼
Table of contents
2 Table of contents
2 List of figures
3 Introduction
4 The need for digital preservation
6 Threats to digital continuity
7 Who must preserve, and what we should keep
7 How to preserve
8 Methods for software preservation
9 Choosing a method
9 Personal archiving
12 Conclusion
13 Bibliography
17 Project Proposal Form
List of figures
1.1 and 1.2 Kjen Wilkens, Sensor Poetics / Weather Camera http://www.kjenwilkens.com/projects/sensor-poetics
2.1 Héctor Seranno, Back Up Objects
http://www.hectorserrano.com/index.php?id=41&amp;m=lab&amp;grupo=backup
￼￼2
Introduction
As we shift into the Information Age and a knowledge-based economy, a number of concerns are being raised over the long-term preservation of digital data and artifacts.
This literature review aims to get an overview of the current problems, techniques and prac- tices in preserving data on digital media, including the definition and understanding of the new paradigms created by digital preservation, the importance of being able to store things on the long term, the processes used to curate and select content to be preserved, and the var- ious methods used to do so. It also encompasses personal archiving which will be the focus of my own research.
3
The need for digital preservation
With an exponential growth in data being produced digitally (SINTEF, 2013), comes a signif- icant growth in the volume of data worth archiving over the long term (Harvey, 2012, p. 33). However, the preservation of digital materials cause much more issues than traditional pres- ervation paradigms; where we can consider that for paper and physical artefacts, “benign ne- glect may be the best treatment” (Harvey, 2012, p. 10; derived from Bastian, Cloonan and Har- vey, 1993; 2011), the rapid changes in technology and the fragility of digital media make this rule dangerous for the survival of data.
First, we should define what are archives, and what they are useful for. As Cook eloquently puts it:
“Archives [...] are a source of memory about the past, about history, heritage, and culture, about personal roots and family connections, about who we are as human beings and about glimpses into our common humanity through recorded information in all media, much more than they are about narrow accountabilities or administrative continuity.” (Cook, 2000)
It is certainly difficult to understand which data was already lost, and what is at stake now. Harvey emitted some criticism of pessimistic views for archiving:
“It was suggested in 2002 that the last 25 years have been a ‘scenario of data loss and poor records that has dogged our progress’ and that, if this is not reversed, ‘the human record of the early 21st century may be unreadable’ (Deegan and Tanner, 2002). Rhetoric of this kind is common in the literature, but is regrettably poorly supported with specifics and evidence. Alarmist descriptions abound: there will be ‘a digital black hole... truly a digital dark age from which information may never reappear’ (Deegan and Taylor, 2002) if we do not address the problem, and we will become an amnesiac society. [...] It is only possible to conclude, as the authors of the Digital Preservation Coalition’s handbook (2008, p. 32) did for the UK, that the evidence of data loss is ‘as yet only largely anecdotal ... [but] it is certain that many potentially valuable digital materials have already been lost’.” (Harvey, 2012, pp. 33–34)
4
While it is difficult to argue that preserving digital objects is necessary, there is certainly a need to make a selection of what deserves to be protected, and to what level. It is clear that the long term preservation of legal and governmental records, scientific publications (Harvey, 2012, p. 26) and business records (Ross and Gow, 1999, p. iii) on digital form is crucial; but the archival of the wider Internet, cultural artefacts and our personal heritage is subject to discus- sion, as fewer items could be considered valuable.
UNESCO note that those who value a more comprehensive collection argue that “any informa- tion may turn out to have long-term value, and that the costs of detailed selection are greater than the costs of collecting and storing everything” (UNESCO, 2003, p. 73). Additionally, digi- tal storage media degrade so rapidly that the content might disappear before a decision about its value has been made (UNESCO, 2003, p. 71; Harvey, 2012, p. 57). Proponents of a more selective collection argue that we could obtain a higher content quality, settle the preserva- tion rights with content producers, and that we do not have the physical resources to store everything anyway (UNESCO, 2003, pp. 70-73; Harvey, 2012, pp. 26, 58).
Both UNESCO (2003) and Cook (2000) recognise that a middle ground must be found, and that both approaches are valid depending on the goal of the preservation intent. To give an ex- ample in archiving the Web, the Internet Archive (Kahle, 1996) has chosen to store copies of as many websites as possible, but due to technical limitations discards all images, stylesheets and multimedia objects; evidently this is not appropriate for valuable websites. The British Library’s Web Archive 1 curates collections of websites by topic and by events (Queen Jubi- lee, London 2005 terrorist attacks, General Elections, Olympic and Paralympic Games, ...) to reflect the state and opinion of the Web at these given points in history; because fewer sites are archived and their content is deemed important, they have archived full quality versions of these sites. See Masanès (2006, p. 83) for a methodology for this type of selection. Cook reminds us however that archivists have a duty of impartiality and objectivity during the se- lection and must ensure all points of view are being kept, to ensure our shared memory does not “becomes counterfeit, or at least transformed into forgery, manipulation, or imagination” (Cook, 2000).
Hiroyuki Kawano also proposes to use reputation models to let crawling robots consider the
￼1 http://www.webarchive.org.uk/ukwa/ [Accessed: 11 Apr 2014]
￼5
“importance, fairness, trustiness, uniqueness and valuation” of a Web page (Kawano, 2008) based on numerous parameters, to decide whether a page is worth archiving. The automated nature of this selection can, by nature, be inaccurate and inadvertently subjective, therefore a manual selection is still sometimes required.
Threats to digital continuity
UNESCO (2003, pp. 30–31) has compiled a list of threats to what they call digital continuity. The two principal ones are the short lifetime of digital materials carriers (i.e. storage media), and the obsolescence of the means to access these digital materials (i.e. the software to read files and the hardware to read discontinued storage media). Research by the Digital Preserva- tion Coalition (2008, p. 154) show that the reliability of storage material varies highly depend- ing on the type of material and the storage conditions, going from a few months to decades or even centuries.
But the media are only part of the equation; because all storage media eventually become obsolete, we will not necessarily be able to read them again even if they are not deteriorat- ed. Weathley (2003, cited by Harvey, 2012, p. 37) denounced that myth of long-lived media: “[an IT vendor] offered us a special polymer that they guaranteed would preserve a CD-ROM for 100 years. They were unable to answer how we would preserve a CD-ROM player for that length of time.” And as Pearson adds, not only is the CD-ROM drive needed, but also the type of cable to plug that drive to the computer, a computer motherboard that has a connector for this cable, an operating system with drivers for these types of connectors, and a program able to open the file itself (Pearson, 2009, slide 11). All of these elements become rapidly obsolete themselves, which mean that media may not be easily accessible on current technology after just one or two decades, even if the storage media last longer than that. Therefore it is needed to periodically migrate (a process also called “refreshing”) the bitstreams from a media type to a more contemporary one in order to ensure that this bitstream will remain easily accessi- ble (Harvey, 2012). As the bitstream is preserved, only the question of preserving software to read it is left.
6
Who must preserve, and what we should keep
Digital media also changes the original preservation paradigm for deciding who takes the re- sponsibility for archiving content, and how. Publishing processes used to be distinct in the “analogue” era. But as Nurnberg (1995, p. 21, cited in Harvey, 2012, p. 9) said, “technology tends to erase distinctions between the separate processes of creation, reproduction and dis- tribution that characterise the classic industrial model of print commodities”. Self-publication (particularly on websites) is much more common, and for many publishers the task of preser- vation is new to them (Harvey, 2012, p. 32); they might not have the awareness, knowledge or resources to do so themselves it is therefore essential that they collaborate with preservation organisations, such as libraries and archives (Ayre and Muir, 2004). See Harvey (2012, chapter 9), Ayre and Muir (2004) and UNESCO (2003, chapter 11) for more resources and case studies.
Ayre and Muir (2004) suggest that the complexity and resources needed for digital preser- vation may mean that individual libraries may not have the possibility to do the preservation themselves like they used to, but instead rely on national libraries and centralised preserva- tion systems to reduce overhead. This system is used by a number of libraries; even here at the University of Dundee, most journals and a selection of books are available through centralised systems (Dawsonera, Ebrary) or publishers directly (ACM, Elsevier Science, Springer).
How to preserve
There are multiple methods for preservation. Each has advantages and issues, although one parameter is particularly influential: authenticity. This refers to the faithfulness of the rendi- tion of the preserved object in the future, compared to when it was created; while the content should not be altered, its presentation may change (Cook, 2000). What is considered appro- priately authentic is subject to debate (del Pozo et al., 2012, p. 7; Cook, 2000) and may vary depending on the preservation intent; del Pozo et al. (2012, p. 8) give, as example, the nor- malisation of a spreadsheet to formats that may accurately retain either the formatting, or the formulae used in each cell.
7
Retaining integrity, that is ensuring the object is “what it purports to be” and is “complete and has not been altered or corrupted” (Ross, 2002, p. 7, cited in Harvey, 2012, p. 54), can also be crucially important, particularly for proving that a business or legal document has not been tampered with (Harvey, 2012, p. 54). But as Lynch (2000), UNESCO (2003, p. 22) and Harvey (2012, p. 54) argue, integrity is mostly a matter of trust and can only be ensured with a thor- ough documentation of the archiving process.
Methods for software preservation
[NB: due to the length limit of this literature review, I only give a simplified overview of the main archiving techniques. See Granger (2000), Thibodeau (2002) and Harvey (2012) for a comprehensive list of methods, including configurable chips, persistent archives, object inter- change format, etc.]
• Hard copies: print documents to archive them. Rarely considered a viable option for large data sets and interactive documents (Granger, 2000; Rothenberg, 1998).
• Computer Museums: Swade (1993) proposed to preserve hardware and software central- ised places. Considered short-term and prohibitively expensive by Granger (2000) but can be useful to recover old media or testing emulators (Rothenberg, 1998).
• Migration: the most popular option, consisting of periodically saving files to a current file type (newer version of a format, different format, or a standardised format) (Granger, 2000; Thibodeau, 2002) to avoid technological obsolescence. It is labour intensive and risky, as it compromises authenticity (Rothenberg, 1998).
• Universal Virtual Computer (UVC): proposed by Lorie (2000, in Thibodeau, 2002, p. 22). It consists of implementing a format viewer in a portable programming language like Java 2 which could then theoretically run on any future platform supporting that pro- gramming language. It is however slow and functionally limited (Lorie 2000), and runs the risk of the portable language itself becoming obsolete.
• Emulation is the most conservative model; it simulates a legacy architec-
2 Most programming languages include instructions that are specific to a given platform, therefore the same file will need to be adapted to run on another platform. “Portable” languages like Java are called high-level and are more abstract. They are run inside a virtual machine (itself low-level, different for every platform) which worries about transforming Java code into instructions that are specific to that platform; as such, the same file of code can be ran on any platform.
￼8
ture on current platforms, allowing to run the original software used to create a file, therefore not modifying them. It “keeps the look and feel as well as the in- teractivity” (Granger, 2000) therefore is highly relevant for video games (Gut- tenbrunner et al., 2010) and interactive systems, and it guarantees authenticity. It is however highly complex to implement (Granger, 2000; byuu, 2011) and need renew- al, since emulators themselves become obsolete (Thibodeau, 2002), as exemplified by em- ulators provided by Apple for their architecture switches (Mesa, A. F., 1997; AppleInsider, 2011) that were later discontinued, effectively removing the ability to run pre-2005 appli- cations on their current models.
Choosing a method
Thibodeau (2002, pp. 15–16) suggests four criteria to choose a preservation method. It must be:
• technically feasible
• sustainable and resistant to technological obsolescence
• practical (the preserving organisation must have the resources to do it)
• appropriate to the type of material being preserved, and the objectives of the preservation
(see del Pozo et al., 2012, p. 7 and Granger, 2000 for examples).
Personal archiving
It is interesting to ask ourselves if the paradigms for digital preservation used by librarians and archivists are still valid for preservation of our personal data. Harvey (2012) and Lukesh (1999, cited in Harvey, 2012, p. 32) note that our personal correspondence and sentimental ar- tefacts stored digitally might be lost if we do not take steps to preserve them, and wonder how we will be able to understand our modern life without these exchanges.
While this is a correct idea, I think that few people kept their letters in the sole purpose of leav- ing researchers decades from now understand our epoch’s social habits and ways of speaking. There has to be other motivations for preserving our conversations and our objects.
9
Richard Banks, a researcher at Microsoft Cambridge, suggests that we first preserve artefacts for ourselves. The majority of the time, we simply keep what we consider significant to re- mind ourselves of particular people, events and places, and to share them with people we care about; we discard what is not meaningful to us by pragmatism (lack of physical space, lack of importance, redundancy) (Banks, 2011, p. 6). This selection process contributes to keeping what we want to remember, and what is the legacy we will want to leave to others after our deaths.
However, the digital age changes this situation. First, the space to store our objects on com- puters and online is becoming virtually unlimited, and so we tend to accumulate data instead of selecting (Banks, 2011); second, the abundance of online services makes it harder to keep track of everything we create. Fortunately, digital objects have fundamental properties that physical artefacts don’t have, which actually provide numerous advantages in terms of pres- ervation: the ease of organising a collection of data through indexing, cataloguing and adding metadata (Kirk and Sellen, 2010, p. 35), and the ability to duplicate and share that object accu- rately and virtually cost-free (Banks, 2011).
The cataloguing advantage allows us to process it using computer science. Hangal et al. (2011) have developed MUSE (Memories USing Email), a system finding patterns in email archives to help us make sense of this large data set. As the authors note in the introduction, “email has become a de facto medium of record; many people consciously deposit important information into email, knowing they can look it up later”, and so our inboxes contain highly valuable in- formation.
Early users reported that they have been using it to: make a summary of their work progress; extract and organise certain type of data (for instance, the personal out of the professional, or the important out of the more mundane); finding life milestones inside family emails; picking up work that has been left unfinished and forgotten; or renewing with old relationships. These uses went beyond the authors’ original expectations of simply reminiscing:
“The stories above include an example of each one of the ‘5R’s’ described by Sellen and Whittaker [2010]: recollection, reminiscing, retrieving, reflecting, and remembering in- tentions. Further, it suggests that browsing and remembering the past can affect the fu- ture.” (Hangal et al., 2011)
10
The copying advantage, while useful for sharing, has issues for backing up. Banks (2011) notes the process is still troublesome, and still feels insecure because of cheap looking media, and nowadays, the abstraction with cloud storage (about Flickr: “I have no idea what kind of hard- ware my files are now stored on or even where they are geographically. I just expect to have access to them as long as I pay my bills” Banks, 2011, p. 27).
Kirk and Sellen (2010) found however that ultimately, stories and narrative are more impor- tant than the preservation of the artefact itself. They propose that “technology might play a role in capturing and associating stories or narratives with different physical objects”, imagin- ing we could also digitally augment physical objects or places. Banks (2011) gives as example the Weather Camera (Wilkens, 2011), which records wind force and temperature along with commentary; a system of physicals backup of sentimental items by Serrano (2009) using 3D scanning and 3D printing; and tools by Microsoft Research to recreate an environment in 3D just using photos. A similar technique is used in bigger preservation projects to make high quality 3D models of culturally important places and objects in China (Zhou et al., 2012).
Fig. 1.1: The Weather Camera, by Jens Wilkens. Fig. 1.2: Associating stories and weather to a timeline with the Weather Camera.
Fig. 2: backing up physical sentimental items using a 3D scanner and 3D printing, 11 with Back Up Objects by Héctor Serrano
￼￼￼
Conclusion
This literature review, although shallow considering the breadth of the topic, allowed me to gain an understanding of the current practices and debates in the digital archiving communi- ty, and to gather relevant sources for any future research in each sub-topic. I found the theme of personal archiving captivating but not explored as intensely as large scale preservation for our common knowledge and data; I would like to do deeper research in this particular area of digital preservation, notably on our use of social media and Internet services on the long term.
12
Bibliography
AppleInsider. (2011) ‘Inside Mac OS X 10.7 Lion: Missing Front Row, Rosetta and Java runtime’. AppleInsider. [web page] Accessible at http://appleinsider.com/articles/11/02/26/ mac_os_x_lion_drops_front_row_java_runtime_rosetta.html [Accessed: 19th Apr 2014]
Ayre, C. and Muir, A. (2004) ‘The right to preserve’. D-Lib Magazine, vol. 10 (No. 3). March. Available at http://www.dlib.org/dlib/march04/ayre/03ayre.html [Accessed: 11th Apr 2014]
Banks, R. (2011) The Future of Looking Back. Sebastopol: Microsoft Press.
Bearman, D. (1999) ‘Reality and Chimeras in the Preservation of Electronic Records’. D-Lib Magazine, vol. 5 (No. 4). April. Available at http://www.dlib.org/dlib/april99/ bearman/04bearman.html [Accessed: 7th Apr 2014]
Boyd, D. (2014) It’s Complicated: The Social Lives of Networked Teens. New Haven and London: Yale University Press.
byuu.1 (2011) ‘Accuracy takes power: one man’s 3 GHz quest to build a perfect SNES emulator’. Ars Technica. [web page] Available at: http://arstechnica.com/gaming/2011/08/ accuracy-takes-power-one-mans-3ghz-quest-to-build-a-perfect-snes-emulator/ [Accessed: 8th Apr 2014]
Cook, T. (2000) ‘Beyond the screen: the records continuum and archival cultural heritage.’ Presented at the Australian Society of Archivists Conference, Melbourne, 18 August. Available at http://www.mybestdocs.com/cook-t-beyondthescreen-000818.htm [Accessed: 9th Apr 2014]
Granger, S. (2000) ‘Emulation as a Digital Preservation Strategy’. D-Lib Magazine, vol. 6 (No. 10). October. Available at http://www.dlib.org/dlib/october00/granger/10granger.html [Accessed: 7th Apr 2014]
Guttenbrunner, M., Becker, C. and Rauber, A. (2010) ‘Keeping the game alive: Evaluating strategies for the preservation of console video games’. International Journal of Digital Curation, vol. 5 (Iss. 1), pp. 64-90.
￼￼￼￼￼￼￼￼￼￼1 The article is only credited to the developer’s pseudonym.
13
Bibliography (cont.)
Hangal, S., Lam, M. S. and Heer, J. (2011) ‘MUSE: reviving memories using email archives’, in
Proceedings of the 24th annual ACM symposium on User interface software and technology (UIST ‘11). New York, NY: ACM, pp. 75-84. doi:10.1145/2047196.2047206
Harvey, D. R. (2012) Preserving digital materials. Berlin: De Gruyter Saur.
Kahle, B. (1996) ‘Archiving the Internet’. Scientific American, Iss. March 1997. Available at http://web.archive.org/web/19971211123138/www.archive.org/sciam_article.html [Accessed: 27th Feb 2014]
Kawano, H. (2008) ‘Strategy of Digital Contents Archive Based on Reputation Model’, in ICSENG ‘08. Proceedings of 19th International Conference on Systems Engineering, 19-21 Aug, pp. 288-293. doi: 10.1109/ICSEng.2008.75
Kirk, D. S. and Sellen, A. (2010) ‘On human remains: Values and practice in the home archiving of cherished objects’. ACM Transactions on Human-Computer Interaction, vol. 17 (Iss. 3), article 10. doi:0.1145/1806923.1806924
Library of Congress. (2007) Digital Preservation Program Makes Awards to Preserve American Creative Works. [press release] Available at http://www.loc.gov/today/ pr/2007/07-156.html [Accessed: 14th Apr 2014]
Lubar, S. (1999) ‘Information Culture and the Archival Record’, in American Archivist, vol. 62 (Iss. 1), pp. 10-22.
Lynch, C. (2000) “Authenticity and Integrity in the Digital Environment: An Exploratory Analysis of the Central Role of Trust’, in Authenticity in a Digital Environment. Washington, DC: Council on Library and Information Resources. Available at http://www. clir.org/pubs/abstract/pub92abst.html [Accessed: 16th Apr 2014]
Masanès, J. (2006) Web archiving. Berlin: Springer.
Mesa, A. F. (1997) ‘The PowerPC Triumph’. The Apple Museum. [web page] Available at: http://applemuseum.bott.org/sections/ppc.html [Accessed: 19th Apr 2014]
￼￼￼￼￼￼￼￼￼14
Bibliography (cont.)
Paloque-Bergès, C. (2011) Entre trivialité et culture : une histoire de l’Internet vernaculaire. Émergence et médiations d’un folklore de réseau [Between Triviality and Culture: a History of the Vernacular Internet. Emergence and mediations in network folklore]. PhD thesis, self-published. Available at http://camillepaloqueberges.wordpress.com/phd-thesis/ [Accessed: 6th Apr 2014] 2
Paloque-Bergès, C. (2013) ‘Un patrimoine composite : le public Internet face à l’archivage
de sa matière culturelle’ [A composite heritage: the Internet public confronted to the archiving of its cultural matter], in I. Dragan, P. Stefanescu, N. Pelissier, J.-F. Tétu and
L. Idjeroui-Ravez (eds.) Traces, mémoire, communication. Bucharest: Editura Universităţii din Bucureşti. 2
Pearson, D. (2009) Preserve or Preserve Not, There is No Try: some dilemmas relating
to Personal Digital Archiving. [PowerPoint slides]. National Library of Australia Staff Papers. Available at http://www.nla.gov.au/openpublish/index.php/nlasp/article/view/1388 [Accessed: 9th Apr 2014]
del Pozo, N., Long, A. S. and Pearson, D. (2012) ‘Land of the Lost: a discussion of what can be preserved through digital preservation’. Library Hi Tech, vol. 28 (Iss. 2), pp. 290-300. Available from: doi: 10.1108/07378831011047686. [Accessed: 6th Apr 2014]
Ross, S. and Gow, A. (1999) Digital archaeology: rescuing neglected and damaged data resources: a JISC/NPO study within Electronic Libraries (eLib) Programme on the Preservation of Electronic Materials. London: Library Information Technology Centre. Available at http://www.ukoln.ac.uk/services/elib/papers/supporting/pdf/p2.pdf [Accessed: 17th Apr 2014]
Rothenberg, J. (1998) Avoiding Technological Quicksand: Finding a Viable Technical Foundation for Digital Preservation. [report] Washington, DC: Council on Library and Information Resources.
￼￼￼￼￼2 In French.
15
Bibliography (cont.)
Serrano, H. (2009) Back Up Objects, Héctor Serrano. [web page] Available at: http://www. hectorserrano.com/index.php?id=41&amp;m=lab&amp;grupo=backup [Accessed: 17th Apr 2014].
SINTEF. (2013) ‘Big Data, for better or worse: 90% of world’s data generated over last two years.’ ScienceDaily. [web page] Available at: http://www.sciencedaily.com/ releases/2013/05/130522085217.htm [Accessed: 16th Apr 2014]
Swade, D. (1993) ‘The problems of software conservation”, in Computer Resurrection, vol. 7. Available at: http://www.cs.man.ac.uk/CCS/res/res07.htm#f [Accessed: 12th Apr 2014]
Task Force on Archiving of Digital Information. (1996) Preserving digital information, Report of the Task Force on Archiving of Digital Information. Washington, DC: Council on Library and Information Resources.
Thibodeau, K. (2002) ‘Overview of technological approaches to digital preservation
and challenges in coming years’, in The state of digital preservation: an international perspective. Washington, DC: Council on Library and Information Resources, pp. 4-31.
UNESCO. (2003) Guidelines for the preservation of digital heritage, prepared by the National Library of Australia. Paris: UNESCO. Available at http://unesdoc.unesco.org/ images/0013/001300/130071e.pdf [Accessed: 9 Apr 2014]
Wilkens, K. (2011) Sensor Poetics, Kjenwilkens.com. [web page] Available at http://www. kjenwilkens.com/projects/sensor-poetics [Accessed: 18th Apr 2014]
Zimmer, M. (2010) ‘“But the data is already public”: on the ethics of research in Facebook’, in Ethics and information technology, vol. 12 (Iss. 4), pp. 313-325. doi: 10.1007/s10676-010- 9227-5
Zhou, M., Geng, G. and Wu, Z. (2012) Digital preservation technology for cultural heritage. Berlin: Springer.
￼￼￼￼￼￼￼￼￼￼￼￼16
Project Proposal Form
￼￼Name Programme/Course Project/working title
Project Aims (i.e. what you want to learn about through further investigation. E.g. I want toa learn more about the creative process of animators)
Objectives (i.e. the particular things you propose to do
in order to carry out your investigation. E.g. Make contact with professional animators and arrange
to meet them. Possibly Interview them or observe their creative activities to gather data)
Methods (i.e. the different ways in which you will achieve these objectives. E.g. Open ended interview techniques and grounded theory analysis or video observation of working animators)
Victor Loux
Digital Interaction Design
Digital preservation for the masses
I would like to look at further research papers on personal preservation and focus on ways to apply “large-scale” paradigms to our personal heritage while trying to be respectful of people’s privacy and unintrusive.
I first intend to do further reading in this area, as until now I have been exploring digital preservation as a whole with no particular focus, to get a broader understanding of the issues
and solutions. I would like to interview several people of different age ranges to understand their relationship to technology and their personal digital heritage, notably concerning photographs and conversations.
Using open ended interview techniques, further literature review and possibly research through practice as I intend to explore this theme for my final year project.
￼￼￼￼￼￼￼￼￼￼￼￼17
</Text>
        </Document>
        <Document ID="5">
            <Title>Introduction</Title>
            <Synopsis>Explain what I have shifted to, existing ideas that influence this shift, critique them; explain how I think it might influence my own prototypes
</Synopsis>
            <Text>This part of the dissertation discusses the gap in research that I want to explore, the experience of retrieval of memories. I will try to understand what encompasses this experience: the representation of time and hierarchisation of memories, the presentation, the feeling of durability of the backup, and how to make it enjoyable. 
This dissertation is tied to my honours project, so I will go over aspects of the project: applying the “large-scale” paradigms of conservation  established in the previous chapter for small-scale, personal archiving; the representation/visualisation of memories; how to go against the obsolescence of the object by using open design; and the relationship with families and death.</Text>
        </Document>
        <Document ID="105">
            <Title>Advantages of digital</Title>
            <Synopsis>Kirk and Sellen (2010) found however that ultimately, stories and narrative are more impor- tant than the preservation of the artefact itself. They propose that “technology might play a role in capturing and associating stories or narratives with different physical objects”, imagin- ing we could also digitally augment physical objects or places. Banks (2011) gives as example the Weather Camera (Wilkens, 2011), which records wind force and temperature along with commentary; a system of physicals backup of sentimental items by Serrano (2009) using 3D scanning and 3D printing; and tools by Microsoft Research to recreate an environment in 3D just using photos.</Synopsis>
            <Text>Digital artifacts have certain fundamental advantages over physical objects: the main two being the ease of organising a collection of data through indexing, cataloguing and adding metadata (Kirk and Sellen, 2010, p. 35), and the ability to duplicate and share that object accurately and virtually cost- free (Banks, 2011). This actually provides numerous advantages in terms of preservation.</Text>
        </Document>
        <Document ID="149">
            <Title>Streams</Title>
            <Text>Using streams to represent time differently (see fig. Cartographies/11-StromDerZeiten):

 “However natural it may be to assist the perceptive faculty in its assumption of abstract time by the idea of a line and however inseparable the sensible and mental objects may have become by the figurative method of speech it is astonishing that upon this near advance and with similar assistance from the delicate preciseness of language the image of a Stream should not have presented itself to any one whose consideration had been attracted to this object. The expressions of gliding, and rolling on; or of the rapid current, applied to time, are equally familiar to us with those of long and short. Neither does it require any great discernment to trace as a farther exemplification of this assertion, in the rise and fall of empire, an allusion to the source of a river, and to the increasing rapidity of its current, in proportion with the declivity of their channels towards the engulfing ocean. This metaphor, by presenting something more congenial to a common object of sense, and at the same time more agreeable in its variations to the nature of the abstract notion, gives greater liveliness to the ideas, and impresses events more forcibly upon the mind, than the stiff regularity of the straight line. Its diversified power likewise of separating the various currents into subordinate branches, or of uniting them into one vast ocean of power of dispersing them a second time, but still in such a manner that they are always ready under the guidance of some great conqueror to converge again into one point, tends to render the idea by its beauty more attractive, by its simplicity more perspicuous, and by its resemblance more consistent.” (Strass and Bell, 1812)</Text>
        </Document>
        <Document ID="6">
            <Title>Notes</Title>
        </Document>
        <Document ID="50">
            <Title>Adding context</Title>
            <Text>Add context to data, not just a representation of the existing data
	⁃	Possibly copy experience/UI of the social networks, quote R Banks - experience of person opening up laptop after 10 years and finding everything as is, or creating a Virtual PC from the hard drive of his deceased father. It goes along with the retrieval of the memory
	⁃	Hierarchise the data - finding ways to classify data and make more sense out of it - explore complex representations
</Text>
        </Document>
        <Document ID="7">
            <Title>Ideas</Title>
        </Document>
        <Document ID="46">
            <Title>New Folder</Title>
        </Document>
        <Document ID="8">
            <Title>About</Title>
            <Text>UNDERGRADUATE ESSAY FORMAT

About
This template is designed for writing and formatting an undergraduate essay suitable for humanities subjects. It is based on a format used at several UK universities (such as Cambridge). Bear in mind, however, that many universities have different formatting requirements, so you should check the exact guidelines of the institution to which you are submitting your paper.

How To Use This Template
	•	Edit the “Title” document so that it contains the title of your essay, your instructor’s name and so on. Alternatively, you can just edit the title of your essay and enter the rest of the information during the compile process (see below).
	•	Compose your essay in text documents inside the “Main Content” folder. Whether you use a single document or multiple text files to write your essay is entirely up to you - just make sure they are all inside the “Main Content” folder.
	•	Indent block quotes or other special passages using the ruler settings you require in the finished document and enclose them in a “Preserve Formatting” block. To do this, select the text of the block quote and go to Format &gt; Formatting &gt; Preserve Formatting. (The quickest way of formatting a block quote is to click in the paragraph and select “Essay Block Quote (Preserved)” from the formatting presets, either using the “Presets…” control in the format bar, or by going to the Format  menu and choosing Formatting &gt; Apply Preset - this will indent the text and apply the “Preserve Formatting” block for you.) All other text will have the default indentation and formatting assigned during the compile process.
	•	Use File &gt; Compile… to compile your essay for printing or exporting.
	•	In the Compile sheet, make sure you edit the entries in the “Replacements” pane, replacing “YourStudentID”, “SupervisorSurname” and so on with the appropriate information - these will be used in the page header and on the title page.

Sample Document
The “Example Essay” PDF file in the Research folder shows how the essay will look when compiled.

Final Note
Scrivener project templates are flexible and are not intended to restrict you to a particular workflow. You can change, delete or move the files and folders contained in the template and you can create your own templates by setting up a skeletal project with the files, folders and settings you would like to use for new projects and using File &gt; Save As Template.</Text>
        </Document>
        <Document ID="47">
            <Title>Objectives</Title>
            <Text>Objectives for my own project

	•</Text>
        </Document>
        <Document ID="110">
            <Title>Annotated draft</Title>
            <Text>Introduction
As we shift into the Information Age and a knowledge-based economy, a number of concerns are being raised over the long-term preservation of digital data and artifacts. This literature review aims to get an overview of the current problems, techniques and practices in preserving data on digital media, including the definition and understanding of the new paradigms created by digital preservation, the importance of being able to store things on the long term, the processes used to curate and select content to be preserved, and the various methods used to do so. It also encompasses legal and ethical issues arising from it, and how the paradigms and methods change when applied to different types of archiving (e.g. for researchers, for businesses, and for personal businesses).
Why digital preservation
[Stats of numbers on digital vs physical]
Why digital degrades faster UNESCO’s Threats to digital continuity
UNESCO (2003, pp. 30–31) has identified a list of threats to what they call digital continuity. These are: + “The carriers used to store these digital materials are usually unstable and deteriorate within a few years or decades at most”. Where, in the traditional preservation paradigms for paper-based artifacts, the main rule is that “benign neglect may be the best treatment” (derived from Cloonan (1993, p.596), Harvey (1993, pp.14,140), and Bastian, Cloonan and Harvey (2011, pp.612–613)), this cannot be applied to digital storage media because their lifetime is much shorter. This varies highly with the type of medium used and with the storage conditions; for example, samples by the Digital Preservation Coalition (2008, p. 154) show that D3 cartridge tapes deteriorates after 50 years in optimal conditions (25% RH at 10o) but only one year in high humidity and temperatures (50%RH at 28o). Similar results happen for the more commercially available optical media (down to three months and up to 200 years), which are more sensitive to moisture (Harvey, 2012, p.47).
“Use of digital materials depends on means of access that work in
￼￼
￼particular ways: often complex combinations of tools including hardware and software, which typically become obsolete within a few years and are replaced with new tools that work differently” (UNESCO, 2003). Even if storage media could survive decades or even centuries, technological change means we will not necessarily have the means to access the data later. In other words, “to retain appropriate access to a digital object, we must maintain a mechanism to derive meaning from that object” (del Pozo et al., 2012, p6): for optical media, this includes not only the drives to read the disc, but also a cable to connect the drive to the computer’s motherboard, a motherboard that has a connector for this type of cable, and an operating system that has drivers for this connectivity (Pearson 2009, slide 11). Once we have access to the bitstream from the computer we need to be able to represent it, which means we also need to keep the software in a version that can open the preserved file. There is a myth, kept by both customers and vendors, that media is long-lived: “[An IT vendor] offered us a special polymer that they guaranteed would preserve a CD-ROM for 100 years. They were unable to answer how we would preserve a CD-ROM player for that length of time.” (Weathley, 2003, cited by Harvey, 2012, p. 37)
Poor preservation documentation and metadata means it could be hard for potential users to find the data (UNESCO 2003)
poor preservation (“Critical aspects of functionality, such as formatting of documents or the rules by which databases operate, may not be recognised and may be discarded or damaged in preservation processing.”, UNESCO 2003) + “So much contextual information may be lost that the materials themselves are un- intelligible or not trusted even when they can be accessed” -&gt; poor integrity
“Materials may be lost in the event of disasters such as fire, flood, equipment failure, or virus or direct attack that disables stored data and operating systems”
“Access barriers such as password protection, encryption, security devices, or hard-coded access paths may prevent ongoing access beyond the very limited circumstances for which they were designed”. [This argument is particularly true for web archiving, where crawlers of national libraries retrieve all sort of files to preserve them but do not
￼￼￼￼￼
￼necessarily have the permission or know the rights owner][back that up maybe yo]
Importance of data Library material: for society
“if we librarians do not rise to the occasion, successive generations will know less and have access to less for the first time in human history. This is not a challenge from which we can shrink or a mission in which we can fail” (Gorman, 1997, cited in Harvey, 2012, p. 27)
For businesses (records)
Personal archiving / domestic archiving
Preserving interactive material inc. video games
In addition to the preservation of “static” documents and images, there is a significant desire to preserve video games, which are getting recognised as a form of art similarly to movies and music. The Library of Congress (2007) gave awards for the preservation of video games; there is a growing interest in being able to play “retro” games, but the consoles are obsolete. A large community of hobbyists reverse engineered consoles and produced emulators of these platforms for PC and other platforms different from their intended one. Because no documentation is available, this is a Herculean task; byuu (2011), the developer of an emulator for Super Nintendo games, recalls this for reengineering certain cartridges:
[Low-level emulation] is also a very expensive operation, monetarily speaking: to obtain the DSP program code requires melting the integrated circuit with nitric acid, scanning in the surface of a chip with an electron microscope, and then either staining and manually reading out or physically altering and monitoring the traces to extract the program and data ROMs. This kind of work can cost up to millions of dollars to have done professionally, depending upon the chip’s complexity, due to the extremely specialized knowledge and equipment involved. (byuu, 2011)
Obviously this is not necessarily a route many institutions would like to take for preserving digital objects. Because this also causes intellectual
￼￼
￼property issues, Nintendo and Sony have launched their own preservation programs, allowing players to play “classic” games on the newer generations of consoles; this is a far more accurate emulation (as they do have the internal documentation to create these emulators) and respects their monetary interests (players have to buy the games, whereas in hobbyist emulation, you would have to download the game file).
Useful for researchers in the future
Beyond the governmental and cultural needs for preservation, there is also great need for preserving scientific data. “Data are the foundation on which scientific, engineering, and medical knowledge is built” (Committee on Ensuring the Utility and Integrity of Research Data in a Digital Age, 2009, p. ix, cited in Harvey, 2012, p. 26); therefore it is crucial in a digital era, with enormous data sets and research papers becoming widely public, to ensure this doesn’t disappear.
Understanding our history
There is a need to define what is the purpose of archives. As a tool to learn from the past, or to keep records, but as Cook (2000) eloquently puts it:
“Archives [...] are a source of memory about the past, about history, heritage, and culture, about personal roots and family connections, about who we are as human beings and about glimpses into our common humanity through recorded information in all media, much more than they are about narrow accountabilities or administrative continuity.” (Cook, 2000)
Preservation should, therefore, be considered very important. There is some skepticism to this:
￼
￼￼“It was suggested in 2002 that the last 25 years have been a ‘scenario of data loss and poor records that has dogged our progress’ and that, if this is not reversed, ‘the human record of the early 21st century may be unreadable’ (Deegan and Tanner, 2002). Rhetoric of this kind is common in the literature, but is regrettably poorly supported with specifics and evidence. Alarmist descriptions abound: there will be ‘a digital black hole... truly a digital dark age from which information may never reappear’ (Deegan and Taylor, 2002) if we do not address the problem, and we will become an amnesiac society. [...] It is only possible to conclude, as the authors of the Digital Preservation Coalition’s handbook (2008, p.32) did for the UK, that the evidence of data loss is ‘as yet only largely anecdotal ... [but] it is certain that many potentially valuable digital materials have already been lost’.” (Harvey, 2012, pp. 33–34)
While it’s difficult to argue that preserving digital objects is necessary, there is certainly a need to define what deserves to be protected, and to what level.
Understanding the beginnings of the Internet, as we have shifted in an information society
Preserving an accurate + [integrity] image of our society at a given point Twitter &amp; Usenet
Camille Paloque-Bergès (2011; 2013) proposes that we can already find historical and sociological evidence of a given period in Internet time when browsing archives of Usenet [footnote: An Internet discussion system established in the 1980s and that is widely regarded as the predecessor of the Web and online fora]. She analysed how the way of speaking has evolved [footnote: See also http://wwwtxt.org (or @wwwtxt on Twitter), a project collecting short phrases of the Usenet to demonstrate this] and how an self-organised community had built itself, noting that we can “feel [the community’s] informational generativity” and understand that one of the biggest qualities of the early users of the Internet was their “recursivity as a public” (Paloque-Bergès, 2013); that is, how they welcomed newcomers — notably through the use of FAQs [footnote: Frequently Asked Questions], which weren’t a common concept back then. There are also large elements of nostalgia coming back (Paloque-Bergès, 2011).
It is also interesting to come back on the archival of Usenet. The messages were not systematically archived, and obviously not available on the Web as
￼it did not exist and is separate. Today’s collections have mainly been recreated from donations of people who did archived Usenet newsgroup, originally to DejaVu News in 1995, and Google Groups in 2001. A number of privacy concerns were raised with the publication of these searchable archives; messages who were, 15 to 20 years earlier, thought to be confidential are now considered as digital heritage. There was also a large backlash against the archiving made by for-profit companies, and the credit taken later by Google for its donors, with other critiques like Brad Templeton’s (an Internet veteran and important figure; cited in Paloque- Berges, 2013) remark that the archive was lacking in integrity. However, the searchability of these archives did bring welcomed technical possibilities for an Internet historiography, including a timeline where “the announcement of the Chernobyl disaster on Usenet stands alongside the first message of Linus Torvalds calling for help on what would become the Linux operating system” (Paloque-Bergès, 2013).
Similar concerns appeared at the announcement of the archival of Twitter by the Library of Congress in 2010. The two are not really comparable, mostly because the archive is not public and is made by a not-for-profit body. On the mailing list of the Association of Internet Researchers, the idea was met with interest: “The public twitter stream is of historical cultural significance and is an amazing repository of mundane moments in the daily lives of many people and records of what they thought important” (Baym, 2010) [on http://listserv.aoir.org/pipermail/air-l-aoir.org/2010- April/021125.html]”. However, the privacy concerns (notably with public and private feeds) were discussed, and as Michael Zimmer remarked:
“This is the classic “but the information is already public” argument that, while technically true, presumes a false dichotomy that information is either strictly public or private, ignoring any contextual norms that might have guided the initial release of information or how a person expects that information to flow.” (Zimmer, M. [on http://listserv.aoir.org/pipermail/air-l-aoir.org/2010-April/021136.html]; see also his paper (2010) on Facebook privacy, going further in that argument)
Paloque-Bergès (2010) theorises this argument, suggesting that “information is not an object, but a statement; it goes through socio-technical processes; it doesn’t have an objective nature”. A message in a tweet can have a very
￼
￼different context when taken apart of the moment it was posted. It also emphasises that privacy could change (the user could, later, switch their account to Private, delete the tweet or their account altogether, but this wouldn’t be reflected in the archive. We cannot know what is the user’s original intent when they posted their message, and, as Paloque-Bergès (2010) noted, we aren’t sure the user themselves know.
“More often than not, in a technical ecosystem in which making content private is more difficult than sharing broadly, teens choose to share, even if doing so creates the impression that they have given up on privacy. It’s not that every teen is desperate for widespread attention; plenty simply see no reason to take the effort to minimize the visibility of their photos and conversations.” (Boyd, 2014).
Advantages over physical preservation Mass preservation, easier access and organisation Recording digitally material that’s not born-digital
Cost of restoration; case studies of lost data
A number of institutions have already experienced loss of data, or data being corrupted. This can have disastrous consequences for businesses, for example, as noted by McAteer (1996, cited by Ross and Gow, 1999, p. iii): “of the 350 companies unexpectedly relocated by the [1993] World Trade Centre (NYC) bombing 150 [43%] ceased trading, many because they lost access to key business records held in electronic form”.
Data can, obviously, be sometimes restored. Ross and Gow (1999, pp. 39–42, referenced in Harvey, 2012, p. 34) provide a number of case studies. Harvey (2012, p. 34) also gives the ironic example of the website for the “Functional Requirements for Evidence in Recordkeeping” project, directed by the University of Pittsburgh which was accidentally deleted; but thankfully was automatically saved by the Internet Archive.
There is still the possibility of restoring data by specialised carriers; however Pearson (2009, slide 15) notes that while this was generally effective, this did not work on some items. He also regrets that the
￼
￼restoration loses all metadata in the process; this can be an issue for sensitive or classified information (for business or governmental archives) as the carrier would be able to see it; finally, it is a prohibitely expensive operation, and so a carefully planned preservation could be much cheaper (and obviously more reliable) than neglect. However, these services are always useful in certain cases. Harvey (2012, p. 41) cites the statistics by the data recovery giant Kroll Ontrack (2011): “hardware and system problems cause most customers to approach them (29 per cent), followed by human error (27 per cent). Software corruption or program problems, computer viruses and natural disasters are the other reasons listed.”
Defining paradigms for digital
preservation
Pre-digital paradigms
Inability to preserve the medium, only the bitstream Importance of metadata, relationship &amp; context Definition of terms
What to preserve / Selectivity
Harvey 5 pillars for selectivity (Harvey 2012, p. 26)
If unique information objects that are vulnerable and sensitive and therefore subject to risks can be preserved and protected
If preservation ensures long-term accessibility for researchers and the public;
If preservation fosters the accountability of governments and organisations
If there is an economic or societal advantage in re-using information
￼￼￼￼
￼If there is a legal requirement to keep it (NSF-DELOS Working Group on Digital Archiving and Preservation, 2003, p.3).
Banks reasons for domestic archiving
In The Future of Looking Back, Richard Banks (2011) analyses the reasons that lead us to keep or discard physical objects at home. He argues that while making a selection is a positive thing because it forces us to filter our belongings and focus on what is really meaningful, we also run the risk of ridding ourselves of things that we wish we would have kept in retrospect. He points out that the organisation of digital files make this trickier: because we we can keep pretty much everything on virtual space, it is easier to accidentally delete meaningful things alongside the less important. (Banks, 2011 p. 22).
In the context of personal archiving, Banks notes that in the past, the legacy we leave behind us was a highly selective one. Because our possessions are limited by physical space, our ancestors until now “have implicitly indicated that an item is significant through the simple act of keeping it. Although thousands of objects likely crossed their path over their lifetime, the constraints of money and space meant that only a few were retained, often the most significant.” (Banks, 2011, p. 78). He does however argue that this is no longer true in a digital world, because new possibilities mean we simply accumulate digital objects without necessarily ordering them by importance.
Technical possibility of archiving Pros/cons of archiving the entire internet
There are various controversies around whether we should automate the archiving of the Internet, or if we should operate a manual selection. UNESCO (2003, p. 73) notes that those who value a more comprehensive collection (i.e. archiving “all” the Internet) argue that “any information may turn out to have long-term value, and that the costs of detailed selection are greater than the costs of collecting and storing everything”. This argument is particularly true because of the digital media: “unlike non-digital material such as paper-based artifacts, where there is a period of time in which to make selection decisions before deterioration of materials becomes an issue, the time frame for deciding whether or not to preserve digital materials is very short” (Harvey, 2012, p. 57). UNESCO (2003, p.71)
￼
￼backs that claim: “it may not be possible to wait for evidence of enduring value to emerge before making selection decisions”.
Other people [Cook?] think that it would be preferable to keep a curated collection because of a higher technical quality, and this also allow us to settle [negotiate?] preservation rights with the content producers. Harvey adds that we also do not have the technical capabilities to find all the pages on the Internet, and even if we had we would not have the technical possibility to store the whole of the Internet (Harvey 2012, p. 58); again, this is also something recognised by UNESCO: “‘there are usually more things – more information, more records, more publications, more data – than we have the means to keep” (2003, p.70).
The UNESCO (2003) and Cook (2000) both however recognise that a middle ground should be found and that both approaches are valid depending on the goal of the preservation. For instance, the Internet Archive[IA] is going through an automated process (archive as many pages as possible) but, because of technical limitation, only preserves the text and dismisses all other images, stylesheets and external media, which could be critical in some instances. It seems evident that for valued documents these should be retained. The British Library’s Web Archive[ukwa] [http://www.webarchive.org.uk/ukwa/]
Dangers of selection (Cook)
However, Cook (2000) points out the dangers of manual selection for what is worth archiving: “With memory comes the inevitable privileging of certain records and records creators, and the marginalizing or silencing of others” (Cook, 2000, cited in Harvey p. 58). He asserts that the archivists must be impartial and objective, and ensure all points of view are kept to ensure our shared memory does not “becomes counterfeit, or at least transformed into forgery, manipulation, or imagination”. This is something that is clear with manual archiving, but could also be a problem, maybe unintentionally, with the automated discovery of content when archiving Web pages.
Automatic selection (Reputation model)
In order to keep the most valuable content, algorithms could be used. Kawano (2008) proposes to use reputation models, that attempts to
￼determine the relative value of a Web page by considering its “importance, fairness, trustiness, uniqueness and valuation”. This is done using a variety of ways; the algorithm suggested by Kawano relies on the consistency of a web page, that is the amount of time it existed, the number of links it contains and the number of external links to this page (Kawano, 2008, p. 291).
Crawling
Manual curating
Special Collections for events (eg before elections, see webarchive.org.uk for other ideas)
Another interesting selection process that has emerged is the one of specific archiving when certain events are happening. For instance, the UK Web Archive[ukwa] has, on top of their topical-based collections (Energy, Health and Social Care...), a few event-based collections (Queen Jubilee, Olympic and Paralympic Games, London 2005 terrorist attacks...) which reflect the state of the Web at that given moment in history. Masanès (2006, p.83) notes that this type of archiving must be manually curated and thoroughly preparated. He gives a methodology for this, using presidential elections as an example: a time frame for the collection must be defined (“3 months before the election + 1 month after”), what type of sites will be archived and when (“the political parties’ blogs and websites, every week; analysis, commentary and humourous/satire websites, every month; online newspaper articles, once”).
Selection: by genre
There are different genres of websites: institutional website, blogs, personal pages, forums. [[Genre have been studied in the context of the Web to see how they replicate or diverge from the genre in the print- ing world (see Crowston and Williams 1997) - in Masanès p88]]. An interesting point about this is that often, robots (crawlers, or later data analysis) can automatically identify their type [Rehm 2002 -&gt; check reference, Masanès:88] which permits the automatic generation of metadata, or simply to restrict a collection of web pages to certain Web genres.
Web archiving: what stands as
heritage on the web Who should preserve
Digital media changes the original preservation paradigm for deciding who takes the responsibility for archiving content, and how. The processes of creation, fabrication and publication used to be distinct in the “analog” era; but as Nurnberg (1995, p. 21, cited in Harvey, 2012, p. 9) said, “technology tends to erase distinctions between the separate processes of creation, reproduction and distribution that characterize the classic industrial model of print commodities”. Self-publication (particularly on websites) is much more common, and this is also true for businesses and personal data. The Task Force on Archiving of Digital Information (1996, pp. 19–20) suggests that when, in general, “stakeholders disseminate, use, reuse, recreate and re-disseminate various kinds of digital information, they can easily, even inadvertently, destroy valuable information, corrupt the cultural record, [and ultimately thwart the pursuit of knowledge that is their common end]. Against such a danger, a safety net is needed to ensure that digital information objects with long-term cultural and intellectual value survive the expressions of stakeholder interest with their integrity intact.
[&gt; Society has always created objects and records describing its activities, and it has consciously preserved them in a permanent way... Cultural institutions are recognised custodians of this collective memory: archives, librar[ies] and museums play a vital role in organizing, preserving and providing access to the cultural, intellectual and historical resources of society. They have established formal preservation programs for traditional materials and they understand how to safeguard both the contextual circumstances and the authenticity and integrity of the objects and information placed in their care... It is now evident that the computer has changed forever the way information is created, managed, archives and accessed, and that digital information is now an integral part of our cultural and intellectual heritage. However the institutions that have traditionally been responsible for preserving information now face major technical, organisational, resource, and legal challenges in taking on the preservation of digital holdings (B. Smith, 2002, pp.133–134) [cited in Harvey p. 29].]
￼
￼A variety of new stakeholders are now required to collaborate more for a good preservation practice to take place. Lavoie and Dempsey (2004, cited in Harvey, 2012, p. 30) argues that cooperation “can enhance the productive capacity of a limited supply of digital preservation funds, by building shared resources, eliminating redundancies, and exploiting economies of scale”. Chapter 11 of the UNESCO guidelines (2003) give more pratical details on how this can take place, giving structural models and tips on what to share (information, standards, division of labour...). [CLOCKSS cooperation to preserve journals (see Harvey p32 + chapter 9); + Harvey p32 National Library of Netherlands &amp; Elsevier Science + most uni libraries access with publishers, even UoD with eg Dawsonera/ACM]
Legal issues
(see later) special case for emulation
In certain cases, the archiving process is not made in accordance with the original publisher. This is often the case for the automated archiving of the Internet (e.g. Wayback Machine), which automatically crawl pages, but as a website publisher, it is easily possible to opt out of these schemes [see http://archive.org/about/exclude.php [Accessed 17th Apr 2014]]. In the case of emulation, this is what happens with reverse engineering, that is, disassembling the original program to understand how it works and recreate a similar version which can operate from a different hardware or software architecture. This is a common process for proprietary software or hardware, whose source code is not available to the general public and no documentation is provided for developers. Often, this is developed by amateurs and hobbyists, but is illegal as it breaks the intellectual property of the original creator; Nintendo, for example, attacks reverse engineered emulators[http://www.nintendo.com/corp/legal.jsp [Accessed 17th Apr 2014]] and the sharing of ROMs [Read Only Memory; the bitstream of cartridges or discs containing the games]. There is however popular demand for playing these, and Nintendo has developed their own emulators to play legacy games on their new consoles (see the Virtual Console [http://en.wikipedia.org/wiki/Virtual_Console]), while still retaining their property rights (you need to buy the games again on an e-commerce platform). This however causes issue for the longer term preservation: there is no certainty that Nintendo will continue to provide this service on their future generations of consoles, or if they ever cease operations.
￼A similar issue happened with Apple Computer. In 2005, Apple changed the architecture (that is, the “language” of instructions needed for the processor) of their personal computers from PowerPC to Intel x86. This means that applications needed to be rewritten, sometimes from the ground up, to work natively on the newer Apple computers. To ease the transition, Apple created Rosetta, a software that dynamically translates PowerPC instructions to x86 instructions, so that buyers of the new Apple computers were still able to open legacy applications that were not yet rewritten for x86 — although this came with a significant performance hit. In 2009 however, support for PowerPC was entirely discontinued [...]
Libraries
Good curating and preservation starts early. Smith (2003, pp. 2–3, cited in Harvey, 2012, p. 32) suggests that “the critical dependency of preservation on good stewardship begins with the act of creation, and the creator has a decisive role in the longevity of the digital object”. Harvey (2012, p. 32) notes that “for most creators of information in any form this is a new role”; therefore publishers of digital content are not necessarily well prepared for preservation, and this is why collaboration with librarians and archivists can be helpful (Ayre and Muir, 2004).
Ayre and Muir (2004) note that if the publisher worries about preservation themselves then they generally already own the rights for preservation, but there are negotiations to be done if an external body helps with preservation.
National libraries
Ayre and Muir (2004) suggest that the complexity and resources needed for digital preservation may mean that individual libraries may not have the possibility to do the preservation themselves like they used to, but instead rely on national libraries and centralised preservation systems to reduce overhead. This system is used by a number of libraries; even here at the University of Dundee, most journals and a selection of books are available through centralised systems (Dawsonera, Elsevier Science) or publishers directly (ACM). A report by Hedstrom and Montgomery (1998, cited by Ayre and Muir, 2004) found that most research librarians in the UK thought that a legal deposit to national archives was the best option, but this was not a
￼clear consensus. The report also highlighted that these librarians thought that “other libraries and publishers would need to, and should, have some involvement in preservation”.
Because all preservation methods involve copying of the original object, it might be illegal to make such a copy without the necessary rights, even if it is for preservation purposes. Certain countries have laws to overcome this and allow national libraries to preserve all materials; Ayre and Muir (2004) list a few of them and their specificities, but the situation has since evolved, as a lot of legislations have enacted changes to specifically include digital materials.
Archivists
New stakeholders
Profits vs nonprofits
Harvey (2012) points out doubts in the real interests of for-profit companies to achieve long-term preservation;
Even given the prevailing market-driven political ethos, it is difficult to envisage a situation where market forces will be sufficient to ensure the preservation of this digital material. The opposite is more likely to apply: ‘in some cases market forces work against long-term preservation by locking customers into proprietary formats and systems’ (Workshop on Research Challenges in Digital Archiving and Long-term Preservation, 2003, pp.x-xi). (Harvey, 2012, p. 32). [link with Google concerns]
How to preserve
Software methods Making hard copies
What appears to be the simplest solution is to make physical copies (i.e. print) of the digital documents. Although this certainly solves some
￼
￼problems in regards to technological obsolescence, Rothenberg (1999) and Granger (2000) pointed out that this cannot be done for more interactive material besides text and images. There is also, obviously, an issue with the amount of content that can be stored in this way — this would not work for archiving websites. Granger (2000) recognises that “if one’s sole concern is with the intellectual content of a document and the document is of a fairly simple nature [...] this at least provides some form of security”.
Standardisation
Standardisation is the transformation of a document’s content to a format that is standardised, open and well documented, ensuring that we will be able to open it with a variety of applications on a variety of platforms and that we can rebuild a program for visualising that document — as opposed to vendor-specific, closed source program that could be discontinued at any point in the future. Rothenberg (1999, p. 10) claims that this is a bad solution because “standardisation sows the seeds of its own destruction by encouraging vendors to implement non-standard features in order to secure market share” (therefore these non-standard features will be lost if opened in a different application). Granger (2000) argues that the blame is not to be put on the method, but on vendors. Standardisation is a type of migration and therefore share some of it cons (notably, this absence of vendor-specific features could imply a loss in the presentation and other characteristics). Standards eventually become obsolete so it is also paramount to preserve the specifications of the format, or to define formats that are unambiguous, such as CSV or XML.
A slightly different approach to this method is rebuilding viewers. In this scenario, the bitstream is not migrated to a standardised format, but kept as is; we simply re-build a viewer that is able to read that format. This is not as straightforward as it sounds, because most of the time, formats for proprietary software are not documented and decrypting them would require reverse engineering. If they are documented, then it is likely that this format is already a standard. There exists cases where reverse engineering is possible, or where a non-standard format is documented; for example, Microsoft Word files are not a standard, but there is plenty of documentation available online to produce code able to read them. An experiment by the VERS project (Thibodeau 2002, p. 22) has shown that it is possible to reengineer a viewer for the PDF file format from the specification available; as long as we also preserve these specifications, we
￼will be able to read PDF files in the future.
UVC
Lorie (2000, in Thibodeau, 2002, p. 22) has also proposed that the use of a portable programming language, such as Java [footnote: Most programming languages include instructions that are specific to a given platform, therefore the same file will need to be adapted to run on another platform. “Portable” languages like Java are called high-level and are more abstract. They are run inside a virtual machine, which worries about transforming Java code into instructions that are specific to that platform; as such, the same file of code can be ran on any platform.]. Lorie developed a software called Universal Virtual Computer (UVC), able to decrypt a variety of file formats, which can run on any platform supporting Java; because Java could be implemented on virtually any architecture, this means that a reader could be run on any platform without any effort. However, Lorie points out, this approach is limited in terms of functionality, and comes at the price of performance (because portable programming languages are platform-agnostic, this means the code is not optimised for any of the platforms). I am also wondering if this would really work on the long term, as we cannot be sure that future implementations of Java will be backward compatible with code written for a previous version.
Migration
Migration is the transformation of a document’s content to a format that is not necessarily standardised, but at least current; or, the newer version of same format. This could be, for example, transferring documents from the WordPerfect 6 format (used in the 80s and late 90s) to a Word 2011 format, or a Word 98 file to Word 2011. This should be done regularly, as after a certain period of time new programs will stop having backward compatibility to import these old formats (Thibodeau, 2002). This method has several downsides, mainly being that it is labour intensive and expensive (Thibodeau, 2002; Rothenberg, 1999); product lines, and therefore the migration path, could be terminated at any point (Thibodeau, 2002); the presentation and other characteristics could be changed (although this can be overcome by migrating to more flexible formats like PDF or LaTeX). Rothenberg (1999) adds that this method is also prone to error, risky (as it could compromise the integrity of the data), not scalable and it requires new solutions for each new format. Granger (2000) argues however that
￼comparatively, other methods (particularly emulation) are potentially much more expensive and labour intensive, and that for most preservation programmes, migration remains the only viable method.
Computer museums
Another approach to preservation, proposed by Swade (1993), would be to preserve a variety of hardware, software and devices to access obsolete media in “computer museums”. This is generally not considered a good solution; Granger (2000) notes that it would be unreasonably expensive to keep old machines running for an extended period of time, that computers chips will eventually decay anyway (and repair or replacement will be impossible), and that storage media not stand the test of time. There are still two possible use cases, identified by Rothenberg (1999): ensuring that emulators work as expected (by comparing them to original platforms), and helping with the recovery of lost data on obsolete media
Emulation
Emulation is the dynamic transformation of instruction sets to allow software and/or operating system designed for an obsolete architecture or platform to run on current systems. It is a highly complex method that offers certain benefits; notably, this is the most conservative method, as it is not destructive of the content or presentation. It allows us to run the original program used to create a digital object without altering that bitstream ; it “keeps the look and feel as well as the interactivity” (Granger, 2000) and as such, it is particularly interesting for objects rich in interaction, such as video games, data visualisations, or formats that are particularly complex (for example, medical imagery, 3D modelisations), for which methods like migration would be hard. It also ensures complete integrity and authenticity, which could be crucial for some data types.
There are numerous criticisms against emulation. A number of legal and intellectual property problems are raised, if the emulation strategy is not supported by the original author [see Nintendo &amp; Apple]. The complexity involved in creating emulators might be not worth it and requires a coherent organisation to reduce overhead (Granger 2000). Additionally, Thibodeau (2002, p. 21) argues that keeping the original functionality is not necessarily a positive thing when it comes to the delivery of the preserved content; “most users in the future will never have encountered—not to
￼mention learned how to use—most of the products they will need to access the preserved information objects”, and “it would cut users off from the possibility of using more advanced technologies for discovery, delivery, and analysis”.
“Emulators themselves become obsolete; therefore, it becomes necessary either to replace the old em- ulator with a new one or to create a new emulator that allows the old emulator to work on new platforms. In fact, if you get into an emula- tion strategy, you have bought into a migration strategy. Either strategy adds complexity over time.” (thibodeau 2002 p.20)
[NB: due to the length limit of this literature review, I have only explained the most popular archiving techniques. See Thibodeau (2002) and Harvey (2012) for a more comprehensive list of methods, including configurable chips, persistent archives, object interchange format, etc.]
Choosing one
Thibodeau (2002, pp. 15–16) suggests four criteria to choose a preservation method: feasibility, sustainability, practicality and appropriateness.
Feasability means that the hardware and the software for implementing a given method must be existing and developed. Sustainability is ensuring that this method can be “applied indefinitely into the future, or that there are credible grounds for asserting that another path will offer a logical sequel to the method, should it cease being sustainable”. This means that the method must be resistant to technological obsolescence, both for storing the data (medium will change) and accessing it (means to access the media ane decode the bitstream will change).
Practicality is establishing that this method will be reasonably difficult and affordable. [This can generally change depending on the resources of the preserving organisation, the available technology at that time and the valuation of the digital objects.]
Finally, appropriateness is using a method that is relevant to the type of material being preserved, and the objectives of the preservation. For example, del Pozo et al. (2012, p. 7) suggests that if the integrity and
￼
￼authenticity of the documents are not paramount, then a migration that keeps the contents but discards the presentation might be the least expensive and simplest option. Inversely, when trying to preserve highly interactive material such as video games or when the specifications for a format are unknown, then emulation is a more pertinent option, because migration would be extremely difficult and inaccurate (Granger, 2000).
Authenticity/integrity and issues with each method
Different preservation methods yield different results in what is called authenticity, that is how faithful the rendition [its presentation] of the preserved object will be. Authenticity is subjective and therefore there is debate around this issue (Del Pozo et al., 2012, p. 7; Cook, 2000); however most agree that while as little difference in the presentation is desirable, it is rarely (if ever) possible to keep a truthful rendition. As an example, del Pozo et al. (2012, p. 8) use a spreadsheet within the migration method:
“For instance, one institution may decide to normalise a spreadsheet into a PDF, which would retain the cell values but destroy any formula used to calculate them. This would favour a particular presentation of that digital object over the information form of the original. On the other hand, another institution might decide to normalise spreadsheets into ODF, which may lose some formatting but retain the formula used to derive each cell. In this case, being able to retain the information form of the file would be seen as more important.”
Thibodeau (2002, p. 28) however points out that “people will want to use the best available technology—or at least technologies they know how to use— for discovery, retrieval, processing, and delivery of preserved information. There is a danger that to the degree that preservation solutions keep things unaltered they will create barriers to satisfying this basic user requirement.” An overly faithful preservation would therefore not be desirable. Again, this is subject to a high number of parameters.
Another important issue is integrity, i.e. “ensuring the object is complete and has not been altered or corrupted” (Harvey, 2012, p. 54; Ross [CITED], 2002, p. 7); that is, the object “is what it purports to be”, and no parts are missing from it. This is particularly important, Harvey (2012, p. 54) notes, for business archiving, where the legal value of a digital object could be lost if we cannot demonstrate that this object hasn’t been tampered with, or that a
￼
￼migration processes have not incurred data losses. However, as Lynch (2000) has argued, the integrity of objects is only a matter of trust. The guidelines of UNESCO (2003, p. 22) recommend that the best protection for authenticity is proper documentation of the archival process; Harvey (2012, p. 54) expands on this idea: “for instance, if it can be established that a digital object has always been kept in the custody of an archive in which every change to it has been recorded (such as changes resulting from migration of the bit-stream – the dates at which migration occurred, to which media, and so on), then we can be more secure about its integrity.”
Emulation / legal rights with videogames Use of standards
Some standards such as OAIS (Open Archival Information System), originally developed for space science, could be used. Sadly [no implementation][source??]
Personal archiving
On a more personal level, Harvey (2012) and Lukesh (1999) note that personal correspondance might be lost if we do not take steps to preserve them:
“The widespread shift from writing letters to the use of email has diminished the likelihood that personal correspondence will remain accessible to future historians. Lukesh asked in 1999, ‘Where will our understandings of today and, more critically, the next century be, if this rich source of information is no longer available?’, as scientists, scholars, historians and almost everyone increasingly use email” (Lukesh, 1999) (in Harvey, 2012 p. 32)
While this is a correct idea, I think that, just like the archiving of Usenet, few people kept their letters in the sole purpose of leaving researchers decades from now understand our epoch’s social habits and ways of speaking. There has to be other motivations for preserving our conversations and our objects.
￼
￼Richard Banks, a researcher at Microsoft Cambridge, suggests that we first preserve artifacts for ourselves. The majority of the time, we simply keep what we consider significant to remind ourselves of particular people, events and places, and to share them with people we care about; we discard what is not meaningful to us by pragmatism (lack of physical space, lack of importance, redundance) (Banks, 2011, p. 6). This selection process contributes to keeping what we want to remember, and what is the legacy we will want to leave to others after our deaths. However, the digital age changes this situation. First, the space to store our objects on computers and online is becoming virtually unlimited; second, storing photos and data on online services mean we may forget about them in a few years (Banks, 2011).
What happens after our death
### [Digital heritage]
Shifts in usages (more photos)
Advantages (sharing, organisation etc.); record part of our lives we never did before
Digital artifacts have certain fundamental advantages over physical objects: the main two being the ease of organising a collection of data through indexing, cataloguing and adding metadata (Kirk and Sellen, 2010, p. 35), and the ability to duplicate and share that object accurately and virtually cost- free (Banks, 2011). This actually provides numerous advantages in terms of preservation.
[[+ tie with “In the past ancestors have signified something is important simply by keeping it” part ]]
The first one, ease of organisation, can be significantly helpful in tackling the abundance of digital data. Banks (2011) suggests that the technologies lying behind the ranking in the result of Web search engines could, eventually, help us in understanding and finding what is our most important data. An attempt has been made by the MUSE (Memories USing
￼Email) project (Hangal et al., 2011), which aims to find patterns in our emails archives to help us make sense of this large data set. As the authors note in the introduction:
“Email has become a de facto medium of record; many people consciously deposit important information into email, knowing they can look it up later, and thereby use their email account as an informal backup device. Therefore, email archives contain or reflect memories that are extremely valuable for the purposes of reminiscence.” (Hangal et al., 2011)
Early users reported that they have been using it to: make a summary of their work progress over the year; extract and organise certain type of data (for instance, the personal out of the professional, or the important out of the more mundane); finding life milestones inside family emails; picking up work that’s been left unfinished and forgotten; renewing with old relationships; and serendipitous discovery. The authors note in the conclusion that the different uses of their program went beyond their original expectations of simply reminiscing: “the stories above include an example of each one of the ‘5R’s’ described by Sellen and Whittaker [2010]: recollection, reminiscing, retrieving, reflecting, and remembering intentions. Further, it suggests that browsing and remembering the past can affect the future.”
The second advantage, the ability to accurately and easily make copies and backups, is fairly obviously an advantage for personal preservation. However, Banks (2011) notes, the process of backing up is troublesome and storage media do not make our data immune. Additionally, the use of online services to store our data creates an abstraction between the storage material and us; about Flickr: “I have no idea what kind of hardware my files are now stored or even where they are geographically. I just expect to have access to them as long as I pay my bills” (Banks, 2011, p. 27).
Keeping memories attached to digital objects
Banks (2011) also suggests that we could blend in the physical and the digital to allow recording of things we never had the opportunity to before. He gives as example the Weather Camera, an experimental object by Kjen Wilkens (2011) that records wind and temperature along with commentary,
￼
￼instead of an image, as an alternative to recall a place; or 3D backups by Héctor Serrano (2009), another experimental project that allows us to make backups of sentimentally important objects through 3D scanning and 3D printing. Similarly, Banks notes tools by Microsoft Research to recreate places in a 3D environment based on photos, allowing to recreate a sentimentally important place at any given point in time provided enough photos are available. A similar technique is used in bigger preservation projects, as described by Zhou et al. (2012), to make high quality 3D models of culturally important places and objects in China.
Kirk and Sellen (2010) found however that ultimately, stories and narrative are more important than the preservation of the artifact itself. They propose that “technology might play a role in capturing and associating stories or narratives with different physical objects”, imagining that we could augment physical objects with RFID tags (and I would add, virtual reality technologies) to pass down stories attached to these objects.
Conclusion
</Text>
        </Document>
        <Document ID="106">
            <Title>Social networks</Title>
            <Text>The emergence of reliable broadband and smartphones meant that instead of storing things locally, we often compose and publish them directly online, on social networks or in the cloud, with no stable backup of our own.
 The safety of the content is an issue that’s already been under public spotlight, both in terms of privacy (your content is sometimes publicly accessible) and security (hacking and social engineering[1], governmental surveillance programmes).  What interests me here is the lifespan of the data; the uncertainty of what happens to our data if the provider goes out of business, arbitrarily decides to suspend one’s account[2], or accidentally loses it (be it due to an accidental manipulation, technical failure or a natural disaster at the data centres).
Of course, any data (digital or not) is vulnerable to time, and it is generally possible to back them up and use techniques to keep it available over long period of time. The problems brought by social networks are that:
	•	these techniques are not widely available for the general public 
	•	there is a lack of understanding or concern from the public about this
	•	The data is harder to access and therefore harder to back up


[1] A type of emotive manipulation on someone to gain access to confidential information or an online account. It is based on the malicious exploitation of trust of the victim, and is not a technical flaw.
[2] http://consumerist.com/2011/07/22/google-deletes-last-7-years-of-users-digital-life-shrugs/</Text>
        </Document>
        <Document ID="51">
            <Title>Remembrance values</Title>
            <Synopsis>Existing research by Miriam Julius {Jung:2013ua} on remembrance values
• What they are posting &amp; on which SNS
• Defining what memory means for them
• Retrieval methods to go back to data 
   ⁃ Laborious methods are currently used to retrieve old posts</Synopsis>
            <Text>[Cite &amp; analyse results of existing interview/survey by Miriam Julius {Julius:2013ua} on remembrance values
	•	What people are posting on social networks
	•	Which SNSs are they mostly using
	•	Defining what memory means for them
	•	Retrieval methods used to find old data
	⁃	Laborious methods are currently used to retrieve old posts]</Text>
        </Document>
        <Document ID="9">
            <Title>Example Essay</Title>
            <Text>B Twemlow (BXT32) / Bing-Tredmille / 30 Oct 2010 ... 1
BARRY TWEMLOW (BXT32 - DOGNOBBLER COLLEGE) ENGLISH TRIPOS PART II – TRAGEDY PAPER ESSAY FOR DR. HEIRONYMUS BING-TREDMILLE (30 Oct 2010)
“Cur vel habeo delenit interpretaris, usu et alienum epicuri consetetur.” Discuss
Lorem ipsum qui porro insolens reprimique an, appetere verterem conceptam eam eu. Ut nam nullam commune explicari. Vis ex mazim choro accusamus, sed civibus complectitur ex. Mazim aliquando vel in.
Pri vero aliquam docendi ut, cu mei suas adhuc facilisis, saepe eloquentiam ad nec. Epicuri detracto principes sit in, vis latine luptatum recusabo et. An sed officiis vulputate, minim oratio duo no. Prima minimum oportere eum in. Id pro augue virtute. Eam unum laboramus adolescens te, alia tritani vix id, sit in labores iracundia moderatius.
Mel in meis mutat scripserit, summo tantas apeirian ius id. Mei eu voluptua neglegentur philosophia, corrumpit evertitur sea no. Fabellas invidunt invenire sed cu. No mel volumus intellegebat. Ex novum delicata usu, eros probo ad mel. Amet cibo vim ex. Per tibique probatus insolens id.
Sonet consequuntur eam ex, ea placerat expetendis referrentur mea. Unum commodo minimum mea at, at sea possim posidonium concludaturque. Sit quodsi deseruisse ne, dicta consul insolens pro ad. No essent persius mediocritatem est, sed et dico sanctus, qui te latine signiferumque. Ad facilis percipitur duo, usu equidem euripidis an, id erat dicunt temporibus pri.
Alia puto eos ut, errem tempor melius vel eu. Vim te omnis scripta quaeque, in his
B Twemlow (BXT32) / Bing-Tredmille / 30 Oct 2010 ... 2
oratio veniam fierent. Eum dictas molestie definiebas ne, te maiorum persequeris vim. Pri at integre dolorem.
Cum menandri postulant te, causae accusam te has. No solet gubergren pro, tollit molestie signiferumque cu usu, qui nostrum partiendo adversarium ut. Vis oportere constituto id, mel odio eirmod verterem id, fugit dicunt argumentum pri ex. Sensibus petentium iracundia ut vim. Ius ei dicta molestie sententiae, modus voluptaria ut nam.
Nonummy delicatissimi eu pro, ubique inimicus voluptaria at sea. Cu nec delectus invenire. Eos possim vocibus ut. Ut kasd solet qualisque sit, cu lucilius signiferumque sea, no populo consequat usu.
Cu vel habeo delenit interpretaris, usu et alienum epicuri consetetur. Vel an soleat alienum, no vim impedit sapientem. Id nec ceteros consectetuer interpretaris, eum ad audiam eloquentiam, ad quo falli mandamus. Pro ad quodsi denique, ad intellegat assueverit honestatis per, cum eu labitur inimicus. Pro altera definitionem ea, vim no unum mutat dicunt. Et quo malis nullam essent.
Antiopam maluisset ne mea. Ei solet saepe qui. Cu vocent fuisset vix. Eos ad electram assentior, erat assum saepe sed ne. Ut facete eruditi elaboraret duo, ex est tempor vocibus consetetur. Mea debet singulis ut, erat facer simul mei te.
Mei id quis petentium, meis nostrum eu ius, vocent takimata principes ei nec. Sea laoreet delectus in, aperiri omittantur eum cu, detracto nominavi at cum. Mel an dolores officiis, patrioque adipiscing definitiones ius ad, at sed cibo velit. Pri solet vocent prompta no, eos ut quaerendum consequuntur, nam no commodo voluptatibus. Mei at forensibus comprehensam.’i Lorem ipsum qui porro insolens reprimique an, appetere verterem conceptam eam eu. Ut nam nullam commune explicari. Vis ex mazim choro accusamus, sed civibus complectitur ex. Mazim aliquando vel in.
B Twemlow (BXT32) / Bing-Tredmille / 30 Oct 2010 ... 3
Cum menandri postulant te, causae accusam te has. No solet gubergren pro, tollit molestie signiferumque cu usu, qui nostrum partiendo adversarium ut. Vis oportere constituto id, mel odio eirmod verterem id, fugit dicunt argumentum pri ex. Sensibus petentium iracundia ut vim. Ius ei dicta molestie sententiae, modus voluptaria ut nam.
Nonummy delicatissimi eu pro, ubique inimicus voluptaria at sea. Cu nec delectus invenire. Eos possim vocibus ut. Ut kasd solet qualisque sit, cu lucilius signiferumque sea, no populo consequat usu.
Mei id quis petentium, meis nostrum eu ius, vocent takimata principes ei nec. Sea laoreet delectus in, aperiri omittantur eum cu, detracto nominavi at cum. Mel an dolores officiis, patrioque adipiscing definitiones ius ad, at sed cibo velit. Pri solet vocent prompta no, eos ut quaerendum consequuntur, nam no commodo voluptatibus. Mei at forensibus comprehensam.ii Lorem ipsum qui porro insolens reprimique an, appetere verterem conceptam eam eu. Ut nam nullam commune explicari. Vis ex mazim choro accusamus, sed civibus complectitur ex. Mazim aliquando vel in.
Pri vero aliquam docendi ut, cu mei suas adhuc facilisis, saepe eloquentiam ad nec. Epicuri detracto principes sit in, vis latine luptatum recusabo et. An sed officiis vulputate, minim oratio duo no. Prima minimum oportere eum in. Id pro augue virtute. Eam unum laboramus adolescens te, alia tritani vix id, sit in labores iracundia moderatius.
Mel in meis mutat scripserit, summo tantas apeirian ius id. Mei eu voluptua neglegentur philosophia, corrumpit evertitur sea no. Fabellas invidunt invenire sed cu. No mel volumus intellegebat. Ex novum delicata usu, eros probo ad mel. Amet cibo vim ex. Per tibique probatus insolens id.
B Twemlow (BXT32) / Bing-Tredmille / 30 Oct 2010 ... 4
Alia puto eos ut, errem tempor melius vel eu. Vim te omnis scripta quaeque, in his oratio veniam fierent. Eum dictas molestie definiebas ne, te maiorum persequeris vim. Pri at integre dolorem.
Cu vel habeo delenit interpretaris, usu et alienum epicuri consetetur. Vel an soleat alienum, no vim impedit sapientem. Id nec ceteros consectetuer interpretaris, eum ad audiam eloquentiam, ad quo falli mandamus. Pro ad quodsi denique, ad intellegat assueverit honestatis per, cum eu labitur inimicus. Pro altera definitionem ea, vim no unum mutat dicunt. Et quo malis nullam essent.iii Cu vel habeo delenit interpretaris, usu et alienum epicuri consetetur. Vel an soleat alienum, no vim impedit sapientem. Id nec ceteros consectetuer interpretaris, eum ad audiam eloquentiam, ad quo falli mandamus. Pro ad quodsi denique, ad intellegat assueverit honestatis per, cum eu labitur inimicus. Pro altera definitionem ea, vim no unum mutat dicunt. Et quo malis nullam essent.iv
i John Donne, Selected Prose, ed. Helen Gardner and Timothy Healy (Oxford: Clarendon Press, 1967), p. 46. ii Donne, Selected Prose, p. 26.
iii ‘He was a martyr manqué,’ Carey writes, ‘and had to live with a set of basic psychic configurations which had been oriented towards death by his educators.’ John Donne: Life, Mind and Art (London: Faber, 1990), p . 213.
iv Donne, ‘To Mr Tilman after he had taken orders’, ll. 15-16, 17-18, in The Divine Poems, ed. Helen Gardner (Oxford: Clarendon Press, 1952), p. 32. Future references to this edition will be given in the text as line numbers.
</Text>
        </Document>
        <Document ID="48">
            <Title>Back up SNS</Title>
            <Text>Back up your SNS
	⁃	Protect against networks going down &lt;make a § over that&gt;, being discontinued or forgotten
	⁃	Protect against  technological obsolescence
	⁃	Hard to retrieve data on long timelines
	•</Text>
        </Document>
        <Document ID="52">
            <Title>Questions</Title>
            <Text>What is the end goal
What is the project space (literally or the area / keywords)
What do I know
Need to find out
Who has already done it
What is the [visual?] language used

	•	How could I design something to preserve &amp; organise memories created digitally to give them the same value as physical objects who are sentimentally important?
	•	How could I design a technologically sustainable object that is meant to exist for decades without becoming obsolete?
	•	How do I represent a lifetime graphically and in a hierarchically relevant way?

Different types of questions (see slides Fraser presentation)/
Research question: are our memories safe on the internet? 
Safe -&gt; preserved, cherished, archived, kept on the long term; safe = privacy in the mind of most people, not ‘technically’ safe in their integrity</Text>
        </Document>
        <Document ID="111">
            <Title>Loss of metadata</Title>
            <Text>Poor preservation documentation and metadata means it could be hard for potential users to find the data (UNESCO 2003). “Critical aspects of functionality, such as formatting of documents or the rules by which databases operate, may not be recognised and may be discarded or damaged in preservation processing.” (UNESCO 2003)
Poor integrity: “So much contextual information may be lost that the materials themselves are unintelligible or not trusted even when they can be accessed”</Text>
        </Document>
        <Document ID="107">
            <Title>Reflection on prototypes</Title>
        </Document>
        <Document ID="49">
            <Title>Make sustainable system</Title>
            <Text>Make a sustainable system
	⁃	Can be renewed/upgraded
	⁃	Use strong technology (see embedded systems, SSDs with no moving parts) as recommended by R Banks
	⁃	open design —  what is the purpose of it, what would happen to your company if such a product launched, who owns what, etc. [Tie in with part on open design in literature review]</Text>
        </Document>
        <Document ID="53">
            <Title>Need for preservation</Title>
            <Synopsis>Get the FULL context and relationship — why is there a shift to digital in storing stuff that’s emotionally important, precisely what in the nature of social networks prove that they will disappear and that their content needs to be preserved. </Synopsis>
            <Notes>Reference The Archive Team, and Jason Scott’s arguments/talks for cultural preservation of data

Use divergent technique &gt; keep asking WHAT IS… WHAT IS… WHAT IS to define memories, statuses, social networks, communities, internet etc

Correct data to singular</Notes>
        </Document>
        <Document ID="54">
            <Title>Guru's Day Questions</Title>
            <Text>Title of your project: Social media backups
Question for the gurus (40 words): How could I design an interface that would allow users to go through a lifetime of digital memories in a meaningful and organised way?
Description of your project (50 words): A time capsule for the contents that you post on social networks, to keep your digital memories safe and organised over decades, just like diaries or photo albums. The project aims to use open design to be sustainable.</Text>
        </Document>
        <Document ID="112">
            <Title>Personal archiving</Title>
            <Text>
In The Future of Looking Back, Richard Banks (2011) analyses the reasons that lead us to keep or discard physical objects at home. He argues that while making a selection is a positive thing because it forces us to filter our belongings and focus on what is really meaningful, we also run the risk of ridding ourselves of things that we wish we would have kept in retrospect. He points out that the organisation of digital files make this trickier: because we we can keep pretty much everything on virtual space, it is easier to accidentally delete meaningful things alongside the less important. (Banks, 2011 p. 22).
In the context of personal archiving, Banks notes that in the past, the legacy we leave behind us was a highly selective one. Because our possessions are limited by physical space, our ancestors until now “have implicitly indicated that an item is significant through the simple act of keeping it. Although thousands of objects likely crossed their path over their lifetime, the constraints of money and space meant that only a few were retained, often the most significant.” (Banks, 2011, p. 78). He does however argue that this is no longer true in a digital world, because new possibilities mean we simply accumulate digital objects without necessarily ordering them by importance.</Text>
            <Notes>Parrallel between generic &amp; personal stuff, that is museums/archives vs personal storage
</Notes>
        </Document>
        <Document ID="55">
            <Title>Lecture: structure</Title>
            <Text>10/10/14 - towards a first draft

draft: 12 noon, 27th October, hard copy, bind if needed
NOT about trying to write sth coherent, well argued &amp; well written
it’s about finding a *structure*
organising thoughts &amp; planning wht you want to write
block in ideas, supporting material
organise material using “containers”
	literally (moving paper notes) or metaphorically
	label (headings)
	macro/micro headings
think of it as a design problem, not a writing assignment
words = things, can be manipulated, altered, changed, swapped or thrown away
			= thing that can be shaped
organise into §s and headings
	ok to use empty ones &amp; them as placeholders
	if copying-pasting chunks of other own material, highlight it to be reminded to edit/rewrite
	&lt;Notes to self&gt;

# contains
- title
- ToC
- ToFigures
- Intro = summary of what is to follow, maybe do it first, 1-2pages
- Chapters / body
- Conclusions
- References - further reading / separate bibliography + reference list
- Appendices &gt; from primary research
	- transcript interviews or only sections of it

@todo test Scrivener
</Text>
        </Document>
        <Document ID="108">
            <Title>Reflection on experiments</Title>
        </Document>
        <Document ID="56">
            <Title>Timelines podcast</Title>
        </Document>
        <Document ID="60">
            <Title>Lecture: gathering data</Title>
            <Text>Talk 17/10/14 how to use data

talk-aloud protocols: ask participants to report what they’re doing
OR talk after (retroactive)
record (?) + transcribe questions &amp; answers, prompt them to tell us what they see, highlight 
CODE the informatº &gt; use colour codes / highlight in same colour different elements
(for instance objects in blue; environment in violet…). Categorise questions (Realness, Anything else…)
Colour code same issues in the end (interview analysis)
Avoid questionnaires &gt; more qualitative than qtitative, prefer interviews
In the report: reference the different interviews/appendices by number (Realness: bla blah findings (4.3.2))
Quote of the itw below the results to back up claims
Link things together to make a point don’t just leave it at the end
Detail age groups/genders not individually by participant but as a whole (on 13 participants, 8 female and 5 male). “General demographics”

Explain methodology (eg authoethnography) -&gt; keep track of what you do, what you think about it, 

3 things to do w/ your data
## Introduce it properly ie talk about participants, methods &amp; procedures involved in gathering data
Provide intro to why gather the data
Who participated in it, ages, gender, profession/background

Methods: itws, questnnrs, video observs, focus grps
What procedures, how long were the itws, what happened during video obs or focus grp

## Present the most important parts
Protocols used to analyse the data
Explain how it is codes
Finding patterns that repeat or themes that emerge from the data,
or new information that contradicts existing theories

State what the results are, provide explanation, provide examples of real-world data
Quotes

## Provide evidence to back up analysis eg example of actual data

Provide ALL raw data in appendices
Questions asked
Provide on disc if video/audio not transcribed… -&gt; then provide timecodes

—
Explain if it contradicts/keeps in line with own opinion, treat it like any other source

SECTIONS of a RESEARCH CHAPTER in the body: (appendices only contain raw data/transcripts)
Intro : why we did this
participants + demographics
methodological approach + method &gt; description of the procedure used (eg what is itw, what is auto ethnography, wht it was relevant)
procedure followed / what did you do, happened, analysed
results presented alongside examples
discussion of results &amp; evidence

</Text>
        </Document>
        <Document ID="113">
            <Title>Pro/cons of archiving everything</Title>
            <Text>
There are various controversies around whether we should automate the archiving of the Internet, or if we should operate a manual selection. UNESCO (2003, p. 73) notes that those who value a more comprehensive collection (i.e. archiving “all” the Internet) argue that “any information may turn out to have long-term value, and that the costs of detailed selection are greater than the costs of collecting and storing everything”. This argument is particularly true because of the digital media: “unlike non-digital material such as paper-based artifacts, where there is a period of time in which to make selection decisions before deterioration of materials becomes an issue, the time frame for deciding whether or not to preserve digital materials is very short” (Harvey, 2012, p. 57). UNESCO (2003, p.71)backs that claim: “it may not be possible to wait for evidence of enduring value to emerge before making selection decisions”.
Other people [Cook?] think that it would be preferable to keep a curated collection because of a higher technical quality, and this also allow us to settle [negotiate?] preservation rights with the content producers. Harvey adds that we also do not have the technical capabilities to find all the pages on the Internet, and even if we had we would not have the technical possibility to store the whole of the Internet (Harvey 2012, p. 58); again, this is also something recognised by UNESCO: “‘there are usually more things – more information, more records, more publications, more data – than we have the means to keep” (2003, p.70).
The UNESCO (2003) and Cook (2000) both however recognise that a middle ground should be found and that both approaches are valid depending on the goal of the preservation. For instance, the Internet Archive[IA] is going through an automated process (archive as many pages as possible) but, because of technical limitation, only preserves the text and dismisses all other images, stylesheets and external media, which could be critical in some instances. It seems evident that for valued documents these should be retained. The British Library’s Web Archive[ukwa] [http://www.webarchive.org.uk/ukwa/]</Text>
            <Notes>Parrallel between generic &amp; personal stuff, that is museums/archives vs personal storage
</Notes>
        </Document>
        <Document ID="109">
            <Title>Evaluate them</Title>
        </Document>
        <Document ID="57">
            <Title>Desire to visualise differently</Title>
            <Text>
[(Haynes, 2013b) http://astormisblowing.org/2013/10/11/how-do-you-map-a-life/:
We didn’t always picture life like this. The ancient Stoic philosophers, for example, saw every individual life as a perfect circle, whether that life lasts 30 or 80 years. The notion that a life’s worth should be assessed as full or lacking by its abstract quantities would have seemed faulty to them.
And it surely is faulty. A measurement is only a shadow of the thing. If we mistake the map for the land, the score for the performance, the timeline for the life, then we reduce existence to its shadows. As the philosopher Henri Bergson warned, lived time in all its fullness can’t be measured. Doing so risks reducing a life to a machine or commodity.
So what if we tried an experiment and mapped our life as we really experience it to be: a map without measurements? One that gives space to events according to their significance rather than their length in clock-time?
What if we made that map just for our self and included what we would leave out of a job application – a map that includes the mazes and trapdoors along with the parades of glory and the leaps of progress? Might we value the failures and the ‘time wasted’ more then? And when we accept and value our past experience in all its fullness, how does that change our view of the future?</Text>
        </Document>
        <Document ID="61">
            <Title>Literature review</Title>
        </Document>
        <Document ID="58">
            <Title>Transcript </Title>
            <Text>
Ben Hammersley {Haynes:2014aa} @ 13:00 on the irrelevance of timelines
It’s artificial and it comes from the constraints that the timeline gives in describing your life. […] As human beings we all do multiple things at the same time. And so to try [to avoid that] instead of being a timeline it would be multiple timelines, and some interests would fade away and some parts of our lives would [shine?]. It’s not a single line but because the format forces you into a single line there’s a natural convergence within the crowd to specific measurable things. So if you see that everyone else puts their GCSE results on there, then even if you were 16 and the thing that was most important to you was your ballet class rather than your maths GCSE, then you wouldn’t put your ballet class there, you would put the GCSEs. There is a sort of crowd following thing, of “that’s the appropriate thing to post at that point in the timeline on Facebook”, but that’s not necessarily indicative of your own personal values, it’s just something that’s brought about by the architecture of [Facebook]. 
Aleks K disagrees
The only difference is that they’re there for the record. If you think about playgrounds dynamics, you would have seen exactly the same thing. Everybody would be sitting around talking about their GCSE results, not throwing out that they’re absolutely in love with ballet. What the web does, which is extraordinary — and especially for a psychologist like myself, is that there’s all these unspoken things that happen, there’s all these dynamics that happen, socially. Whether it’s to do with indicators of trust in another person, or indicators of belonging to a particular group, that, historically, throughout our social and psychological lives, we have expressed. That is literally what the study of social psychology is, it’s trying to identify and map, and unfortunately in many ways predict, what people will do and how people will behave and how people will think and act based on the dynamics of what’s going on, the dynamics that often appear quite subjective, because you’re experiencing them in your head. However, what the web does, in a really exciting way, is that it makes that visible. So suddenly you’re seeing these influence pathways, those influence patterns in the playground, which, if you were a teacher, you’d be able to describe. Now everybody can see it, and now we can see how we can form in everyday lives, based on the signals of our social groups. So I completely disagree that it’s forcing us into conformity; there’s lots and lots of different influences. The only area where I fear for is enforcing conformity. It’s the inability for people to make mistakes, because these mistakes, given how it is that we’re interacting with this information online now, those mistakes are therefore the record, which means that people will go back and they’ll look at the mistakes and they’ll define you by the mistakes that you’ve made in the past that you may have learned from, but you haven’t been able to leave them behind you because they’re there.</Text>
        </Document>
        <Document ID="62">
            <Title>Workshop</Title>
        </Document>
        <Document ID="114">
            <Title>5 reasons</Title>
            <Synopsis>Harvey 2012, p.26</Synopsis>
            <Text>	•	If unique information objects that are vulnerable and sensitive and therefore subject to risks can be preserved and protected
	•	If preservation ensures long-term accessibility for researchers and the public;
	•	If preservation fosters the accountability of governments and organisations
	•	If there is an economic or societal advantage in re-using information
	•	If there is a legal requirement to keep it (NSF-DELOS Working Group on Digital Archiving and Preservation, 2003, p.3).
— Harvey 2012, p.26. &lt;To rewrite/delete&gt;</Text>
            <Notes>Parrallel between generic &amp; personal stuff, that is museums/archives vs personal storage
</Notes>
        </Document>
        <Document ID="63">
            <Title>Introduction</Title>
            <Synopsis>Provide intro to why gather the data
</Synopsis>
            <Text>In order to get a better understanding on how we could map a life, I have organised a workshop where I have asked some people to perform a few tasks such as graphically representing their life and retrieving old content on social networks, and answer a few questions about the project. This chapter will detail the methodological approach used to prepare the workshop and questions, then analyse the results and make conclusions on how these results can be used to design a retrieval interface. The raw questions and results are available in the appendices A to E.</Text>
            <Comments>Change with actual appendices name
</Comments>
        </Document>
        <Document ID="59">
            <Title>Untitled</Title>
        </Document>
        <Document ID="64">
            <Title>Demographics</Title>
            <Synopsis>Demographics: Who participated in it, ages, gender, profession/background.
Maybe use a table? Do not allow to individually identify the participants</Synopsis>
            <Text>The workshop was attended by 9 participants. All of them were final year design students at Duncan of Jordanstone College of Art and Design, in Dundee.
Amongst the participants were 3 women and 6 men. Six were native speakers of English; three were from overseas and had English as a second language. All were raised in a Western culture and their native language reads left-to-right,which ensured consistent results (as we’ve seen before, culture and language alters our visual representation of time); but one of the native English speakers has been raised with a bicultural identity and also natively speaks Cantonese, which reads left-to-right horizontally but can also be written vertically (top to bottom, and columns read right to left; like Mandarin, this can influence perception of time, see Boroditsky, 2011, pp. 335-336). They are all frequent users of Facebook; while most owned Twitter accounts, few of them actually used it, so the questions focused on Facebook.</Text>
        </Document>
        <Document ID="115">
            <Title>Dangers of selection</Title>
            <Text>However, Cook (2000) points out the dangers of manual selection for what is worth archiving: “With memory comes the inevitable privileging of certain records and records creators, and the marginalizing or silencing of others” (Cook, 2000, cited in Harvey p. 58). He asserts that the archivists must be impartial and objective, and ensure all points of view are kept to ensure our shared memory does not “becomes counterfeit, or at least transformed into forgery, manipulation, or imagination”. This is something that is clear with manual archiving, but could also be a problem, maybe unintentionally, with the automated discovery of content when archiving Web pages.</Text>
            <Notes>Parrallel between generic &amp; personal stuff, that is museums/archives vs personal storage
</Notes>
        </Document>
        <Document ID="65">
            <Title>Methodological approach</Title>
        </Document>
        <Document ID="120">
            <Title>Keeping more context</Title>
            <Text>Banks (2011) also suggests that we could blend in the physical and the digital to allow recording of things we never had the opportunity to before. He gives as example the Weather Camera, an experimental object by Kjen Wilkens (2011) that records wind and temperature along with commentary, instead of an image, as an alternative to recall a place; or 3D backups by Héctor Serrano (2009), another experimental project that allows us to make backups of sentimentally important objects through 3D scanning and 3D printing.
Similarly, Banks notes tools by Microsoft Research to recreate places in a 3D environment based on photos, allowing to recreate a sentimentally important place at any given point in time provided enough photos are available. A similar technique is used in bigger preservation projects, as described by Zhou et al. (2012), to make high quality 3D models of culturally important places and objects in China.
Kirk and Sellen (2010) found however that ultimately, stories and narrative are more important than the preservation of the artifact itself. They propose that “technology might play a role in capturing and associating stories or narratives with different physical objects”, imagining that we could augment physical objects with RFID tags (and I would add, virtual reality technologies) to pass down stories attached to these objects.</Text>
        </Document>
        <Document ID="116">
            <Title>Automatic selection (Reputation model)</Title>
            <Synopsis>How to curate by popularity on SNS (likes, comments, shares)</Synopsis>
            <Text>In order to keep the most valuable content, algorithms could be used. Kawano (2008) proposes to use reputation models, that attempts to determine the relative value of a Web page by considering its “importance, fairness, trustiness, uniqueness and valuation”. This is done using a variety of ways; the algorithm suggested by Kawano relies on the consistency of a web page, that is the amount of time it existed, the number of links it contains and the number of external links to this page (Kawano, 2008, p. 291).</Text>
            <Notes>Parrallel between generic &amp; personal stuff, that is museums/archives vs personal storage
</Notes>
        </Document>
        <Document ID="70">
            <Title>Bibliography</Title>
        </Document>
        <Document ID="66">
            <Title>Procedure followed</Title>
        </Document>
        <Document ID="71">
            <Title>Literature review</Title>
            <Text>AppleInsider. (2011) ‘Inside Mac OS X 10.7 Lion: Missing Front Row, Rosetta and Java runtime’. AppleInsider. [web page] Accessible at http://appleinsider.com/articles/11/02/26/mac_os_x_lion_drops_front_row_java_runtime_rosetta.html [Accessed: 19th Apr 2014] 
Ayre, C. and Muir, A. (2004) ‘The right to preserve’. D-Lib Magazine, vol. 10 (No. 3). March. Available at http://www.dlib.org/dlib/march04/ayre/03ayre.html [Accessed: 11th Apr 2014]
Banks, R. (2011) The Future of Looking Back. Sebastopol: Microsoft Press.
Bearman, D. (1999) ‘Reality and Chimeras in the Preservation of Electronic Records’. D-Lib Magazine, vol. 5 (No. 4). April. Available at http://www.dlib.org/dlib/april99/bearman/04bearman.html [Accessed: 7th Apr 2014]
Boyd, D. (2014) It’s Complicated: The Social Lives of Networked Teens. New Haven and London: Yale University Press.
byuu. (2011) ‘Accuracy takes power: one man’s 3 GHz quest to build a perfect SNES emulator’. Ars Technica. [web page] Available at: http://arstechnica.com/gaming/2011/08/accuracy-takes-power-one-mans-3ghz-quest-to-build-a-perfect-snes-emulator/  [Accessed: 8th Apr 2014]
Cook, T. (2000) ‘Beyond the screen: the records continuum and archival cultural heritage.’ Presented at the Australian Society of Archivists Conference, Melbourne, 18 August. Available at http://www.mybestdocs.com/cook-t-beyondthescreen-000818.htm [Accessed: 9th Apr 2014]
Granger, S. (2000) ‘Emulation as a Digital Preservation Strategy’. D-Lib Magazine, vol. 6 (No. 10). October. Available at http://www.dlib.org/dlib/october00/granger/10granger.html [Accessed: 7th Apr 2014]
Guttenbrunner, M., Becker, C. and Rauber, A. (2010) ‘Keeping the game alive: Evaluating strategies for the preservation of console video games’. International Journal of Digital Curation, vol. 5 (Iss. 1), pp. 64-90.
Hangal, S., Lam, M. S. and Heer, J. (2011) ‘MUSE: reviving memories using email archives’, in Proceedings of the 24th annual ACM symposium on User interface software and technology (UIST ‘11). New York, NY: ACM, pp. 75-84. doi:10.1145/2047196.2047206
Harvey, D. R. (2012) Preserving digital materials. Berlin: De Gruyter Saur.
Kahle, B. (1996) ‘Archiving the Internet’. Scientific American, Iss. March 1997. Available at http://web.archive.org/web/19971211123138/www.archive.org/sciam_article.html [Accessed: 27th Feb 2014]
Kawano, H. (2008) ‘Strategy of Digital Contents Archive Based on Reputation Model’, in ICSENG ‘08. Proceedings of 19th International Conference on Systems Engineering, 19-21 Aug, pp. 288-293. doi: 10.1109/ICSEng.2008.75
Kirk, D. S. and Sellen, A. (2010) ‘On human remains: Values and practice in the home archiving of cherished objects’. ACM Transactions on Human-Computer Interaction, vol. 17 (Iss. 3), article 10. doi:0.1145/1806923.1806924
Library of Congress. (2007) Digital Preservation Program Makes Awards to Preserve American Creative Works. [press release] Available at http://www.loc.gov/today/pr/2007/07-156.html [Accessed: 14th Apr 2014]
Lubar, S. (1999) ‘Information Culture and the Archival Record’, in American Archivist, vol. 62 (Iss. 1), pp. 10-22.
Lynch, C. (2000) “Authenticity and Integrity in the Digital Environment: An Exploratory Analysis of the Central Role of Trust’, in Authenticity in a Digital Environment. Washington, DC: Council on Library and Information Resources. Available at http://www.clir.org/pubs/abstract/pub92abst.html [Accessed: 16th Apr 2014]
Masanès, J. (2006) Web archiving. Berlin: Springer.
Mesa, A. F. (1997) ‘The PowerPC Triumph’. The Apple Museum. [web page] Available at: http://applemuseum.bott.org/sections/ppc.html [Accessed: 19th Apr 2014]
Paloque-Bergès, C. (2011) Entre trivialité et culture : une histoire de l’Internet vernaculaire. Émergence et médiations d’un folklore de réseau [Between Triviality and Culture: a History of the Vernacular Internet. Emergence and mediations in network folklore]. PhD thesis, self-published. Available at http://camillepaloqueberges.wordpress.com/phd-thesis/ [Accessed: 6th Apr 2014] 
Paloque-Bergès, C. (2013) ‘Un patrimoine composite : le public Internet face à l’archivage de sa matière culturelle’ [A composite heritage: the Internet public confronted to the archiving of its cultural matter], in I. Dragan, P. Stefanescu, N. Pelissier, J.-F. Tétu and L. Idjeroui-Ravez (eds.) Traces, mémoire, communication. Bucharest: Editura Universităţii din Bucureşti. 2
Pearson, D. (2009) Preserve or Preserve Not, There is No Try: some dilemmas relating to Personal Digital Archiving. [PowerPoint slides]. National Library of Australia Staff Papers. Available at http://www.nla.gov.au/openpublish/index.php/nlasp/article/view/1388 [Accessed: 9th Apr 2014]
del Pozo, N., Long, A. S. and Pearson, D. (2012) ‘Land of the Lost: a discussion of what can be preserved through digital preservation’. Library Hi Tech, vol. 28 (Iss. 2), pp. 290-300. Available from: doi: 10.1108/07378831011047686. [Accessed: 6th Apr 2014]
Ross, S. and Gow, A. (1999) Digital archaeology: rescuing neglected and damaged data resources: a JISC/NPO study within Electronic Libraries (eLib) Programme on the Preservation of Electronic Materials. London: Library Information Technology Centre. Available at http://www.ukoln.ac.uk/services/elib/papers/supporting/pdf/p2.pdf [Accessed: 17th Apr 2014]
Rothenberg, J. (1998) Avoiding Technological Quicksand: Finding a Viable Technical Foundation for Digital Preservation. [report] Washington, DC: Council on Library and Information Resources.
Serrano, H. (2009) Back Up Objects, Héctor Serrano. [web page] Available at: http://www.hectorserrano.com/index.php?id=41&amp;m=lab&amp;grupo=backup [Accessed: 17th Apr 2014].
SINTEF. (2013) ‘Big Data, for better or worse: 90% of world’s data generated over last two years.’ ScienceDaily. [web page] Available at: http://www.sciencedaily.com/releases/2013/05/130522085217.htm [Accessed: 16th Apr 2014]
Swade, D. (1993) ‘The problems of software conservation”, in Computer Resurrection, vol. 7. Available at: http://www.cs.man.ac.uk/CCS/res/res07.htm#f [Accessed: 12th Apr 2014]
Task Force on Archiving of Digital Information. (1996) Preserving digital information, Report of the Task Force on Archiving of Digital Information. Washington, DC: Council on Library and Information Resources.
Thibodeau, K. (2002) ‘Overview of technological approaches to digital preservation and challenges in coming years’, in The state of digital preservation: an international perspective. Washington, DC: Council on Library and Information Resources, pp. 4-31.
UNESCO. (2003) Guidelines for the preservation of digital heritage, prepared by the National Library of Australia. Paris: UNESCO. Available at http://unesdoc.unesco.org/images/0013/001300/130071e.pdf [Accessed: 9 Apr 2014]
Wilkens, K. (2011) Sensor Poetics, Kjenwilkens.com. [web page] Available at http://www.kjenwilkens.com/projects/sensor-poetics [Accessed: 18th Apr 2014]
Zimmer, M. (2010) ‘“But the data is already public”: on the ethics of research in Facebook’, in Ethics and information technology, vol. 12 (Iss. 4), pp. 313-325. doi: 10.1007/s10676-010-9227-5
Zhou, M., Geng, G. and Wu, Z. (2012) Digital preservation technology for cultural heritage. Berlin: Springer.</Text>
            <Comments>The article is only credited to the developer’s pseudonym.
In French.</Comments>
        </Document>
        <Document ID="67">
            <Title>Results</Title>
        </Document>
    </Documents>
</SearchIndexes>